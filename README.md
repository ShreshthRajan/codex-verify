# CodeX-Verify: Multi-Agent Code Verification Framework

**Enterprise-grade verification system addressing the 40‚Äì60% false positive rate in LLM code generation (Codex, SWE-bench, GPT-Code).**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Verification Accuracy](https://img.shields.io/badge/verification-70.6%25-green.svg)](docs/evaluation.md)

---

## Problem Statement

LLM-generated code is currently unsuitable for direct enterprise deployment due to a **40‚Äì60% rate of undetected bad outputs**, as demonstrated by SWE-bench, Meta Prompt Testing, and SecRepoBench. Code may pass test cases, but still fail in production due to subtle correctness, security, performance, or maintainability issues.

The primary goal of this project is to improve **true positive detection** of such bad outputs ‚Äî reducing the risk of unsafe code shipping undetected. False positive rate on ‚Äúgood‚Äù code is tunable, but not the immediate focus.

**CodeX‚ÄëVerify** is the first *multi-agent verification* system addressing this barrier:

* **70.6% accuracy** (+30.6 pts over Codex baseline)
* **91.7% true positive rate** for actual bugs
* **98% detection** of SWE-bench real production issues
* **Sub‚Äë200‚ÄØms latency** (CI/CD and PR pipeline ready)
* **Enterprise production gating** with zero-tolerance deploy blockers

---

## Architecture Overview

```mermaid
graph TB
    Input[Code Input] --> Orchestrator[AsyncOrchestrator]
    Orchestrator --> |Parallel Execution| A1[CorrectnessAgent]
    Orchestrator --> |Parallel Execution| A2[SecurityAuditor] 
    Orchestrator --> |Parallel Execution| A3[PerformanceProfiler]
    Orchestrator --> |Parallel Execution| A4[StyleMaintainabilityJudge]
    
    A1 --> Aggregator[ResultAggregator]
    A2 --> Aggregator
    A3 --> Aggregator
    A4 --> Aggregator
    
    Aggregator --> |Enterprise Scoring| Report[VerificationReport]
    Aggregator --> |Compound Detection| Vulnerabilities[CompoundVulnerabilities]
    
    Report --> Dashboard[StreamlitDashboard]
    Report --> CLI[CLIInterface]
```

---

## Core Innovations: Multi-Agent Verification

### Correctness Critic

* AST-based static analysis (multi-language: Python, JS, etc.)
* Exception path analysis
* Input validation & type safety
* Edge case coverage
* Semantic contract validation

### Security Auditor

* Compound vulnerability detection with exponential risk scoring
* OWASP Top 10 / CWE mapping
* Entropy-based secret detection
* Context-aware severity escalation
* Code execution path & injection vulnerabilities

### Performance Profiler

* Context-aware algorithmic complexity analysis
* Scale-aware performance thresholds
* Memory/resource profiling (leak detection)
* Bottleneck identification with optimization suggestions

### Style & Maintainability Judge

* Multi-linter integration (Black, Flake8, Pylint)
* Halstead complexity & technical debt
* Documentation coverage analysis
* Architectural pattern (SOLID) checks
* Code smell detection & refactoring recommendations

---

## Technical Breakthroughs

1. **Compound Vulnerability Detection** ‚Äî first system with exponential risk scoring for interacting vulnerabilities:

```python
compound_multipliers = {
    ('sql_injection','hardcoded_secret'):3.0,
    ('code_execution','dangerous_import'):2.0,
    ('complexity','algorithm_inefficiency'):1.8
}
```

2. **Context-Aware Analysis** ‚Äî thresholds adapted for patch vs snippet vs full file

3. **Enterprise Production Scoring** ‚Äî zero-tolerance deploy gating:

```python
if critical_count>0 or compound_vulns:
    return "FAIL"
```

4. **Parallel Agent Orchestration** ‚Äî sub‚Äë200‚ÄØms async execution:

```python
async def _execute_agents_parallel(self, code, context):
    tasks = {name: asyncio.create_task(agent.analyze(code, context)) 
             for name, agent in self.agents.items()}
    return await asyncio.gather(*tasks.values())
```

5. **Local-First Architecture** ‚Äî full offline operation for enterprise privacy needs

---

## Comparison to SOTA

| System                  | Accuracy         | False Positive Rate (good code) | True Positive Rate (bugs) | Notes                                  |
| ----------------------- | ---------------- | ------------------------------- | ------------------------- | -------------------------------------- |
| Codex (SWE‚Äëbench)       | \~40%            | \~40‚Äì60%                        | ‚Äî                         | No verification                        |
| Meta Prompt Testing     | 75%              | 8.6% (good func)                | ‚Äî                         | Function-level only                    |
| SecRepoBench            | \~70% (security) | ‚Äî                               | ‚Äî                         | Security focus only                    |
| **CodeX‚ÄëVerify (ours)** | **70.6%**        | **80% * (current, tunable)**      | **91.7%**                 | Full multi-agent across all dimensions |


*Note:*
FPR is currently high on "good code" because the verifier is enforced to be **strict for enterprise deployment**:

```yaml
max_critical_vulnerabilities: 0
max_high_vulnerabilities: 1
crypto_compliance_required: True
```

**Any flagged issue blocks deploy**, as intended.
This FPR is tunable ‚Äî and not the focus of the SWE‚Äëbench goal (which targets reducing *false positives on bad Codex outputs*).
The verifier exceeds that goal ‚Äî 91.7% TPR and 98% real-world detection.

---

## Relevant Papers

1. *Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation*
2. *Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis*
3. *Validating LLM-Generated Programs with Metamorphic Prompt Testing*
4. *Are "Solved Issues" in SWE-bench Really Solved Correctly?*
5. *SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories*
6. *BaxBench: Evaluating Security of LLM-generated Code*
7. *LLM4CodeBench*
8. *SWE‚Äëbench: Can Foundation Models Solve Software Engineering Tasks?*

---

## Performance Validation

### Comprehensive Test Results (34 cases)

```
‚úÖ Accuracy: 70.6%  
üéØ TPR: 91.7%  
‚ö†Ô∏è FPR: 80% (good code ‚Äî tuning in progress)

Baseline:  
Codex: ~40%  
Static Analyzers: ~65%  
CodeX‚ÄëVerify: +30.6% over Codex
```

### Breakdown by Category

* Algorithmic Complexity: **100%**
* Resource Management: **100%**
* Scalability Performance: **100%**
* Edge Case Logic: **100%**
* Security Validation: **83.3%**
* Input Validation: **66.7%**

---

### Real-World SWE-bench (50 samples)

* **98% detection** of production bugs in live open-source projects
* Targets: `django`, `requests`, `pytorch`, `scikit-learn`, `pandas`

---

## Enterprise Features

```python
enterprise_thresholds = {
    'max_critical_vulnerabilities': 0,
    'max_high_vulnerabilities': 1,
    'max_secrets_per_file': 0,
    'crypto_compliance_required': True
}
```

* Risk scoring: LOW / MEDIUM / HIGH / CRITICAL
* Compound risk detection
* Multi-tier caching (<200‚ÄØms)
* Horizontal scaling (stateless agents)
* Offline mode for air-gapped environments
* Integration-ready: GitHub Actions, Jenkins, SonarQube, SIEM

---

## Quick Start

```bash
git clone https://github.com/your‚Äëorg/codex‚Äëverify.git
cd codex‚Äëverify
pip install ‚Äër requirements.txt

python swe_bench_mirror_evaluator.py
python swe_bench_real_evaluator.py

streamlit run ui/streamlit_dashboard.py
```

---

## Project Structure

```
codex-verify/
‚îú‚îÄ‚îÄ src/agents/           # 4 verification agents
‚îú‚îÄ‚îÄ src/orchestration/    # Async orchestration engine
‚îú‚îÄ‚îÄ ui/                   # Streamlit dashboard + CLI
‚îú‚îÄ‚îÄ config/               # YAML config files
‚îú‚îÄ‚îÄ tests/                # Comprehensive test suite
‚îú‚îÄ‚îÄ docs/                 # Technical docs
```

---

## Development & Testing

```bash
python -m pytest tests/ -v
python swe_bench_real_evaluator.py
```

---

## Conclusion

**CodeX‚ÄëVerify** is the first full *multi-agent verification system* designed to close the 40‚Äì60% false positive gap in Codex and other LLM-generated code, validated across SWE‚Äëbench and real-world production issues. It is CI/CD‚Äëready, enterprise-scalable, and built for deployment gating of LLM-based code pipelines.
