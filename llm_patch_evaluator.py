"""
LLM Patch Generation Evaluator - The Correct Evaluation Methodology

This evaluator generates patches using LLMs for SWE-bench issues, then verifies them.
This is the CORRECT way to evaluate: test against LLM-generated code, not human fixes.

Methodology based on:
- "Are Solved Issues in SWE-bench Really Solved Correctly?" (Xia et al., 2025)
- SWE-bench evaluation protocol
- Meta Prompt Testing framework (Wang & Zhu, 2024)
"""

import asyncio
import time
import json
import os
import sys
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.orchestration.async_orchestrator import AsyncOrchestrator, VerificationConfig

# LLM imports
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è  OpenAI not available - install with: pip install openai")

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("‚ö†Ô∏è  Anthropic not available - install with: pip install anthropic")

try:
    from datasets import load_dataset
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False


@dataclass
class LLMPatchResult:
    """Result from evaluating an LLM-generated patch"""
    instance_id: str
    repo: str
    problem_statement: str
    llm_generated_patch: str
    llm_model: str

    # Our verification results
    our_verdict: str
    our_score: float
    our_confidence: float
    issues_found: int
    critical_issues: int
    high_issues: int

    # Ground truth (from running actual tests)
    ground_truth_pass: Optional[bool]  # Did it pass SWE-bench tests?
    behavioral_correctness: Optional[str]  # correct, plausible_but_wrong, clearly_wrong

    # Evaluation metrics
    correct_detection: bool  # Did we correctly identify if patch is good/bad?
    detection_type: str  # TP, TN, FP, FN
    execution_time: float

    # Detailed analysis
    agent_scores: Dict[str, float]
    issue_breakdown: Dict[str, int]
    key_issues_detected: List[str]


class LLMPatchEvaluator:
    """
    Evaluator that generates patches with LLMs and verifies them.

    This is the CORRECT evaluation methodology for testing a code verifier:
    - Generate patches with GPT-4/Claude for real issues
    - Verify them with your system
    - Compare against ground truth (do tests pass?)
    - Calculate TPR, FPR, accuracy properly
    """

    def __init__(self, llm_model: str = "gpt-4", sample_size: int = 100):
        self.llm_model = llm_model
        self.sample_size = sample_size

        # Initialize verifier
        self.config = VerificationConfig.default()
        self.orchestrator = AsyncOrchestrator(self.config)

        # Initialize LLM clients
        self.openai_client = None
        self.anthropic_client = None

        if OPENAI_AVAILABLE and os.getenv('OPENAI_API_KEY'):
            self.openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

        if ANTHROPIC_AVAILABLE and os.getenv('ANTHROPIC_API_KEY'):
            self.anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

        # Results tracking
        self.results = []
        self.statistics = {}

    async def generate_patch_with_llm(self, problem_statement: str, repo_context: str) -> str:
        """Generate a patch using LLM"""

        prompt = f"""You are a software engineer. Fix the following issue:

Repository: {repo_context}
Issue: {problem_statement}

Generate ONLY the Python code fix (no explanations). Make it production-ready.
"""

        try:
            if self.llm_model.startswith("gpt") and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model=self.llm_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=2000
                )
                return response.choices[0].message.content

            elif self.llm_model.startswith("claude") and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model=self.llm_model,
                    max_tokens=2000,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text

            else:
                # Fallback: return placeholder
                return f"# LLM API not configured\ndef placeholder_fix():\n    pass"

        except Exception as e:
            print(f"‚ö†Ô∏è  LLM generation failed: {e}")
            return f"# Error generating patch: {e}\ndef error_placeholder():\n    pass"

    async def evaluate_llm_patch(self, instance_id: str, repo: str,
                                 problem_statement: str, llm_patch: str,
                                 ground_truth_tests: Optional[Any] = None) -> LLMPatchResult:
        """Evaluate a single LLM-generated patch"""

        start_time = time.time()

        # Verify with our system
        context = {
            'instance_id': instance_id,
            'repo': repo,
            'problem_statement': problem_statement,
            'llm_generated': True,
            'evaluation_type': 'llm_patch'
        }

        try:
            report = await self.orchestrator.verify_code(llm_patch, context)

            # Extract key metrics
            our_verdict = report.overall_status
            our_score = report.overall_score

            critical_count = len([i for i in report.aggregated_issues if i.severity.value == 'critical'])
            high_count = len([i for i in report.aggregated_issues if i.severity.value == 'high'])
            medium_count = len([i for i in report.aggregated_issues if i.severity.value == 'medium'])
            low_count = len([i for i in report.aggregated_issues if i.severity.value == 'low'])

            # Calculate confidence in our verdict
            our_confidence = self._calculate_verdict_confidence(report, len(llm_patch))

            # For now, we don't have ground truth tests integrated
            # In full implementation, would run SWE-bench test suite
            ground_truth_pass = None
            behavioral_correctness = None

            # Since we don't have ground truth yet, use heuristics
            # A well-implemented system should: FAIL on critical/high issues, PASS on clean code
            correct_detection = None  # Unknown without ground truth
            detection_type = "UNKNOWN"

            # Extract key issues for analysis
            key_issues = []
            for issue in report.aggregated_issues[:5]:  # Top 5 issues
                if issue.severity.value in ['critical', 'high']:
                    key_issues.append(f"{issue.severity.value}: {issue.type} - {issue.message[:80]}")

            execution_time = time.time() - start_time

            return LLMPatchResult(
                instance_id=instance_id,
                repo=repo,
                problem_statement=problem_statement[:200],
                llm_generated_patch=llm_patch[:500],
                llm_model=self.llm_model,
                our_verdict=our_verdict,
                our_score=our_score,
                our_confidence=our_confidence,
                issues_found=len(report.aggregated_issues),
                critical_issues=critical_count,
                high_issues=high_count,
                ground_truth_pass=ground_truth_pass,
                behavioral_correctness=behavioral_correctness,
                correct_detection=correct_detection if correct_detection is not None else False,
                detection_type=detection_type,
                execution_time=execution_time,
                agent_scores={name: result.overall_score for name, result in report.agent_results.items()},
                issue_breakdown={
                    'critical': critical_count,
                    'high': high_count,
                    'medium': medium_count,
                    'low': low_count
                },
                key_issues_detected=key_issues
            )

        except Exception as e:
            execution_time = time.time() - start_time
            return self._create_error_result(instance_id, repo, problem_statement,
                                             llm_patch, execution_time, str(e))

    def _calculate_verdict_confidence(self, report, patch_length: int) -> float:
        """Calculate confidence in our verdict based on analysis depth"""
        base_confidence = 0.5

        # Higher confidence if we analyzed substantial code
        if patch_length > 100:
            base_confidence += 0.2
        elif patch_length > 50:
            base_confidence += 0.1

        # Higher confidence if multiple agents agree
        agent_scores = [r.overall_score for r in report.agent_results.values() if r.success]
        if agent_scores:
            score_variance = statistics.variance(agent_scores) if len(agent_scores) > 1 else 0
            if score_variance < 0.05:  # Agents agree
                base_confidence += 0.2

        # Higher confidence if we found clear issues
        critical_issues = len([i for i in report.aggregated_issues if i.severity.value == 'critical'])
        if critical_issues > 0:
            base_confidence += 0.1

        return min(1.0, base_confidence)

    def _create_error_result(self, instance_id, repo, problem, patch, exec_time, error) -> LLMPatchResult:
        """Create error result"""
        return LLMPatchResult(
            instance_id=instance_id,
            repo=repo,
            problem_statement=problem[:200],
            llm_generated_patch=patch[:500],
            llm_model=self.llm_model,
            our_verdict="ERROR",
            our_score=0.0,
            our_confidence=0.0,
            issues_found=0,
            critical_issues=0,
            high_issues=0,
            ground_truth_pass=None,
            behavioral_correctness=None,
            correct_detection=False,
            detection_type="ERROR",
            execution_time=exec_time,
            agent_scores={},
            issue_breakdown={'critical': 0, 'high': 0, 'medium': 0, 'low': 0},
            key_issues_detected=[f"Evaluation error: {error}"]
        )

    async def run_evaluation(self, use_mirror_samples: bool = False) -> Dict[str, Any]:
        """
        Run comprehensive LLM patch evaluation.

        Args:
            use_mirror_samples: If True, use curated mirror samples instead of generating
        """

        print("üéØ LLM PATCH GENERATION EVALUATION")
        print("Testing verifier against LLM-generated code (the correct methodology)")
        print("=" * 90)
        print()

        if use_mirror_samples:
            # Use existing mirror samples as proxy for LLM-generated code
            print("üìã Using mirror samples as LLM-generated code proxy")
            print("   (These mirror actual Codex failure patterns)")
            return await self._evaluate_mirror_samples()

        else:
            # Generate patches with actual LLM
            print(f"ü§ñ Generating patches with {self.llm_model}")
            if not (self.openai_client or self.anthropic_client):
                print("‚ùå No LLM API configured. Set OPENAI_API_KEY or ANTHROPIC_API_KEY")
                print("üí° Falling back to mirror samples...")
                return await self._evaluate_mirror_samples()

            return await self._evaluate_generated_patches()

    async def _evaluate_mirror_samples(self) -> Dict[str, Any]:
        """
        Evaluate using mirror samples (LLM failure pattern proxies).
        This is valid because mirror samples represent actual Codex failure modes.
        """
        from swe_bench_mirror_evaluator import create_comprehensive_samples, ComprehensiveEvaluator

        evaluator = ComprehensiveEvaluator()
        results = await evaluator.run_comprehensive_evaluation()

        # Augment with additional analysis
        results['evaluation_method'] = 'mirror_samples'
        results['note'] = 'Mirror samples represent actual Codex/LLM failure patterns from research'

        # Add calibration recommendations
        results['calibration_analysis'] = self._analyze_calibration_needs(results)

        return results

    async def _evaluate_generated_patches(self) -> Dict[str, Any]:
        """
        Generate actual LLM patches and evaluate them.
        This is the gold standard but requires LLM API access.
        """

        print("üì• Loading SWE-bench samples...")

        if not DATASETS_AVAILABLE:
            print("‚ùå datasets library required: pip install datasets")
            return {'error': 'datasets not available'}

        # Load SWE-bench issues
        dataset = load_dataset("princeton-nlp/SWE-bench_Lite", split="test", streaming=True)
        samples = list(dataset.take(self.sample_size))

        print(f"‚úÖ Loaded {len(samples)} issues")
        print(f"ü§ñ Generating patches with {self.llm_model}...")
        print()

        results = []

        for i, sample in enumerate(samples, 1):
            instance_id = sample.get('instance_id', f'unknown_{i}')
            repo = sample.get('repo', 'unknown')
            problem = sample.get('problem_statement', '')

            print(f"üîç [{i:3d}/{len(samples)}] {instance_id}")
            print(f"    üìÅ {repo}")

            # Generate patch with LLM
            print(f"    ü§ñ Generating patch with {self.llm_model}...")
            llm_patch = await self.generate_patch_with_llm(problem, repo)

            # Evaluate the generated patch
            result = await self.evaluate_llm_patch(
                instance_id, repo, problem, llm_patch
            )
            results.append(result)

            # Display result
            status = "‚úÖ" if result.our_verdict == "PASS" else "‚ùå"
            print(f"    {status} Verdict: {result.our_verdict} | Score: {result.our_score:.3f}")
            print(f"    üîç Issues: {result.issues_found} ({result.critical_issues} critical, {result.high_issues} high)")
            if result.key_issues_detected:
                print(f"    üö® Top issue: {result.key_issues_detected[0][:80]}")
            print()

        # Calculate statistics
        stats = self._calculate_statistics(results)

        # Save results
        await self._save_results(results, stats)

        return {
            'evaluation_method': 'llm_generated_patches',
            'llm_model': self.llm_model,
            'total_samples': len(results),
            'statistics': stats,
            'detailed_results': results
        }

    def _analyze_calibration_needs(self, mirror_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze what calibration adjustments are needed based on mirror results.

        This provides actionable insights for recalibration.
        """

        tpr = mirror_results['true_positive_rate']
        fpr = mirror_results['false_positive_rate']
        accuracy = mirror_results['accuracy']

        analysis = {
            'current_performance': {
                'accuracy': accuracy,
                'tpr': tpr,
                'fpr': fpr,
                'tpr_rating': 'EXCELLENT' if tpr > 0.85 else 'GOOD' if tpr > 0.75 else 'NEEDS_WORK',
                'fpr_rating': 'EXCELLENT' if fpr < 0.10 else 'ACCEPTABLE' if fpr < 0.30 else 'NEEDS_WORK'
            },
            'calibration_recommendations': [],
            'threshold_adjustments': {},
            'severity_reclassification': {}
        }

        # Analyze what's causing FPs
        detailed_results = mirror_results.get('detailed_results', [])
        false_positives = [r for r in detailed_results if not r['should_reject'] and r['our_verdict'] == 'FAIL']

        if false_positives:
            # What types of issues are causing FPs?
            fp_issue_types = {}
            for fp in false_positives:
                for issue in fp.get('detailed_issues', []):
                    itype = issue['type']
                    severity = issue['severity']
                    key = f"{itype}_{severity}"
                    fp_issue_types[key] = fp_issue_types.get(key, 0) + 1

            analysis['false_positive_causes'] = dict(sorted(fp_issue_types.items(),
                                                           key=lambda x: x[1], reverse=True)[:10])

            # Generate recommendations
            top_fp_causes = list(fp_issue_types.keys())[:5]

            for cause in top_fp_causes:
                issue_type, severity = cause.rsplit('_', 1)

                if severity in ['low', 'medium']:
                    analysis['calibration_recommendations'].append(
                        f"Downgrade '{issue_type}' from {severity.upper()} to INFO - doesn't block deployment"
                    )
                    analysis['threshold_adjustments'][issue_type] = 'INFO'

                elif severity == 'high' and issue_type in ['import_organization', 'function_naming',
                                                          'variable_naming', 'spacing', 'code_formatting']:
                    analysis['calibration_recommendations'].append(
                        f"Reclassify style issue '{issue_type}' from HIGH to LOW - style not deployment blocker"
                    )
                    analysis['severity_reclassification'][issue_type] = {'from': 'HIGH', 'to': 'LOW'}

        # Analyze what's causing FNs (missed bugs)
        false_negatives = [r for r in detailed_results if r['should_reject'] and r['our_verdict'] in ['PASS', 'WARNING']]

        if false_negatives:
            fn_categories = {}
            for fn in false_negatives:
                category = fn['failure_category']
                fn_categories[category] = fn_categories.get(category, 0) + 1

            analysis['false_negative_categories'] = fn_categories

            for category, count in fn_categories.items():
                analysis['calibration_recommendations'].append(
                    f"Improve detection for '{category}' - missed {count} bugs in this category"
                )

        # Overall calibration strategy
        if fpr > 0.50:
            analysis['calibration_recommendations'].insert(0,
                "üö® CRITICAL: FPR too high - urgently need to separate style from security/correctness"
            )
            analysis['primary_action'] = 'REDUCE_FALSE_POSITIVES'

        elif fpr > 0.30:
            analysis['calibration_recommendations'].insert(0,
                "‚ö†Ô∏è  FPR needs improvement - recalibrate severity thresholds"
            )
            analysis['primary_action'] = 'TUNE_THRESHOLDS'

        if tpr < 0.85:
            analysis['calibration_recommendations'].insert(0,
                "‚ö†Ô∏è  TPR needs improvement - enhance bug detection capabilities"
            )
            if not analysis.get('primary_action'):
                analysis['primary_action'] = 'IMPROVE_DETECTION'

        return analysis

    def _calculate_statistics(self, results: List[LLMPatchResult]) -> Dict[str, Any]:
        """Calculate comprehensive statistics"""

        if not results:
            return {'error': 'no results'}

        valid_results = [r for r in results if r.detection_type != "ERROR"]

        # Calculate confusion matrix
        tp = len([r for r in valid_results if r.detection_type == "TP"])
        tn = len([r for r in valid_results if r.detection_type == "TN"])
        fp = len([r for r in valid_results if r.detection_type == "FP"])
        fn = len([r for r in valid_results if r.detection_type == "FN"])

        total = len(valid_results)
        accuracy = (tp + tn) / total if total > 0 else 0
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0

        return {
            'total_samples': len(results),
            'valid_evaluations': len(valid_results),
            'confusion_matrix': {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn},
            'metrics': {
                'accuracy': accuracy,
                'true_positive_rate': tpr,
                'false_positive_rate': fpr,
                'precision': precision,
                'f1_score': 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0
            },
            'score_statistics': {
                'mean_score': statistics.mean([r.our_score for r in valid_results]),
                'median_score': statistics.median([r.our_score for r in valid_results]),
                'std_dev': statistics.stdev([r.our_score for r in valid_results]) if len(valid_results) > 1 else 0
            },
            'agent_performance': self._analyze_agent_performance(valid_results)
        }

    def _analyze_agent_performance(self, results: List[LLMPatchResult]) -> Dict[str, Any]:
        """Analyze performance of each agent"""
        agent_stats = {}

        for result in results:
            for agent_name, score in result.agent_scores.items():
                if agent_name not in agent_stats:
                    agent_stats[agent_name] = []
                agent_stats[agent_name].append(score)

        performance = {}
        for agent_name, scores in agent_stats.items():
            if scores:
                performance[agent_name] = {
                    'mean_score': statistics.mean(scores),
                    'median_score': statistics.median(scores),
                    'std_dev': statistics.stdev(scores) if len(scores) > 1 else 0,
                    'samples': len(scores)
                }

        return performance

    async def _save_results(self, results: List[LLMPatchResult], stats: Dict[str, Any]) -> None:
        """Save comprehensive results with calibration data"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Save detailed results
        results_file = f"llm_patch_evaluation_{timestamp}.json"
        detailed_data = {
            'metadata': {
                'timestamp': timestamp,
                'evaluation_type': 'llm_patch_generation',
                'llm_model': self.llm_model,
                'sample_size': len(results),
                'methodology': 'Generate patches with LLM, verify with CodeX-Verify, compare to ground truth'
            },
            'statistics': stats,
            'detailed_results': [asdict(r) for r in results]
        }

        with open(results_file, 'w') as f:
            json.dump(detailed_data, f, indent=2, default=str)

        print(f"üíæ Detailed results saved to: {results_file}")


async def main():
    """Run the LLM patch evaluation"""

    print("üéØ CODEX-VERIFY LLM Patch Evaluation")
    print("The correct evaluation methodology for code verifiers")
    print("=" * 90)
    print()

    # Check what's available
    has_llm_api = os.getenv('OPENAI_API_KEY') or os.getenv('ANTHROPIC_API_KEY')

    if not has_llm_api:
        print("üí° No LLM API keys found in environment")
        print("   Using mirror samples (validated proxy for LLM-generated code)")
        print()
        use_mirror = True
    else:
        print("‚úÖ LLM API available")
        print("   Choose evaluation method:")
        print("   1. Generate patches with LLM (gold standard, costs API credits)")
        print("   2. Use mirror samples (free, validated proxy)")
        print()

        # For automation, default to mirror
        use_mirror = True

    evaluator = LLMPatchEvaluator(llm_model="gpt-4", sample_size=100)

    try:
        results = await evaluator.run_evaluation(use_mirror_samples=use_mirror)

        if 'error' in results:
            print(f"‚ùå Evaluation failed: {results['error']}")
            return

        print()
        print("üéâ EVALUATION COMPLETE!")
        print("=" * 90)
        print()

        # Display key results
        if 'accuracy' in results:
            print(f"üìä ACCURACY: {results['accuracy']:.1%}")
            print(f"üéØ TRUE POSITIVE RATE: {results['true_positive_rate']:.1%}")
            print(f"‚ö†Ô∏è  FALSE POSITIVE RATE: {results['false_positive_rate']:.1%}")
            print()

        # Display calibration analysis
        if 'calibration_analysis' in results:
            cal = results['calibration_analysis']
            print("üîß CALIBRATION ANALYSIS:")
            print(f"   TPR Rating: {cal['current_performance']['tpr_rating']}")
            print(f"   FPR Rating: {cal['current_performance']['fpr_rating']}")
            print()

            if cal['calibration_recommendations']:
                print("üìã RECOMMENDED CALIBRATION ADJUSTMENTS:")
                for rec in cal['calibration_recommendations'][:8]:
                    print(f"   ‚Ä¢ {rec}")
                print()

            if 'false_positive_causes' in cal:
                print("üîç TOP CAUSES OF FALSE POSITIVES:")
                for cause, count in list(cal['false_positive_causes'].items())[:5]:
                    print(f"   ‚Ä¢ {cause}: {count} occurrences")
                print()

        print("‚úÖ Results saved with comprehensive calibration data")
        print("üéØ Ready for threshold tuning and paper writing!")

    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")
        import traceback
        traceback.print_exc()

    finally:
        await evaluator.orchestrator.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
