# Claude Patch Generation & Evaluation Report

Generated: 2025-10-12 19:29:18

## Executive Summary

Generated and evaluated **5 patches** using Claude Sonnet 4.5 for real SWE-bench issues.

### Key Metrics
- **Success Rate**: 100.0%
- **Average Verification Score**: 0.440
- **Total Issues Detected**: 59
- **Critical Issues**: 0

## Verdict Distribution
- **FAIL**: 4 samples (80.0%)
- **WARNING**: 1 samples (20.0%)

## Quality Distribution (Our Estimate)
- **LOW**: 4 samples (80.0%)
- **MEDIUM**: 1 samples (20.0%)

## Agent Performance
### Style
- Mean Score: 0.689
- Range: 0.290 - 1.000
- Std Dev: 0.285

### Performance
- Mean Score: 1.000
- Range: 1.000 - 1.000
- Std Dev: 0.000

### Correctness
- Mean Score: 0.729
- Range: 0.450 - 1.000
- Std Dev: 0.220

### Security
- Mean Score: 1.000
- Range: 1.000 - 1.000
- Std Dev: 0.000


## Top Repositories
- **astropy/astropy**: 5 samples (avg score: 0.440, pass rate: 0.0%)

## Performance
- **Total Execution Time**: 102.7 seconds (1.7 minutes)
- **Avg Generation Time**: 17.91s
- **Avg Verification Time**: 0.11s

## Conclusion

Successfully generated and evaluated 5 Claude patches, providing real LLM-generated data for publication-quality evaluation.
