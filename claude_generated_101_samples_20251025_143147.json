[
  {
    "problem_id": "batch1_sample_1",
    "issue_description": "SQL injection vulnerability in database query construction",
    "repo_context": "User authentication system for web application",
    "codex_solution": "import sqlite3\n\ndef authenticate_user(username, password):\n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    # BUG: SQL injection via string formatting\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    cursor.execute(query)\n    result = cursor.fetchone()\n    conn.close()\n    return result is not None",
    "actual_issue": "SQL injection vulnerability allows attackers to bypass authentication with crafted input like username=\"admin'--\"",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch1_sample_2",
    "issue_description": "Race condition in file writing with concurrent access",
    "repo_context": "Multi-threaded logging system",
    "codex_solution": "import os\n\ndef append_to_log(filename, message):\n    # BUG: Race condition - check and write not atomic\n    if os.path.exists(filename):\n        with open(filename, 'a') as f:\n            f.write(message + '\\n')\n    else:\n        with open(filename, 'w') as f:\n            f.write(message + '\\n')\n    return True",
    "actual_issue": "Race condition between exists check and file open can cause data loss or errors in concurrent scenarios",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch1_sample_3",
    "issue_description": "Memory leak from unclosed file handles in loop",
    "repo_context": "Batch file processing system",
    "codex_solution": "def process_files(file_list):\n    results = []\n    for filepath in file_list:\n        # BUG: File handle never closed if exception occurs\n        f = open(filepath, 'r')\n        content = f.read()\n        results.append(len(content))\n        f.close()\n    return results",
    "actual_issue": "File handles leak when exceptions occur before close(), exhausting system resources with many files",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch1_sample_4",
    "issue_description": "Proper input validation and sanitization",
    "repo_context": "Email validation utility",
    "codex_solution": "import re\n\ndef validate_email(email):\n    if not isinstance(email, str):\n        return False\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return re.match(pattern, email) is not None and len(email) <= 254",
    "actual_issue": "None - correct code",
    "failure_category": "security",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch1_sample_5",
    "issue_description": "Command injection via subprocess with shell=True",
    "repo_context": "System administration tool for file operations",
    "codex_solution": "import subprocess\n\ndef compress_file(filename, output):\n    # BUG: Command injection via shell=True and unsanitized input\n    cmd = f\"tar -czf {output} {filename}\"\n    result = subprocess.run(cmd, shell=True, capture_output=True)\n    return result.returncode == 0",
    "actual_issue": "Command injection vulnerability allows arbitrary command execution through malicious filenames like 'file.txt; rm -rf /'",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch1_sample_6",
    "issue_description": "Integer division causing precision loss in financial calculations",
    "repo_context": "Payment processing system",
    "codex_solution": "def calculate_split_payment(total_cents, num_people):\n    # BUG: Integer division loses remainder\n    per_person = total_cents // num_people\n    return [per_person] * num_people",
    "actual_issue": "Remainder is lost in division, causing total to not match when split. For 100 cents / 3 people, returns [33, 33, 33] = 99 cents instead of 100",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch1_sample_7",
    "issue_description": "Proper use of context managers for resource cleanup",
    "repo_context": "Database connection handler",
    "codex_solution": "import sqlite3\n\ndef query_database(db_path, query):\n    with sqlite3.connect(db_path) as conn:\n        cursor = conn.cursor()\n        cursor.execute(query)\n        results = cursor.fetchall()\n    return results",
    "actual_issue": "None - correct code",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch1_sample_8",
    "issue_description": "Infinite recursion from missing base case",
    "repo_context": "Tree traversal algorithm",
    "codex_solution": "def count_nodes(node):\n    if node is None:\n        return 0\n    # BUG: Missing check for circular references\n    left_count = count_nodes(node.left)\n    right_count = count_nodes(node.right)\n    parent_count = count_nodes(node.parent)\n    return 1 + left_count + right_count + parent_count",
    "actual_issue": "Infinite recursion when traversing parent pointer creates circular reference between parent and child nodes",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch1_sample_9",
    "issue_description": "Correct implementation of binary search",
    "repo_context": "Search utility for sorted arrays",
    "codex_solution": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch1_sample_10",
    "issue_description": "Inefficient O(n\u00b2) algorithm when O(n) solution exists",
    "repo_context": "Data deduplication service",
    "codex_solution": "def remove_duplicates(items):\n    result = []\n    for item in items:\n        # BUG: O(n\u00b2) complexity - checking membership in list\n        if item not in result:\n            result.append(item)\n    return result",
    "actual_issue": "Quadratic time complexity due to linear search in list for each item. Should use set for O(1) membership testing",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch2_sample_1",
    "issue_description": "SQL injection vulnerability in user authentication",
    "repo_context": "Authentication module for web application",
    "codex_solution": "def authenticate_user(username, password):\n    import sqlite3\n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    cursor.execute(query)\n    result = cursor.fetchone()\n    conn.close()\n    return result is not None",
    "actual_issue": "SQL injection vulnerability - user input directly concatenated into SQL query",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch2_sample_2",
    "issue_description": "Race condition in file writing",
    "repo_context": "Multi-threaded logging system",
    "codex_solution": "import os\n\ndef write_log(message, log_file='app.log'):\n    if not os.path.exists(log_file):\n        with open(log_file, 'w') as f:\n            f.write('Log started\\n')\n    \n    with open(log_file, 'a') as f:\n        f.write(f'{message}\\n')",
    "actual_issue": "Race condition between checking file existence and writing - multiple threads could create file simultaneously",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch2_sample_3",
    "issue_description": "Memory leak due to circular reference",
    "repo_context": "Event handling system with observers",
    "codex_solution": "class EventEmitter:\n    def __init__(self):\n        self.listeners = []\n    \n    def add_listener(self, callback):\n        self.listeners.append(callback)\n    \n    def emit(self, event):\n        for listener in self.listeners:\n            listener(event, self)",
    "actual_issue": "Listeners that reference the emitter create circular references, no removal mechanism provided",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch2_sample_4",
    "issue_description": "Integer overflow in time calculation",
    "repo_context": "Scheduling system for task execution",
    "codex_solution": "def calculate_delay_ms(hours):\n    \"\"\"Calculate delay in milliseconds\"\"\"\n    seconds = hours * 3600\n    milliseconds = seconds * 1000\n    return int(milliseconds)",
    "actual_issue": "Large hour values can cause integer overflow when converting to milliseconds",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch2_sample_5",
    "issue_description": "Inefficient nested loop causing O(n^3) complexity",
    "repo_context": "Data processing pipeline for large datasets",
    "codex_solution": "def find_duplicates(lists):\n    duplicates = []\n    for i, list1 in enumerate(lists):\n        for j, list2 in enumerate(lists[i+1:]):\n            for item in list1:\n                if item in list2 and item not in duplicates:\n                    duplicates.append(item)\n    return duplicates",
    "actual_issue": "Inefficient O(n^3) algorithm with repeated 'in' checks on list instead of using sets",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch2_sample_6",
    "issue_description": "Proper input validation and sanitization",
    "repo_context": "Email validation utility",
    "codex_solution": "import re\n\ndef validate_email(email):\n    if not email or not isinstance(email, str):\n        return False\n    \n    if len(email) > 254:\n        return False\n    \n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch2_sample_7",
    "issue_description": "Division by zero not handled",
    "repo_context": "Statistical analysis module",
    "codex_solution": "def calculate_average(numbers):\n    \"\"\"Calculate average of a list of numbers\"\"\"\n    total = sum(numbers)\n    count = len(numbers)\n    return total / count",
    "actual_issue": "Division by zero when empty list is passed - no validation",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch2_sample_8",
    "issue_description": "Efficient caching with proper memory management",
    "repo_context": "API response caching layer",
    "codex_solution": "from functools import lru_cache\nimport time\n\n@lru_cache(maxsize=128)\ndef fetch_user_data(user_id):\n    \"\"\"Fetch user data with caching\"\"\"\n    if not isinstance(user_id, int) or user_id <= 0:\n        raise ValueError('Invalid user ID')\n    \n    return {'id': user_id, 'fetched_at': time.time()}",
    "actual_issue": "None - correct code",
    "failure_category": "performance",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch2_sample_9",
    "issue_description": "Hardcoded credentials in source code",
    "repo_context": "Database connection utility",
    "codex_solution": "import psycopg2\n\ndef connect_to_database():\n    conn = psycopg2.connect(\n        host='localhost',\n        database='production_db',\n        user='admin',\n        password='P@ssw0rd123!'\n    )\n    return conn",
    "actual_issue": "Hardcoded database credentials in source code - major security vulnerability",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch2_sample_10",
    "issue_description": "Thread-safe singleton with proper locking",
    "repo_context": "Configuration manager",
    "codex_solution": "import threading\n\nclass Config:\n    _instance = None\n    _lock = threading.Lock()\n    \n    def __new__(cls):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n                    cls._instance.settings = {}\n        return cls._instance",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch3_sample_001",
    "issue_description": "Race condition in cache implementation",
    "repo_context": "High-traffic web service using in-memory cache for session data",
    "codex_solution": "import threading\nimport time\n\nclass SessionCache:\n    def __init__(self):\n        self.cache = {}\n        self.expiry = {}\n    \n    def set(self, key, value, ttl=300):\n        self.cache[key] = value\n        self.expiry[key] = time.time() + ttl\n    \n    def get(self, key):\n        if key in self.expiry and time.time() > self.expiry[key]:\n            del self.cache[key]\n            del self.expiry[key]\n            return None\n        return self.cache.get(key)",
    "actual_issue": "Check-then-act race condition: multiple threads can simultaneously check expiry and delete, causing KeyError. No locking mechanism.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch3_sample_002",
    "issue_description": "SQL injection vulnerability in search endpoint",
    "repo_context": "E-commerce API with product search functionality",
    "codex_solution": "from flask import request, jsonify\nimport sqlite3\n\n@app.route('/api/search')\ndef search_products():\n    query = request.args.get('q', '')\n    category = request.args.get('category', 'all')\n    \n    conn = sqlite3.connect('products.db')\n    cursor = conn.cursor()\n    \n    sql = f\"SELECT * FROM products WHERE name LIKE '%{query}%' AND category = '{category}'\"\n    results = cursor.execute(sql).fetchall()\n    \n    return jsonify(results)",
    "actual_issue": "Direct string interpolation in SQL query allows SQL injection attacks through query and category parameters",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch3_sample_003",
    "issue_description": "Memory leak in file processing pipeline",
    "repo_context": "Batch processing system handling large CSV files",
    "codex_solution": "import csv\n\nclass DataProcessor:\n    def __init__(self):\n        self.processed_data = []\n    \n    def process_file(self, filepath):\n        with open(filepath, 'r') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                processed = self.transform(row)\n                self.processed_data.append(processed)\n        return len(self.processed_data)\n    \n    def transform(self, row):\n        return {k: v.strip().upper() for k, v in row.items()}",
    "actual_issue": "Unbounded memory growth: processed_data accumulates across multiple file processing calls without clearing, causing memory exhaustion",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch3_sample_004",
    "issue_description": "Proper input validation and sanitization",
    "repo_context": "User registration system with validation",
    "codex_solution": "import re\nfrom werkzeug.security import generate_password_hash\n\ndef register_user(username, email, password):\n    if not username or len(username) < 3 or len(username) > 50:\n        raise ValueError('Invalid username length')\n    \n    if not re.match(r'^[a-zA-Z0-9_]+$', username):\n        raise ValueError('Username contains invalid characters')\n    \n    if not re.match(r'^[\\w.-]+@[\\w.-]+\\.\\w+$', email):\n        raise ValueError('Invalid email format')\n    \n    if len(password) < 8:\n        raise ValueError('Password too short')\n    \n    hashed = generate_password_hash(password)\n    return {'username': username, 'email': email, 'password': hashed}",
    "actual_issue": "None - correct code with proper validation and password hashing",
    "failure_category": "security",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch3_sample_005",
    "issue_description": "Integer overflow in financial calculations",
    "repo_context": "Payment processing system calculating transaction totals",
    "codex_solution": "def calculate_total(items):\n    total_cents = 0\n    for item in items:\n        price_cents = int(item['price'] * 100)\n        quantity = item['quantity']\n        total_cents += price_cents * quantity\n    \n    return total_cents / 100.0\n\ndef apply_discount(total, discount_percent):\n    discount = int(total * discount_percent)\n    return total - discount",
    "actual_issue": "Integer overflow risk when multiplying large prices by quantities, and precision loss from int conversion in discount calculation",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch3_sample_006",
    "issue_description": "Deadlock in resource allocation",
    "repo_context": "Database connection pool manager with resource locking",
    "codex_solution": "import threading\n\nclass ConnectionPool:\n    def __init__(self):\n        self.lock_a = threading.Lock()\n        self.lock_b = threading.Lock()\n        self.connections = []\n        self.available = []\n    \n    def allocate(self, conn_id):\n        with self.lock_a:\n            with self.lock_b:\n                self.connections.append(conn_id)\n    \n    def release(self, conn_id):\n        with self.lock_b:\n            with self.lock_a:\n                self.connections.remove(conn_id)",
    "actual_issue": "Classic deadlock: allocate acquires lock_a then lock_b, while release acquires lock_b then lock_a, creating circular wait condition",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch3_sample_007",
    "issue_description": "Proper exception handling with cleanup",
    "repo_context": "File upload service with temporary file management",
    "codex_solution": "import os\nimport tempfile\nimport shutil\n\ndef process_upload(file_data):\n    temp_dir = tempfile.mkdtemp()\n    try:\n        temp_path = os.path.join(temp_dir, 'upload.dat')\n        with open(temp_path, 'wb') as f:\n            f.write(file_data)\n        \n        result = validate_and_process(temp_path)\n        return result\n    finally:\n        shutil.rmtree(temp_dir, ignore_errors=True)\n\ndef validate_and_process(path):\n    with open(path, 'rb') as f:\n        return len(f.read())",
    "actual_issue": "None - correct code with proper resource cleanup in finally block",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch3_sample_008",
    "issue_description": "Time-of-check to time-of-use vulnerability",
    "repo_context": "File permission checker for secure file operations",
    "codex_solution": "import os\nimport stat\n\ndef secure_read_file(filepath):\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f'{filepath} not found')\n    \n    file_stat = os.stat(filepath)\n    if file_stat.st_mode & stat.S_IROTH:\n        raise PermissionError('File is world-readable')\n    \n    with open(filepath, 'r') as f:\n        return f.read()",
    "actual_issue": "TOCTOU vulnerability: file permissions or symlink target can change between stat check and open, allowing unauthorized access",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch3_sample_009",
    "issue_description": "Off-by-one error in pagination",
    "repo_context": "API endpoint returning paginated results",
    "codex_solution": "def paginate_results(items, page, page_size=20):\n    if page < 1:\n        page = 1\n    \n    start_idx = (page - 1) * page_size\n    end_idx = start_idx + page_size\n    \n    total_pages = len(items) // page_size\n    \n    return {\n        'items': items[start_idx:end_idx],\n        'page': page,\n        'total_pages': total_pages,\n        'has_next': page < total_pages\n    }",
    "actual_issue": "Off-by-one in total_pages calculation: doesn't account for remainder, causing last partial page to be ignored",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch3_sample_010",
    "issue_description": "Proper use of context managers and generators",
    "repo_context": "Log file analyzer processing large files efficiently",
    "codex_solution": "import gzip\nfrom contextlib import contextmanager\n\n@contextmanager\ndef open_log_file(filepath):\n    if filepath.endswith('.gz'):\n        f = gzip.open(filepath, 'rt')\n    else:\n        f = open(filepath, 'r')\n    try:\n        yield f\n    finally:\n        f.close()\n\ndef parse_logs(filepath):\n    with open_log_file(filepath) as f:\n        for line in f:\n            if 'ERROR' in line:\n                yield line.strip()",
    "actual_issue": "None - correct code using context managers and generators for memory-efficient processing",
    "failure_category": "performance",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch4_sample_1",
    "issue_description": "Race condition in distributed lock implementation allows multiple processes to acquire same lock",
    "repo_context": "Microservices architecture using Redis for distributed locking to prevent duplicate job execution",
    "codex_solution": "import redis\nimport time\nimport uuid\n\nclass DistributedLock:\n    def __init__(self, redis_client, key, timeout=10):\n        self.redis = redis_client\n        self.key = f'lock:{key}'\n        self.timeout = timeout\n        self.token = str(uuid.uuid4())\n    \n    def acquire(self):\n        # BUG: Check-then-act race condition\n        if self.redis.get(self.key) is None:\n            time.sleep(0.001)  # Simulates network delay\n            self.redis.setex(self.key, self.timeout, self.token)\n            return True\n        return False\n    \n    def release(self):\n        stored_token = self.redis.get(self.key)\n        if stored_token == self.token:\n            self.redis.delete(self.key)",
    "actual_issue": "Race condition between checking if lock exists and setting it. Multiple processes can pass the None check simultaneously before any sets the lock, causing all to acquire the lock. Should use SETNX or SET NX atomic operation.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch4_sample_2",
    "issue_description": "SQL injection vulnerability in dynamic ORDER BY clause construction",
    "repo_context": "E-commerce API allowing users to sort product listings by various fields",
    "codex_solution": "def get_products(category, sort_by='price', order='asc'):\n    \"\"\"Fetch products with user-specified sorting\"\"\"\n    allowed_categories = ['electronics', 'clothing', 'books']\n    \n    if category not in allowed_categories:\n        raise ValueError('Invalid category')\n    \n    # BUG: SQL injection in ORDER BY clause\n    query = f\"\"\"\n        SELECT id, name, price, rating \n        FROM products \n        WHERE category = %s\n        ORDER BY {sort_by} {order}\n    \"\"\"\n    \n    return db.execute(query, (category,)).fetchall()",
    "actual_issue": "SQL injection vulnerability: sort_by and order parameters are directly interpolated into SQL without validation. Attacker can inject malicious SQL like sort_by='(CASE WHEN (SELECT password FROM users WHERE id=1) LIKE \\'a%\\' THEN 1 ELSE 2 END)'. Should use whitelist validation.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch4_sample_3",
    "issue_description": "Memory leak in event listener registration without cleanup mechanism",
    "repo_context": "Real-time dashboard that subscribes to data updates and refreshes UI components",
    "codex_solution": "class DataDashboard:\n    def __init__(self, event_bus):\n        self.event_bus = event_bus\n        self.widgets = []\n        self.listeners = []\n    \n    def add_widget(self, widget_config):\n        widget = Widget(widget_config)\n        self.widgets.append(widget)\n        \n        # BUG: Listener accumulation without cleanup\n        def update_handler(data):\n            widget.update(data)\n        \n        self.event_bus.on(widget_config['data_source'], update_handler)\n        self.listeners.append(update_handler)\n    \n    def remove_widget(self, widget_id):\n        self.widgets = [w for w in self.widgets if w.id != widget_id]\n        # BUG: Listeners never removed from event_bus",
    "actual_issue": "Memory leak: event listeners are registered but never unregistered when widgets are removed. Each listener holds references to widget objects, preventing garbage collection. Over time with widget additions/removals, memory grows unbounded.",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch4_sample_4",
    "issue_description": "Correct implementation of retry logic with exponential backoff for API calls",
    "repo_context": "Service integration layer that handles transient failures when calling external APIs",
    "codex_solution": "import time\nimport random\nfrom functools import wraps\n\ndef retry_with_backoff(max_retries=3, base_delay=1, max_delay=32):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except (ConnectionError, TimeoutError) as e:\n                    if attempt == max_retries - 1:\n                        raise\n                    delay = min(base_delay * (2 ** attempt), max_delay)\n                    jitter = random.uniform(0, delay * 0.1)\n                    time.sleep(delay + jitter)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch4_sample_5",
    "issue_description": "Integer overflow in timestamp calculation causes incorrect expiration logic",
    "repo_context": "Session management system that calculates token expiration times",
    "codex_solution": "import time\n\nclass SessionManager:\n    MAX_SESSION_DURATION = 2147483647  # Max 32-bit int\n    \n    def create_session(self, user_id, duration_seconds=None):\n        current_time = int(time.time())\n        \n        if duration_seconds is None:\n            duration_seconds = self.MAX_SESSION_DURATION\n        \n        # BUG: Integer overflow on 32-bit systems or with large timestamps\n        expiration = current_time + duration_seconds\n        \n        session = {\n            'user_id': user_id,\n            'created_at': current_time,\n            'expires_at': expiration\n        }\n        return session\n    \n    def is_valid(self, session):\n        return int(time.time()) < session['expires_at']",
    "actual_issue": "Integer overflow: adding MAX_SESSION_DURATION to current Unix timestamp (already ~2 billion) causes overflow on systems with 32-bit integers or when stored in 32-bit database fields, resulting in negative or wrapped values that break expiration logic.",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch4_sample_6",
    "issue_description": "Inefficient N+1 query pattern in ORM usage causes performance degradation",
    "repo_context": "Blog platform displaying posts with author information and comment counts",
    "codex_solution": "def get_blog_feed(page=1, per_page=20):\n    \"\"\"Get paginated blog posts with metadata\"\"\"\n    offset = (page - 1) * per_page\n    \n    # BUG: N+1 query problem\n    posts = db.query(Post).limit(per_page).offset(offset).all()\n    \n    results = []\n    for post in posts:\n        # Each iteration triggers separate queries\n        author = db.query(User).filter(User.id == post.author_id).first()\n        comment_count = db.query(Comment).filter(Comment.post_id == post.id).count()\n        \n        results.append({\n            'id': post.id,\n            'title': post.title,\n            'author_name': author.name,\n            'comments': comment_count\n        })\n    \n    return results",
    "actual_issue": "N+1 query problem: for each post, two additional database queries are executed (author lookup and comment count). With 20 posts, this results in 41 queries instead of 1-3. Should use eager loading with joins or select_related/prefetch_related.",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch4_sample_7",
    "issue_description": "Correct implementation of connection pooling with proper resource management",
    "repo_context": "Database access layer for high-traffic web application requiring efficient connection reuse",
    "codex_solution": "import psycopg2\nfrom psycopg2 import pool\nfrom contextlib import contextmanager\n\nclass DatabasePool:\n    def __init__(self, minconn=1, maxconn=10, **db_params):\n        self.pool = pool.ThreadedConnectionPool(minconn, maxconn, **db_params)\n    \n    @contextmanager\n    def get_connection(self):\n        conn = self.pool.getconn()\n        try:\n            yield conn\n            conn.commit()\n        except Exception:\n            conn.rollback()\n            raise\n        finally:\n            self.pool.putconn(conn)\n    \n    def close_all(self):\n        self.pool.closeall()",
    "actual_issue": "None - correct code",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch4_sample_8",
    "issue_description": "Path traversal vulnerability in file download endpoint allows unauthorized access",
    "repo_context": "Document management system providing file download functionality to authenticated users",
    "codex_solution": "import os\nfrom flask import send_file, abort\n\nDOCUMENT_ROOT = '/var/app/documents'\n\ndef download_file(user_id, filename):\n    \"\"\"Download user document by filename\"\"\"\n    # BUG: Path traversal vulnerability\n    user_dir = os.path.join(DOCUMENT_ROOT, str(user_id))\n    file_path = os.path.join(user_dir, filename)\n    \n    if not os.path.exists(file_path):\n        abort(404)\n    \n    if not os.path.isfile(file_path):\n        abort(400)\n    \n    return send_file(file_path, as_attachment=True)",
    "actual_issue": "Path traversal vulnerability: filename parameter is not validated. Attacker can use '../' sequences like filename='../../etc/passwd' to access files outside user directory. Should validate that resolved path stays within intended directory using os.path.abspath and prefix checking.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch4_sample_9",
    "issue_description": "Correct implementation of input validation and sanitization for user data",
    "repo_context": "User profile update API that accepts and validates various user input fields",
    "codex_solution": "import re\nfrom html import escape\n\ndef update_user_profile(user_id, profile_data):\n    \"\"\"Update user profile with validation\"\"\"\n    allowed_fields = {'display_name', 'bio', 'website', 'location'}\n    \n    validated_data = {}\n    for field, value in profile_data.items():\n        if field not in allowed_fields:\n            continue\n        \n        if not isinstance(value, str):\n            raise ValueError(f'Invalid type for {field}')\n        \n        value = value.strip()\n        if len(value) > 500:\n            raise ValueError(f'{field} too long')\n        \n        if field == 'website':\n            if value and not re.match(r'^https?://', value):\n                raise ValueError('Invalid URL')\n        \n        validated_data[field] = escape(value)\n    \n    return db.update_profile(user_id, validated_data)",
    "actual_issue": "None - correct code",
    "failure_category": "security",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch4_sample_10",
    "issue_description": "Deadlock potential in multi-resource locking with inconsistent lock ordering",
    "repo_context": "Banking system that transfers money between accounts requiring locks on both accounts",
    "codex_solution": "import threading\n\nclass BankAccount:\n    def __init__(self, account_id, balance):\n        self.account_id = account_id\n        self.balance = balance\n        self.lock = threading.Lock()\n\ndef transfer_money(from_account, to_account, amount):\n    \"\"\"Transfer money between accounts\"\"\"\n    # BUG: Inconsistent lock ordering causes deadlock\n    with from_account.lock:\n        if from_account.balance < amount:\n            raise ValueError('Insufficient funds')\n        \n        with to_account.lock:\n            from_account.balance -= amount\n            to_account.balance += amount\n            return True",
    "actual_issue": "Deadlock potential: when thread A transfers from account1 to account2 while thread B transfers from account2 to account1, they acquire locks in opposite order causing deadlock. Should always acquire locks in consistent order (e.g., by account_id) regardless of transfer direction.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch5_sample_1",
    "issue_description": "Race condition in cache implementation with TTL",
    "repo_context": "High-traffic web service using in-memory cache with expiration",
    "codex_solution": "import time\nimport threading\n\nclass Cache:\n    def __init__(self):\n        self.data = {}\n        self.expiry = {}\n    \n    def set(self, key, value, ttl=60):\n        self.data[key] = value\n        self.expiry[key] = time.time() + ttl\n    \n    def get(self, key):\n        if key in self.data:\n            if time.time() < self.expiry[key]:\n                return self.data[key]\n            del self.data[key]\n            del self.expiry[key]\n        return None",
    "actual_issue": "Race condition: multiple threads can simultaneously check and delete expired entries, causing KeyError. No thread synchronization for concurrent access.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch5_sample_2",
    "issue_description": "File upload handler with path traversal vulnerability",
    "repo_context": "Web application allowing users to upload profile pictures",
    "codex_solution": "import os\nfrom flask import request\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    file = request.files['file']\n    filename = file.filename\n    \n    if not filename.endswith(('.jpg', '.png')):\n        return 'Invalid file type', 400\n    \n    upload_dir = '/var/www/uploads'\n    filepath = os.path.join(upload_dir, filename)\n    file.save(filepath)\n    return 'File uploaded', 200",
    "actual_issue": "Path traversal vulnerability: filename like '../../../etc/passwd.jpg' bypasses directory restriction. No sanitization of user-provided filename.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch5_sample_3",
    "issue_description": "Decimal precision loss in financial calculations",
    "repo_context": "Payment processing system calculating transaction fees",
    "codex_solution": "def calculate_fee(amount, rate=0.029):\n    fee = amount * rate\n    total = amount + fee\n    return round(total, 2)\n\ndef process_payment(amount):\n    fee = calculate_fee(amount)\n    net_amount = amount\n    gross_amount = fee\n    \n    return {\n        'net': net_amount,\n        'fee': gross_amount - net_amount,\n        'gross': gross_amount\n    }",
    "actual_issue": "Using float arithmetic for money causes precision errors. Should use Decimal type. Also, logic error in process_payment where fee assignment is incorrect.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch5_sample_4",
    "issue_description": "Proper implementation of retry logic with exponential backoff",
    "repo_context": "HTTP client for external API calls with fault tolerance",
    "codex_solution": "import time\nimport requests\nfrom requests.exceptions import RequestException\n\ndef fetch_data(url, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=5)\n            response.raise_for_status()\n            return response.json()\n        except RequestException as e:\n            if attempt == max_retries - 1:\n                raise\n            wait_time = (2 ** attempt) + (time.time() % 1)\n            time.sleep(wait_time)\n    return None",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch5_sample_5",
    "issue_description": "Inefficient algorithm causing quadratic time complexity",
    "repo_context": "Data processing pipeline handling large datasets",
    "codex_solution": "def remove_duplicates(items):\n    result = []\n    for item in items:\n        if item not in result:\n            result.append(item)\n    return result\n\ndef process_records(records):\n    unique_records = remove_duplicates(records)\n    cleaned = []\n    for record in unique_records:\n        if record not in cleaned:\n            cleaned.append(record.strip().lower())\n    return cleaned",
    "actual_issue": "O(n\u00b2) time complexity due to repeated 'in' checks on lists. Should use set for O(n) performance. Critical for large datasets.",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch5_sample_6",
    "issue_description": "JWT token validation with proper signature verification",
    "repo_context": "Authentication middleware for REST API",
    "codex_solution": "import jwt\nfrom functools import wraps\nfrom flask import request, jsonify\n\nSECRET_KEY = 'your-secret-key'\n\ndef require_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        token = request.headers.get('Authorization', '').replace('Bearer ', '')\n        if not token:\n            return jsonify({'error': 'No token'}), 401\n        try:\n            payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])\n            request.user = payload\n        except jwt.InvalidTokenError:\n            return jsonify({'error': 'Invalid token'}), 401\n        return f(*args, **kwargs)\n    return decorated",
    "actual_issue": "None - correct code",
    "failure_category": "security",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch5_sample_7",
    "issue_description": "Command injection via subprocess without input sanitization",
    "repo_context": "System administration tool for log file analysis",
    "codex_solution": "import subprocess\n\ndef search_logs(pattern, log_file):\n    if not pattern or not log_file:\n        return []\n    \n    cmd = f'grep \"{pattern}\" {log_file}'\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    \n    if result.returncode == 0:\n        return result.stdout.split('\\n')\n    return []",
    "actual_issue": "Command injection vulnerability: unsanitized pattern and log_file parameters passed to shell. Attacker can inject arbitrary commands using shell metacharacters.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch5_sample_8",
    "issue_description": "Off-by-one error in pagination logic",
    "repo_context": "REST API endpoint for paginated search results",
    "codex_solution": "def paginate(items, page, page_size=10):\n    total_items = len(items)\n    total_pages = total_items // page_size\n    \n    if page < 1 or page > total_pages:\n        return []\n    \n    start_idx = (page - 1) * page_size\n    end_idx = start_idx + page_size\n    \n    return items[start_idx:end_idx]",
    "actual_issue": "Off-by-one error: total_pages calculation doesn't account for remainder. When total_items=25 and page_size=10, total_pages=2 but should be 3. Last page items are inaccessible.",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch5_sample_9",
    "issue_description": "Thread pool with proper resource cleanup",
    "repo_context": "Background task processor for email sending service",
    "codex_solution": "from concurrent.futures import ThreadPoolExecutor\nimport atexit\n\nclass TaskProcessor:\n    def __init__(self, max_workers=5):\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n        atexit.register(self.shutdown)\n    \n    def submit_task(self, func, *args, **kwargs):\n        return self.executor.submit(func, *args, **kwargs)\n    \n    def shutdown(self, wait=True):\n        self.executor.shutdown(wait=wait)",
    "actual_issue": "None - correct code",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch5_sample_10",
    "issue_description": "Memory leak in event listener registration",
    "repo_context": "Event-driven architecture with publisher-subscriber pattern",
    "codex_solution": "class EventBus:\n    def __init__(self):\n        self.listeners = {}\n    \n    def subscribe(self, event_type, callback):\n        if event_type not in self.listeners:\n            self.listeners[event_type] = []\n        self.listeners[event_type].append(callback)\n    \n    def publish(self, event_type, data):\n        if event_type in self.listeners:\n            for callback in self.listeners[event_type]:\n                callback(data)\n    \n    def clear_event(self, event_type):\n        if event_type in self.listeners:\n            self.listeners[event_type] = []",
    "actual_issue": "Memory leak: no unsubscribe mechanism. Listeners accumulate indefinitely, holding references to objects that should be garbage collected. Especially problematic with lambda closures.",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch6_sample_1",
    "issue_description": "SQL injection vulnerability in user authentication",
    "repo_context": "Authentication service for web application",
    "codex_solution": "import sqlite3\n\ndef authenticate_user(username, password):\n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    # BUG: Direct string interpolation allows SQL injection\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    cursor.execute(query)\n    result = cursor.fetchone()\n    conn.close()\n    return result is not None",
    "actual_issue": "SQL injection vulnerability through string interpolation in query construction",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch6_sample_2",
    "issue_description": "Race condition in file-based counter",
    "repo_context": "Multi-threaded request counter for analytics",
    "codex_solution": "import os\n\ndef increment_counter(counter_file):\n    # BUG: Race condition - read and write not atomic\n    if os.path.exists(counter_file):\n        with open(counter_file, 'r') as f:\n            count = int(f.read())\n    else:\n        count = 0\n    \n    count += 1\n    \n    with open(counter_file, 'w') as f:\n        f.write(str(count))\n    \n    return count",
    "actual_issue": "Race condition between read and write operations in multi-threaded environment",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch6_sample_3",
    "issue_description": "Inefficient nested loop for data matching",
    "repo_context": "Data processing pipeline for large datasets",
    "codex_solution": "def match_transactions(orders, payments):\n    matched = []\n    # BUG: O(n*m) complexity, should use hash map\n    for order in orders:\n        for payment in payments:\n            if order['order_id'] == payment['order_id']:\n                matched.append({'order': order, 'payment': payment})\n                break\n    return matched",
    "actual_issue": "O(n*m) time complexity when O(n+m) is achievable with hash map",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch6_sample_4",
    "issue_description": "Division by zero in statistical calculation",
    "repo_context": "Analytics module for calculating averages",
    "codex_solution": "def calculate_average(numbers):\n    # BUG: No check for empty list\n    total = sum(numbers)\n    count = len(numbers)\n    return total / count\n\ndef get_stats(data):\n    filtered = [x for x in data if x > 0]\n    avg = calculate_average(filtered)\n    return {'average': avg, 'count': len(filtered)}",
    "actual_issue": "Division by zero when filtered list is empty",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch6_sample_5",
    "issue_description": "Memory leak through unclosed file handles",
    "repo_context": "Log processing utility for large log files",
    "codex_solution": "def process_logs(log_files):\n    results = []\n    for log_file in log_files:\n        # BUG: File not closed if exception occurs\n        f = open(log_file, 'r')\n        lines = f.readlines()\n        for line in lines:\n            if 'ERROR' in line:\n                results.append(line.strip())\n        f.close()\n    return results",
    "actual_issue": "File handles not closed on exception, causing resource leak",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch6_sample_6",
    "issue_description": "Deserialization vulnerability with pickle",
    "repo_context": "Session management for web application",
    "codex_solution": "import pickle\nimport base64\nfrom flask import request\n\ndef load_session():\n    session_data = request.cookies.get('session')\n    if session_data:\n        # BUG: Unsafe deserialization of user-controlled data\n        decoded = base64.b64decode(session_data)\n        return pickle.loads(decoded)\n    return {}",
    "actual_issue": "Arbitrary code execution through pickle deserialization of untrusted data",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch6_sample_7",
    "issue_description": "Integer overflow in timestamp calculation",
    "repo_context": "Time-based token generation system",
    "codex_solution": "import time\n\ndef generate_expiry_token(duration_days):\n    # BUG: No validation on duration_days, can overflow\n    current_time = int(time.time())\n    expiry_time = current_time + (duration_days * 24 * 60 * 60)\n    token = f\"TOKEN_{expiry_time}\"\n    return token, expiry_time",
    "actual_issue": "Integer overflow possible with large duration_days values",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch6_sample_8",
    "issue_description": "Correct implementation of binary search",
    "repo_context": "Search utility for sorted arrays",
    "codex_solution": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch6_sample_9",
    "issue_description": "Proper file handling with context manager",
    "repo_context": "Configuration file reader",
    "codex_solution": "import json\n\ndef read_config(config_path):\n    try:\n        with open(config_path, 'r') as f:\n            config = json.load(f)\n        return config\n    except FileNotFoundError:\n        return {}\n    except json.JSONDecodeError:\n        return {}",
    "actual_issue": "None - correct code",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch6_sample_10",
    "issue_description": "Efficient duplicate removal using set",
    "repo_context": "Data cleaning utility",
    "codex_solution": "def remove_duplicates(items):\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n\ndef clean_data(data_list):\n    unique_data = remove_duplicates(data_list)\n    return [x for x in unique_data if x is not None]",
    "actual_issue": "None - correct code",
    "failure_category": "performance",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch7_sample_1",
    "issue_description": "Database connection pool exhaustion due to connections never being returned",
    "repo_context": "Web application with database connection pooling for high-traffic API endpoints",
    "codex_solution": "class DatabasePool:\n    def __init__(self, max_connections=10):\n        self.pool = [self._create_connection() for _ in range(max_connections)]\n        self.available = list(self.pool)\n    \n    def _create_connection(self):\n        return {'id': id(object()), 'active': False}\n    \n    def get_connection(self):\n        if not self.available:\n            raise Exception('No connections available')\n        conn = self.available.pop(0)\n        conn['active'] = True\n        return conn\n    \n    def release_connection(self, conn):\n        conn['active'] = False\n        self.available.append(conn)\n\ndef fetch_user_data(user_id):\n    pool = DatabasePool()\n    conn = pool.get_connection()\n    # BUG: Connection never released, pool exhausts quickly\n    return {'user_id': user_id, 'data': 'some_data'}",
    "actual_issue": "Connection is acquired but never released back to the pool, causing pool exhaustion after 10 requests",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch7_sample_2",
    "issue_description": "Race condition in cache implementation allows stale data to overwrite fresh data",
    "repo_context": "Multi-threaded caching layer for distributed system with concurrent read/write operations",
    "codex_solution": "import time\nimport threading\n\nclass Cache:\n    def __init__(self):\n        self.data = {}\n        self.timestamps = {}\n    \n    def set(self, key, value):\n        timestamp = time.time()\n        # BUG: Race condition - no locking between timestamp check and write\n        if key in self.timestamps and self.timestamps[key] > timestamp:\n            return\n        time.sleep(0.001)  # Simulates processing delay\n        self.data[key] = value\n        self.timestamps[key] = timestamp\n    \n    def get(self, key):\n        return self.data.get(key)\n\ndef update_cache_concurrently(cache, key):\n    for i in range(5):\n        cache.set(key, f'value_{i}')\n        time.sleep(0.002)",
    "actual_issue": "Race condition between timestamp check and data write allows older values to overwrite newer ones in concurrent scenarios",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch7_sample_3",
    "issue_description": "SQL injection vulnerability in user authentication function",
    "repo_context": "Legacy authentication system being migrated to modern framework with user login endpoints",
    "codex_solution": "def authenticate_user(username, password):\n    \"\"\"Authenticate user against database\"\"\"\n    import sqlite3\n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    \n    # BUG: SQL injection vulnerability - direct string interpolation\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    cursor.execute(query)\n    result = cursor.fetchone()\n    \n    conn.close()\n    return result is not None\n\ndef login(username, password):\n    if authenticate_user(username, password):\n        return {'status': 'success', 'user': username}\n    return {'status': 'failed'}",
    "actual_issue": "SQL injection vulnerability allows attackers to bypass authentication with inputs like: username=\"admin' OR '1'='1\"",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch7_sample_4",
    "issue_description": "Integer overflow in financial calculation causing incorrect monetary values",
    "repo_context": "Financial processing system handling large transaction amounts and currency conversions",
    "codex_solution": "def calculate_compound_interest(principal, rate, years):\n    \"\"\"Calculate compound interest for investment\"\"\"\n    # BUG: Using integer division and multiplication causes precision loss\n    amount = principal\n    for _ in range(years):\n        interest = (amount * rate) // 100\n        amount = amount + interest\n    return amount\n\ndef process_investment(principal, annual_rate, years):\n    final_amount = calculate_compound_interest(principal, annual_rate, years)\n    profit = final_amount - principal\n    return {'initial': principal, 'final': final_amount, 'profit': profit}",
    "actual_issue": "Integer division causes loss of fractional cents in each iteration, accumulating to significant errors over time",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch7_sample_5",
    "issue_description": "Correctly implemented binary search with proper bounds checking",
    "repo_context": "Search utility library for sorted data structures used across multiple microservices",
    "codex_solution": "def binary_search(arr, target):\n    \"\"\"Perform binary search on sorted array\"\"\"\n    if not arr:\n        return -1\n    \n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = left + (right - left) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\ndef find_element(sorted_list, element):\n    index = binary_search(sorted_list, element)\n    return {'found': index != -1, 'index': index}",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch7_sample_6",
    "issue_description": "Quadratic complexity in nested loop causes timeout on large datasets",
    "repo_context": "Data processing pipeline handling millions of records for analytics platform",
    "codex_solution": "def find_duplicate_pairs(items):\n    \"\"\"Find all pairs of duplicate items\"\"\"\n    duplicates = []\n    # BUG: O(n\u00b2) complexity - should use hash set for O(n)\n    for i in range(len(items)):\n        for j in range(i + 1, len(items)):\n            if items[i] == items[j]:\n                duplicates.append((i, j))\n    return duplicates\n\ndef process_large_dataset(data):\n    # This will timeout on large inputs\n    result = find_duplicate_pairs(data)\n    return {'duplicate_count': len(result), 'pairs': result}",
    "actual_issue": "Quadratic time complexity causes severe performance degradation on large datasets, should use hash-based approach",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch7_sample_7",
    "issue_description": "Off-by-one error in array slicing causes missing last element",
    "repo_context": "Data transformation utility for batch processing system handling CSV imports",
    "codex_solution": "def chunk_array(arr, chunk_size):\n    \"\"\"Split array into chunks of specified size\"\"\"\n    chunks = []\n    # BUG: range excludes last chunk if not perfectly divisible\n    for i in range(0, len(arr) - 1, chunk_size):\n        chunks.append(arr[i:i + chunk_size])\n    return chunks\n\ndef process_in_batches(data, batch_size=10):\n    batches = chunk_array(data, batch_size)\n    results = []\n    for batch in batches:\n        results.extend([x * 2 for x in batch])\n    return results",
    "actual_issue": "Off-by-one error in range causes last elements to be dropped when array length is not perfectly divisible by chunk_size",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch7_sample_8",
    "issue_description": "Properly implemented LRU cache with correct eviction policy",
    "repo_context": "Caching layer for API responses with memory constraints in containerized environment",
    "codex_solution": "from collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n    \n    def get(self, key):\n        if key not in self.cache:\n            return None\n        self.cache.move_to_end(key)\n        return self.cache[key]\n    \n    def put(self, key, value):\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        self.cache[key] = value\n        if len(self.cache) > self.capacity:\n            self.cache.popitem(last=False)\n\ndef cache_api_response(url, response):\n    cache = LRUCache(100)\n    cache.put(url, response)\n    return cache.get(url)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch7_sample_9",
    "issue_description": "Unbounded recursion without base case validation causes stack overflow",
    "repo_context": "Tree traversal utility for hierarchical data structures in document management system",
    "codex_solution": "def traverse_tree(node, callback):\n    \"\"\"Traverse tree and apply callback to each node\"\"\"\n    # BUG: No check for None or circular references\n    callback(node)\n    if hasattr(node, 'children'):\n        for child in node.children:\n            traverse_tree(child, callback)\n\nclass TreeNode:\n    def __init__(self, value):\n        self.value = value\n        self.children = []\n\ndef process_document_tree(root):\n    results = []\n    traverse_tree(root, lambda n: results.append(n.value))\n    return results",
    "actual_issue": "Missing validation for None nodes and circular references can cause stack overflow; no maximum depth limit",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch7_sample_10",
    "issue_description": "Correctly implemented merge sort with proper edge case handling",
    "repo_context": "Sorting utility library for large-scale data processing in analytics pipeline",
    "codex_solution": "def merge_sort(arr):\n    \"\"\"Sort array using merge sort algorithm\"\"\"\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n    \n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    i = j = 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    return result",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch8_sample_1",
    "issue_description": "Race condition in cache invalidation",
    "repo_context": "Multi-threaded web application with shared cache",
    "codex_solution": "import threading\nimport time\n\nclass Cache:\n    def __init__(self):\n        self.data = {}\n        self.timestamps = {}\n    \n    def set(self, key, value, ttl=60):\n        self.data[key] = value\n        self.timestamps[key] = time.time() + ttl\n    \n    def get(self, key):\n        if key in self.data:\n            if time.time() < self.timestamps[key]:\n                return self.data[key]\n            else:\n                del self.data[key]\n                del self.timestamps[key]\n        return None",
    "actual_issue": "Race condition: Multiple threads can simultaneously check expiration and delete, causing KeyError when one thread deletes while another is still accessing timestamps",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch8_sample_2",
    "issue_description": "SQL injection vulnerability in user search",
    "repo_context": "User management system with search functionality",
    "codex_solution": "import sqlite3\n\ndef search_users(db_path, search_term):\n    conn = sqlite3.connect(db_path)\n    cursor = conn.cursor()\n    \n    query = f\"SELECT * FROM users WHERE username LIKE '%{search_term}%' OR email LIKE '%{search_term}%'\"\n    \n    cursor.execute(query)\n    results = cursor.fetchall()\n    conn.close()\n    \n    return results",
    "actual_issue": "SQL injection vulnerability: search_term is directly interpolated into query string without sanitization, allowing malicious input like \"' OR '1'='1\"",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch8_sample_3",
    "issue_description": "Memory leak in event listener registration",
    "repo_context": "Event-driven system with dynamic listener registration",
    "codex_solution": "class EventManager:\n    def __init__(self):\n        self.listeners = {}\n    \n    def register(self, event_type, callback):\n        if event_type not in self.listeners:\n            self.listeners[event_type] = []\n        self.listeners[event_type].append(callback)\n    \n    def trigger(self, event_type, data):\n        if event_type in self.listeners:\n            for callback in self.listeners[event_type]:\n                callback(data)\n    \n    def clear_event(self, event_type):\n        if event_type in self.listeners:\n            self.listeners[event_type] = []",
    "actual_issue": "Memory leak: No unregister method for individual callbacks, only clear_event which removes all. Objects with registered callbacks can't be garbage collected",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch8_sample_4",
    "issue_description": "Proper input validation and sanitization",
    "repo_context": "Email validation utility for user registration",
    "codex_solution": "import re\n\ndef validate_email(email):\n    if not email or not isinstance(email, str):\n        return False\n    \n    email = email.strip().lower()\n    \n    pattern = r'^[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$'\n    \n    if not re.match(pattern, email):\n        return False\n    \n    if len(email) > 254:\n        return False\n    \n    return True",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch8_sample_5",
    "issue_description": "Integer overflow in timestamp calculation",
    "repo_context": "Scheduling system calculating future execution times",
    "codex_solution": "import time\n\ndef schedule_task(delay_seconds):\n    current_time = int(time.time())\n    \n    execution_time = current_time + delay_seconds\n    \n    return execution_time\n\ndef calculate_delay_days(days):\n    seconds_per_day = 86400\n    return days * seconds_per_day",
    "actual_issue": "Integer overflow: For large day values (e.g., 1000000 days), the multiplication can exceed system limits on 32-bit systems, and no validation prevents unrealistic delays",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch8_sample_6",
    "issue_description": "Inefficient nested loop with O(n\u00b2) complexity",
    "repo_context": "Data processing pipeline for large datasets",
    "codex_solution": "def find_duplicates(items):\n    duplicates = []\n    \n    for i in range(len(items)):\n        for j in range(i + 1, len(items)):\n            if items[i] == items[j] and items[i] not in duplicates:\n                duplicates.append(items[i])\n    \n    return duplicates",
    "actual_issue": "Performance issue: O(n\u00b2) complexity with nested loops and additional O(n) 'not in' check on list. Should use set-based approach for O(n) complexity",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch8_sample_7",
    "issue_description": "Proper resource cleanup with context manager",
    "repo_context": "File processing utility with proper resource management",
    "codex_solution": "import json\n\ndef read_config(filepath):\n    try:\n        with open(filepath, 'r') as f:\n            config = json.load(f)\n        return config\n    except FileNotFoundError:\n        return {}\n    except json.JSONDecodeError:\n        return {}\n\ndef write_config(filepath, config):\n    with open(filepath, 'w') as f:\n        json.dump(config, f, indent=2)\n    return True",
    "actual_issue": "None - correct code",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch8_sample_8",
    "issue_description": "Division by zero in statistical calculation",
    "repo_context": "Analytics module calculating user engagement metrics",
    "codex_solution": "def calculate_engagement_rate(views, clicks, conversions):\n    click_rate = clicks / views\n    conversion_rate = conversions / clicks\n    \n    engagement_score = (click_rate * 0.4) + (conversion_rate * 0.6)\n    \n    return round(engagement_score * 100, 2)",
    "actual_issue": "Division by zero: No validation for zero values in views or clicks, causing ZeroDivisionError when views=0 or clicks=0",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch8_sample_9",
    "issue_description": "Proper atomic file operations",
    "repo_context": "Configuration manager with safe file updates",
    "codex_solution": "import os\nimport tempfile\nimport json\n\ndef update_config_safe(filepath, updates):\n    with open(filepath, 'r') as f:\n        config = json.load(f)\n    \n    config.update(updates)\n    \n    fd, temp_path = tempfile.mkstemp(dir=os.path.dirname(filepath))\n    try:\n        with os.fdopen(fd, 'w') as f:\n            json.dump(config, f, indent=2)\n        os.replace(temp_path, filepath)\n    except Exception:\n        os.unlink(temp_path)\n        raise\n    \n    return True",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch8_sample_10",
    "issue_description": "Incorrect string comparison for password validation",
    "repo_context": "Authentication system with password verification",
    "codex_solution": "def verify_password(stored_password, provided_password):\n    if stored_password == provided_password:\n        return True\n    return False\n\ndef authenticate_user(username, password, user_db):\n    if username not in user_db:\n        return False\n    \n    stored_pass = user_db[username]['password']\n    \n    return verify_password(stored_pass, password)",
    "actual_issue": "Security vulnerability: Direct string comparison is vulnerable to timing attacks. Should use constant-time comparison (hmac.compare_digest). Also stores plaintext passwords instead of hashes",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch9_sample_1",
    "issue_description": "SQL injection vulnerability in user authentication",
    "repo_context": "Authentication module for web application",
    "codex_solution": "import sqlite3\n\ndef authenticate_user(username, password):\n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    cursor.execute(query)\n    result = cursor.fetchone()\n    conn.close()\n    return result is not None",
    "actual_issue": "SQL injection vulnerability - user input concatenated directly into query",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch9_sample_2",
    "issue_description": "Race condition in file-based counter",
    "repo_context": "Request counter for rate limiting",
    "codex_solution": "import os\n\ndef increment_counter(user_id):\n    filename = f'counter_{user_id}.txt'\n    if os.path.exists(filename):\n        with open(filename, 'r') as f:\n            count = int(f.read())\n    else:\n        count = 0\n    count += 1\n    with open(filename, 'w') as f:\n        f.write(str(count))\n    return count",
    "actual_issue": "Race condition - read and write are not atomic, multiple threads can corrupt counter",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch9_sample_3",
    "issue_description": "Integer overflow in financial calculation",
    "repo_context": "Payment processing system",
    "codex_solution": "def calculate_total_with_interest(principal, rate, years):\n    total = principal\n    for _ in range(years):\n        total = total * (1 + rate)\n    return int(total * 100)\n\ndef process_payment(amount_cents):\n    if amount_cents > 2147483647:\n        return None\n    return amount_cents",
    "actual_issue": "Integer overflow - converting float to int can exceed 32-bit limits, causing incorrect values",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch9_sample_4",
    "issue_description": "None - correct implementation of binary search",
    "repo_context": "Search utility module",
    "codex_solution": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = left + (right - left) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch9_sample_5",
    "issue_description": "Memory leak due to circular reference",
    "repo_context": "Event handling system",
    "codex_solution": "class EventEmitter:\n    def __init__(self):\n        self.handlers = []\n    \n    def on(self, event, handler):\n        self.handlers.append({'event': event, 'handler': handler, 'emitter': self})\n    \n    def emit(self, event, data):\n        for h in self.handlers:\n            if h['event'] == event:\n                h['handler'](data)",
    "actual_issue": "Memory leak - storing reference to self in handler dict creates circular reference preventing garbage collection",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch9_sample_6",
    "issue_description": "None - correct CSV parsing with proper escaping",
    "repo_context": "Data import utility",
    "codex_solution": "import csv\n\ndef parse_csv_file(filepath):\n    results = []\n    with open(filepath, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            results.append(dict(row))\n    return results",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch9_sample_7",
    "issue_description": "Path traversal vulnerability in file access",
    "repo_context": "File download API endpoint",
    "codex_solution": "import os\n\ndef get_user_file(username, filename):\n    base_path = '/var/app/uploads'\n    file_path = os.path.join(base_path, username, filename)\n    if os.path.exists(file_path):\n        with open(file_path, 'rb') as f:\n            return f.read()\n    return None",
    "actual_issue": "Path traversal vulnerability - filename can contain '../' to access files outside intended directory",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch9_sample_8",
    "issue_description": "Off-by-one error in pagination logic",
    "repo_context": "API pagination helper",
    "codex_solution": "def paginate_results(items, page, page_size):\n    total_pages = len(items) // page_size\n    if page < 1 or page > total_pages:\n        return []\n    start = (page - 1) * page_size\n    end = start + page_size\n    return items[start:end]",
    "actual_issue": "Off-by-one error - total_pages calculation doesn't account for remainder, last partial page is inaccessible",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch9_sample_9",
    "issue_description": "None - correct input validation with sanitization",
    "repo_context": "Form validation module",
    "codex_solution": "import re\n\ndef validate_username(username):\n    if not username or len(username) < 3:\n        return False\n    if len(username) > 20:\n        return False\n    if not re.match(r'^[a-zA-Z0-9_]+$', username):\n        return False\n    return True",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch9_sample_10",
    "issue_description": "Inefficient nested loop causing O(n^3) complexity",
    "repo_context": "Data processing pipeline",
    "codex_solution": "def find_duplicates(data_list):\n    duplicates = []\n    for i in range(len(data_list)):\n        for j in range(i + 1, len(data_list)):\n            if data_list[i] == data_list[j]:\n                for k in range(len(duplicates)):\n                    if duplicates[k] == data_list[i]:\n                        break\n                else:\n                    duplicates.append(data_list[i])\n    return duplicates",
    "actual_issue": "Performance issue - O(n^3) complexity when O(n) is possible with set, causes timeout on large inputs",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch10_sample_1",
    "issue_description": "Function uses mutable default argument which persists across calls",
    "repo_context": "Utility module for managing user sessions and tracking activity",
    "codex_solution": "def add_session(user_id, sessions=[]):\n    \"\"\"Add user session to tracking list\"\"\"\n    sessions.append(user_id)\n    return sessions\n\ndef get_active_sessions():\n    return add_session('user123')",
    "actual_issue": "Mutable default argument [] is shared across all function calls, causing sessions to accumulate unexpectedly",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch10_sample_2",
    "issue_description": "SQL injection vulnerability in database query",
    "repo_context": "User authentication module for web application",
    "codex_solution": "import sqlite3\n\ndef authenticate_user(username, password):\n    \"\"\"Authenticate user credentials\"\"\"\n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    cursor.execute(query)\n    result = cursor.fetchone()\n    conn.close()\n    return result is not None",
    "actual_issue": "SQL injection vulnerability through string interpolation in query",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch10_sample_3",
    "issue_description": "Race condition in file write operation",
    "repo_context": "Logging system for multi-threaded application",
    "codex_solution": "import os\n\ndef append_log(message, log_file='app.log'):\n    \"\"\"Append message to log file\"\"\"\n    if os.path.exists(log_file):\n        with open(log_file, 'r') as f:\n            content = f.read()\n    else:\n        content = ''\n    \n    with open(log_file, 'w') as f:\n        f.write(content + message + '\\n')",
    "actual_issue": "Race condition between read and write operations can cause log loss in concurrent execution",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch10_sample_4",
    "issue_description": "Proper implementation of context manager for database connections",
    "repo_context": "Database connection pooling module",
    "codex_solution": "import sqlite3\n\nclass DatabaseConnection:\n    def __init__(self, db_path):\n        self.db_path = db_path\n        self.conn = None\n    \n    def __enter__(self):\n        self.conn = sqlite3.connect(self.db_path)\n        return self.conn\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.conn:\n            self.conn.close()\n        return False",
    "actual_issue": "None - correct code with proper resource management",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch10_sample_5",
    "issue_description": "Integer overflow in calculation not handled",
    "repo_context": "Financial calculation module for compound interest",
    "codex_solution": "def calculate_compound_interest(principal, rate, time, frequency=12):\n    \"\"\"Calculate compound interest\"\"\"\n    amount = principal * pow((1 + rate / frequency), frequency * time)\n    return int(amount * 100)\n\ndef get_investment_value(initial):\n    return calculate_compound_interest(initial, 0.15, 50, 365)",
    "actual_issue": "Integer conversion can cause overflow or precision loss for large values",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch10_sample_6",
    "issue_description": "Proper input validation and sanitization",
    "repo_context": "User input processing module",
    "codex_solution": "import re\n\ndef validate_email(email):\n    \"\"\"Validate email format\"\"\"\n    if not email or not isinstance(email, str):\n        return False\n    \n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))\n\ndef sanitize_username(username):\n    \"\"\"Remove invalid characters from username\"\"\"\n    return re.sub(r'[^a-zA-Z0-9_-]', '', username)",
    "actual_issue": "None - correct code with proper validation",
    "failure_category": "security",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch10_sample_7",
    "issue_description": "Memory leak due to circular reference",
    "repo_context": "Graph data structure implementation",
    "codex_solution": "class Node:\n    def __init__(self, value):\n        self.value = value\n        self.parent = None\n        self.children = []\n    \n    def add_child(self, child):\n        self.children.append(child)\n        child.parent = self\n\ndef create_circular_graph():\n    root = Node(1)\n    child = Node(2)\n    root.add_child(child)\n    child.add_child(root)\n    return root",
    "actual_issue": "Circular reference between parent and child prevents garbage collection",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch10_sample_8",
    "issue_description": "Division by zero not handled in statistical calculation",
    "repo_context": "Statistical analysis module",
    "codex_solution": "def calculate_average(numbers):\n    \"\"\"Calculate average of numbers\"\"\"\n    return sum(numbers) / len(numbers)\n\ndef calculate_variance(numbers):\n    avg = calculate_average(numbers)\n    return sum((x - avg) ** 2 for x in numbers) / len(numbers)\n\ndef analyze_dataset(data):\n    return {'avg': calculate_average(data), 'var': calculate_variance(data)}",
    "actual_issue": "Division by zero when empty list is passed to calculate_average",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch10_sample_9",
    "issue_description": "Proper error handling with custom exceptions",
    "repo_context": "API client module with retry logic",
    "codex_solution": "class APIError(Exception):\n    pass\n\ndef fetch_data(url, retries=3):\n    \"\"\"Fetch data with retry logic\"\"\"\n    for attempt in range(retries):\n        try:\n            # Simulated API call\n            if attempt < retries - 1:\n                raise ConnectionError('Network error')\n            return {'status': 'success', 'data': []}\n        except ConnectionError as e:\n            if attempt == retries - 1:\n                raise APIError(f'Failed after {retries} attempts') from e\n            continue",
    "actual_issue": "None - correct code with proper exception handling",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch10_sample_10",
    "issue_description": "Inefficient nested loop causing O(n^3) complexity",
    "repo_context": "Data processing module for large datasets",
    "codex_solution": "def find_triplets_sum(arr, target):\n    \"\"\"Find all triplets that sum to target\"\"\"\n    result = []\n    n = len(arr)\n    for i in range(n):\n        for j in range(n):\n            for k in range(n):\n                if i != j and j != k and i != k:\n                    if arr[i] + arr[j] + arr[k] == target:\n                        result.append((arr[i], arr[j], arr[k]))\n    return result",
    "actual_issue": "Inefficient O(n^3) algorithm with unnecessary iterations and duplicate results",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch11_sample_1",
    "issue_description": "User authentication bypass through SQL injection in login endpoint",
    "repo_context": "Flask web application with user authentication system",
    "codex_solution": "from flask import request\nimport sqlite3\n\n@app.route('/login', methods=['POST'])\ndef login():\n    username = request.form['username']\n    password = request.form['password']\n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    cursor.execute(query)\n    user = cursor.fetchone()\n    if user:\n        return {'status': 'success'}\n    return {'status': 'failed'}, 401",
    "actual_issue": "SQL injection vulnerability - user input directly concatenated into SQL query without parameterization",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch11_sample_2",
    "issue_description": "Dictionary key access without checking existence",
    "repo_context": "API request handler processing JSON payloads",
    "codex_solution": "from flask import request, jsonify\n\n@app.route('/api/process', methods=['POST'])\ndef process_data():\n    data = request.get_json()\n    user_id = data['user_id']\n    action = data['action']\n    timestamp = data['timestamp']\n    \n    result = perform_action(user_id, action, timestamp)\n    return jsonify({'result': result, 'status': 'success'})",
    "actual_issue": "KeyError will be raised if any required key is missing from the JSON payload - should use .get() with defaults or validate presence",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch11_sample_3",
    "issue_description": "Secure password hashing with proper validation",
    "repo_context": "User registration system with security best practices",
    "codex_solution": "from werkzeug.security import generate_password_hash, check_password_hash\nimport re\n\ndef register_user(username, password, email):\n    if len(password) < 8:\n        raise ValueError('Password must be at least 8 characters')\n    if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', email):\n        raise ValueError('Invalid email format')\n    \n    hashed_pw = generate_password_hash(password, method='pbkdf2:sha256')\n    save_to_database(username, hashed_pw, email)\n    return True",
    "actual_issue": "None - correct code with proper password hashing and input validation",
    "failure_category": "security",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch11_sample_4",
    "issue_description": "Float comparison using equality operator",
    "repo_context": "Financial calculation module for transaction processing",
    "codex_solution": "def calculate_balance(transactions):\n    balance = 0.0\n    for trans in transactions:\n        balance += trans['amount']\n    \n    if balance == 0.0:\n        return 'BALANCED'\n    elif balance > 0:\n        return f'SURPLUS: {balance}'\n    else:\n        return f'DEFICIT: {balance}'",
    "actual_issue": "Float equality comparison is unreliable due to floating-point precision errors - should use abs(balance) < epsilon or Decimal type",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch11_sample_5",
    "issue_description": "Unbounded memory growth in cache implementation",
    "repo_context": "Caching layer for API responses",
    "codex_solution": "class ResponseCache:\n    def __init__(self):\n        self.cache = {}\n    \n    def get(self, key):\n        return self.cache.get(key)\n    \n    def set(self, key, value):\n        self.cache[key] = value\n    \n    def clear_old_entries(self):\n        for key in list(self.cache.keys()):\n            if self.is_expired(key):\n                del self.cache[key]",
    "actual_issue": "Cache can grow indefinitely without size limits or TTL enforcement - missing max_size check and automatic eviction policy",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch11_sample_6",
    "issue_description": "Proper file handling with context manager and error handling",
    "repo_context": "Log file processor with robust error handling",
    "codex_solution": "import os\n\ndef process_log_file(filepath):\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f'Log file not found: {filepath}')\n    \n    results = []\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                results.append(parse_log_line(line))\n    \n    return results",
    "actual_issue": "None - correct code with proper file handling using context manager and validation",
    "failure_category": "resource_management",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "batch11_sample_7",
    "issue_description": "Race condition in multi-threaded counter",
    "repo_context": "Concurrent request counter for rate limiting",
    "codex_solution": "class RequestCounter:\n    def __init__(self):\n        self.count = 0\n    \n    def increment(self):\n        current = self.count\n        current += 1\n        self.count = current\n    \n    def get_count(self):\n        return self.count\n    \n    def reset(self):\n        self.count = 0",
    "actual_issue": "Race condition in increment() - multiple threads can read same value before writing, losing counts. Needs threading.Lock or atomic operations",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "batch11_sample_8",
    "issue_description": "Inefficient nested loop with O(n\u00b2) complexity",
    "repo_context": "Data deduplication utility for large datasets",
    "codex_solution": "def remove_duplicates(items):\n    unique = []\n    for item in items:\n        is_duplicate = False\n        for existing in unique:\n            if item == existing:\n                is_duplicate = True\n                break\n        if not is_duplicate:\n            unique.append(item)\n    return unique",
    "actual_issue": "O(n\u00b2) time complexity - should use set for O(n) deduplication or dict to preserve order while checking in O(1)",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch11_sample_9",
    "issue_description": "Proper input validation and sanitization for user data",
    "repo_context": "User profile update endpoint with comprehensive validation",
    "codex_solution": "import re\nfrom flask import request, jsonify\n\nALLOWED_FIELDS = {'name', 'bio', 'location'}\n\n@app.route('/profile/update', methods=['POST'])\ndef update_profile():\n    data = request.get_json()\n    if not data:\n        return jsonify({'error': 'No data provided'}), 400\n    \n    updates = {k: v for k, v in data.items() if k in ALLOWED_FIELDS}\n    for key, value in updates.items():\n        if not isinstance(value, str) or len(value) > 500:\n            return jsonify({'error': f'Invalid {key}'}), 400\n    \n    save_profile(updates)\n    return jsonify({'status': 'success'})",
    "actual_issue": "None - correct code with proper whitelisting, type checking, and length validation",
    "failure_category": "security",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "batch11_sample_10",
    "issue_description": "Division by zero not handled in average calculation",
    "repo_context": "Statistics calculator for user metrics",
    "codex_solution": "def calculate_average_score(scores):\n    if not scores:\n        return None\n    \n    total = sum(scores)\n    count = len([s for s in scores if s is not None])\n    average = total / count\n    \n    return round(average, 2)",
    "actual_issue": "ZeroDivisionError when all scores are None - count becomes 0 but total/count executes. Should check count > 0 before division",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  }
]