[
  {
    "problem_id": "path_traversal_001",
    "issue_description": "File upload handler that saves user-provided files to disk",
    "repo_context": "Web application file upload module",
    "codex_solution": "import os\nimport shutil\nfrom pathlib import Path\n\nclass FileUploadHandler:\n    def __init__(self, upload_dir='/var/uploads'):\n        self.upload_dir = upload_dir\n        os.makedirs(upload_dir, exist_ok=True)\n    \n    def save_uploaded_file(self, filename, content):\n        \"\"\"Save uploaded file to disk\"\"\"\n        # BUG: No path traversal protection - allows ../../../etc/passwd\n        file_path = os.path.join(self.upload_dir, filename)\n        \n        with open(file_path, 'wb') as f:\n            f.write(content)\n        \n        return file_path\n    \n    def get_file(self, filename):\n        \"\"\"Retrieve uploaded file\"\"\"\n        # BUG: Path traversal vulnerability here too\n        file_path = os.path.join(self.upload_dir, filename)\n        \n        if os.path.exists(file_path):\n            with open(file_path, 'rb') as f:\n                return f.read()\n        return None",
    "actual_issue": "Path traversal vulnerability - filename not sanitized, allows directory traversal attacks using ../",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "race_condition_001",
    "issue_description": "Bank account transfer system with concurrent transaction support",
    "repo_context": "Banking application transaction module",
    "codex_solution": "import threading\nimport time\n\nclass BankAccount:\n    def __init__(self, account_id, balance=0):\n        self.account_id = account_id\n        self.balance = balance\n    \n    def withdraw(self, amount):\n        \"\"\"Withdraw money from account\"\"\"\n        # BUG: Check-then-act race condition\n        if self.balance >= amount:\n            time.sleep(0.001)  # Simulate processing delay\n            self.balance -= amount\n            return True\n        return False\n    \n    def deposit(self, amount):\n        \"\"\"Deposit money to account\"\"\"\n        self.balance += amount\n    \n    def transfer(self, target_account, amount):\n        \"\"\"Transfer money to another account\"\"\"\n        # BUG: No locking - race condition allows overdraft\n        if self.withdraw(amount):\n            target_account.deposit(amount)\n            return True\n        return False\n    \n    def get_balance(self):\n        return self.balance",
    "actual_issue": "Race condition in withdraw/transfer methods - no synchronization allows multiple threads to bypass balance check",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "sql_injection_001",
    "issue_description": "User authentication system with database queries",
    "repo_context": "Authentication module for web application",
    "codex_solution": "import sqlite3\nimport hashlib\n\nclass UserAuthenticator:\n    def __init__(self, db_path='users.db'):\n        self.db_path = db_path\n        self.setup_database()\n    \n    def setup_database(self):\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS users (\n                username TEXT PRIMARY KEY,\n                password_hash TEXT,\n                role TEXT\n            )\n        ''')\n        conn.commit()\n        conn.close()\n    \n    def authenticate(self, username, password):\n        \"\"\"Authenticate user credentials\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        # BUG: SQL injection vulnerability - string concatenation\n        query = f\"SELECT * FROM users WHERE username = '{username}' AND password_hash = '{hashlib.sha256(password.encode()).hexdigest()}'\"\n        cursor.execute(query)\n        \n        result = cursor.fetchone()\n        conn.close()\n        return result is not None",
    "actual_issue": "SQL injection vulnerability - user input concatenated directly into SQL query without parameterization",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "memory_leak_001",
    "issue_description": "Cache system that stores frequently accessed data",
    "repo_context": "Caching layer for high-performance application",
    "codex_solution": "import time\nfrom collections import defaultdict\n\nclass DataCache:\n    def __init__(self):\n        self.cache = {}\n        self.access_times = defaultdict(list)\n        self.hit_count = 0\n        self.miss_count = 0\n    \n    def get(self, key):\n        \"\"\"Retrieve value from cache\"\"\"\n        if key in self.cache:\n            self.hit_count += 1\n            # BUG: Unbounded growth of access_times list\n            self.access_times[key].append(time.time())\n            return self.cache[key]\n        else:\n            self.miss_count += 1\n            return None\n    \n    def set(self, key, value):\n        \"\"\"Store value in cache\"\"\"\n        # BUG: No eviction policy - cache grows indefinitely\n        self.cache[key] = value\n        self.access_times[key].append(time.time())\n    \n    def get_stats(self):\n        \"\"\"Get cache statistics\"\"\"\n        return {\n            'hits': self.hit_count,\n            'misses': self.miss_count,\n            'size': len(self.cache)\n        }",
    "actual_issue": "Memory leak - cache has no size limit or eviction policy, access_times list grows unbounded",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "correct_json_parser_001",
    "issue_description": "JSON configuration parser with validation",
    "repo_context": "Configuration management system",
    "codex_solution": "import json\nimport os\nfrom typing import Dict, Any\n\nclass ConfigParser:\n    def __init__(self, config_file: str):\n        self.config_file = config_file\n        self.config = {}\n    \n    def load_config(self) -> Dict[str, Any]:\n        \"\"\"Load and validate configuration file\"\"\"\n        if not os.path.exists(self.config_file):\n            raise FileNotFoundError(f\"Config file not found: {self.config_file}\")\n        \n        with open(self.config_file, 'r') as f:\n            self.config = json.load(f)\n        \n        self._validate_config()\n        return self.config\n    \n    def _validate_config(self):\n        \"\"\"Validate configuration structure\"\"\"\n        required_fields = ['app_name', 'version', 'settings']\n        \n        for field in required_fields:\n            if field not in self.config:\n                raise ValueError(f\"Missing required field: {field}\")\n        \n        if not isinstance(self.config['settings'], dict):\n            raise ValueError(\"Settings must be a dictionary\")\n    \n    def get(self, key: str, default=None):\n        \"\"\"Get configuration value\"\"\"\n        return self.config.get(key, default)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "float_comparison_001",
    "issue_description": "Financial calculation system for invoice processing",
    "repo_context": "Accounting module for e-commerce platform",
    "codex_solution": "class InvoiceCalculator:\n    def __init__(self, tax_rate=0.1):\n        self.tax_rate = tax_rate\n    \n    def calculate_total(self, items):\n        \"\"\"Calculate total with tax\"\"\"\n        subtotal = sum(item['price'] * item['quantity'] for item in items)\n        tax = subtotal * self.tax_rate\n        total = subtotal + tax\n        return total\n    \n    def apply_discount(self, total, discount_percent):\n        \"\"\"Apply discount to total\"\"\"\n        discount = total * (discount_percent / 100)\n        return total - discount\n    \n    def is_payment_complete(self, invoice_total, payment_received):\n        \"\"\"Check if payment covers invoice\"\"\"\n        # BUG: Direct float comparison - fails due to floating point precision\n        if payment_received == invoice_total:\n            return True\n        elif payment_received > invoice_total:\n            return True\n        return False\n    \n    def calculate_change(self, invoice_total, payment):\n        \"\"\"Calculate change to return\"\"\"\n        # BUG: Float arithmetic precision issues\n        return payment - invoice_total",
    "actual_issue": "Floating point comparison without epsilon tolerance - causes incorrect equality checks due to precision errors",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "correct_binary_search_001",
    "issue_description": "Binary search implementation for sorted arrays",
    "repo_context": "Search utilities library",
    "codex_solution": "def binary_search(arr, target):\n    \"\"\"Perform binary search on sorted array\"\"\"\n    if not arr:\n        return -1\n    \n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = left + (right - left) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\ndef find_insertion_point(arr, target):\n    \"\"\"Find insertion point for target in sorted array\"\"\"\n    left, right = 0, len(arr)\n    \n    while left < right:\n        mid = left + (right - left) // 2\n        if arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid\n    \n    return left\n\ndef search_range(arr, target):\n    \"\"\"Find first and last position of target\"\"\"\n    if not arr:\n        return [-1, -1]\n    \n    first = find_insertion_point(arr, target)\n    if first == len(arr) or arr[first] != target:\n        return [-1, -1]\n    \n    last = find_insertion_point(arr, target + 1) - 1\n    return [first, last]",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "regex_dos_001",
    "issue_description": "Email validation system using regular expressions",
    "repo_context": "User input validation module",
    "codex_solution": "import re\n\nclass InputValidator:\n    def __init__(self):\n        # BUG: Catastrophic backtracking vulnerability (ReDoS)\n        self.email_pattern = re.compile(r'^([a-zA-Z0-9]+)*@([a-zA-Z0-9]+)*\\.com$')\n        self.phone_pattern = re.compile(r'^\\d{3}-\\d{3}-\\d{4}$')\n    \n    def validate_email(self, email):\n        \"\"\"Validate email address format\"\"\"\n        # BUG: Can cause exponential time complexity with malicious input\n        # Input like 'aaaaaaaaaaaaaaaaaaaaaaaaa!' causes catastrophic backtracking\n        return self.email_pattern.match(email) is not None\n    \n    def validate_phone(self, phone):\n        \"\"\"Validate phone number format\"\"\"\n        return self.phone_pattern.match(phone) is not None\n    \n    def sanitize_input(self, text):\n        \"\"\"Remove special characters\"\"\"\n        # BUG: Another potentially dangerous regex\n        return re.sub(r'(.*)+[^a-zA-Z0-9]', '', text)",
    "actual_issue": "Regular expression denial of service (ReDoS) - regex patterns with nested quantifiers cause catastrophic backtracking",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "correct_queue_implementation_001",
    "issue_description": "Thread-safe queue implementation for task processing",
    "repo_context": "Task queue system for worker pool",
    "codex_solution": "import threading\nfrom collections import deque\n\nclass ThreadSafeQueue:\n    def __init__(self, maxsize=0):\n        self.maxsize = maxsize\n        self.queue = deque()\n        self.lock = threading.Lock()\n        self.not_empty = threading.Condition(self.lock)\n        self.not_full = threading.Condition(self.lock)\n    \n    def put(self, item, block=True):\n        \"\"\"Add item to queue\"\"\"\n        with self.not_full:\n            while self.maxsize > 0 and len(self.queue) >= self.maxsize:\n                if not block:\n                    raise Exception(\"Queue is full\")\n                self.not_full.wait()\n            \n            self.queue.append(item)\n            self.not_empty.notify()\n    \n    def get(self, block=True):\n        \"\"\"Remove and return item from queue\"\"\"\n        with self.not_empty:\n            while len(self.queue) == 0:\n                if not block:\n                    raise Exception(\"Queue is empty\")\n                self.not_empty.wait()\n            \n            item = self.queue.popleft()\n            self.not_full.notify()\n            return item\n    \n    def size(self):\n        with self.lock:\n            return len(self.queue)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "time_of_check_001",
    "issue_description": "Secure file operations with permission checking",
    "repo_context": "File management system with access control",
    "codex_solution": "import os\nimport stat\n\nclass SecureFileManager:\n    def __init__(self, base_dir):\n        self.base_dir = base_dir\n    \n    def safe_delete(self, filename):\n        \"\"\"Safely delete file after permission check\"\"\"\n        filepath = os.path.join(self.base_dir, filename)\n        \n        # BUG: Time-of-check to time-of-use (TOCTOU) race condition\n        if os.path.exists(filepath):\n            file_stats = os.stat(filepath)\n            \n            # Check if file is writable\n            if file_stats.st_mode & stat.S_IWUSR:\n                # BUG: File could be replaced between check and use\n                os.remove(filepath)\n                return True\n        return False\n    \n    def safe_read(self, filename):\n        \"\"\"Safely read file after permission check\"\"\"\n        filepath = os.path.join(self.base_dir, filename)\n        \n        # BUG: TOCTOU - file permissions could change\n        if os.access(filepath, os.R_OK):\n            with open(filepath, 'r') as f:\n                return f.read()\n        return None",
    "actual_issue": "Time-of-check to time-of-use (TOCTOU) race condition - file state can change between permission check and file operation",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "code_sample_001",
    "issue_description": "User authentication with session management",
    "repo_context": "Web application login system",
    "codex_solution": "import hashlib\nimport secrets\n\ndef authenticate_user(username, password, stored_hash):\n    \"\"\"Authenticate user with password hash comparison\"\"\"\n    if not username or not password:\n        return False\n    \n    # Hash the provided password\n    password_hash = hashlib.md5(password.encode()).hexdigest()\n    \n    # Compare with stored hash\n    if password_hash == stored_hash:\n        session_token = secrets.token_hex(16)\n        return True, session_token\n    \n    return False, None",
    "actual_issue": "Uses MD5 for password hashing which is cryptographically broken and vulnerable to rainbow table attacks. Should use bcrypt, scrypt, or argon2.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "code_sample_002",
    "issue_description": "Calculate average of numbers in a list",
    "repo_context": "Statistical analysis module",
    "codex_solution": "def calculate_average(numbers):\n    \"\"\"Calculate the average of a list of numbers\"\"\"\n    if not numbers:\n        return 0\n    \n    total = sum(numbers)\n    count = len(numbers)\n    \n    return total / count\n\ndef process_data(data_list):\n    \"\"\"Process numerical data and return statistics\"\"\"\n    avg = calculate_average(data_list)\n    return {\n        'average': avg,\n        'count': len(data_list),\n        'total': sum(data_list)\n    }",
    "actual_issue": "Returns 0 for empty list instead of None or raising exception, which could mask errors. Also doesn't handle non-numeric values.",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "code_sample_003",
    "issue_description": "Database query builder with parameterization",
    "repo_context": "ORM-like database interface",
    "codex_solution": "def build_user_query(table, username, role):\n    \"\"\"Build SQL query to fetch user data\"\"\"\n    query = f\"SELECT * FROM {table} WHERE username = '{username}'\"\n    \n    if role:\n        query += f\" AND role = '{role}'\"\n    \n    return query\n\ndef get_user_data(db_connection, username, role=None):\n    \"\"\"Retrieve user data from database\"\"\"\n    query = build_user_query('users', username, role)\n    cursor = db_connection.cursor()\n    cursor.execute(query)\n    return cursor.fetchall()",
    "actual_issue": "SQL injection vulnerability - user input is directly interpolated into query string without parameterization or sanitization.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "code_sample_004",
    "issue_description": "File upload handler with validation",
    "repo_context": "Web application file upload endpoint",
    "codex_solution": "import os\nimport mimetypes\n\ndef save_uploaded_file(file_data, filename, upload_dir):\n    \"\"\"Save uploaded file to server\"\"\"\n    # Validate file extension\n    allowed_extensions = ['.jpg', '.png', '.pdf', '.doc']\n    file_ext = os.path.splitext(filename)[1].lower()\n    \n    if file_ext not in allowed_extensions:\n        raise ValueError(\"Invalid file type\")\n    \n    # Save file\n    file_path = os.path.join(upload_dir, filename)\n    with open(file_path, 'wb') as f:\n        f.write(file_data)\n    \n    return file_path",
    "actual_issue": "Path traversal vulnerability - filename not sanitized, attacker could use '../../../etc/passwd' to write to arbitrary locations.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "code_sample_005",
    "issue_description": "Parse and validate JSON configuration",
    "repo_context": "Application configuration loader",
    "codex_solution": "import json\n\ndef load_config(config_path):\n    \"\"\"Load and validate application configuration\"\"\"\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    \n    # Validate required fields\n    required_fields = ['api_key', 'database_url', 'port']\n    for field in required_fields:\n        if field not in config:\n            raise ValueError(f\"Missing required field: {field}\")\n    \n    # Validate port range\n    if not (1024 <= config['port'] <= 65535):\n        raise ValueError(\"Port must be between 1024 and 65535\")\n    \n    return config",
    "actual_issue": "None - correct code with proper validation and error handling",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "code_sample_006",
    "issue_description": "Cache implementation with expiration",
    "repo_context": "In-memory caching system",
    "codex_solution": "import time\n\nclass SimpleCache:\n    def __init__(self):\n        self.cache = {}\n    \n    def set(self, key, value, ttl=3600):\n        \"\"\"Set cache entry with time-to-live\"\"\"\n        expiry = time.time() + ttl\n        self.cache[key] = {'value': value, 'expiry': expiry}\n    \n    def get(self, key):\n        \"\"\"Get cache entry if not expired\"\"\"\n        if key in self.cache:\n            if time.time() < self.cache[key]['expiry']:\n                return self.cache[key]['value']\n        return None",
    "actual_issue": "Memory leak - expired entries are never removed from cache dictionary, only ignored on retrieval. Cache will grow indefinitely.",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "code_sample_007",
    "issue_description": "Merge two sorted arrays efficiently",
    "repo_context": "Data processing utility functions",
    "codex_solution": "def merge_sorted_arrays(arr1, arr2):\n    \"\"\"Merge two sorted arrays into one sorted array\"\"\"\n    if not arr1:\n        return arr2[:]\n    if not arr2:\n        return arr1[:]\n    \n    result = []\n    i, j = 0, 0\n    \n    while i < len(arr1) and j < len(arr2):\n        if arr1[i] <= arr2[j]:\n            result.append(arr1[i])\n            i += 1\n        else:\n            result.append(arr2[j])\n            j += 1\n    \n    result.extend(arr1[i:])\n    result.extend(arr2[j:])\n    \n    return result",
    "actual_issue": "None - correct code implementing proper merge algorithm with edge case handling",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "code_sample_008",
    "issue_description": "Generate unique identifiers for records",
    "repo_context": "Database record creation system",
    "codex_solution": "import random\nimport string\n\ndef generate_unique_id(length=8):\n    \"\"\"Generate unique identifier for database records\"\"\"\n    characters = string.ascii_uppercase + string.digits\n    unique_id = ''.join(random.choice(characters) for _ in range(length))\n    return unique_id\n\ndef create_record(data):\n    \"\"\"Create new record with unique ID\"\"\"\n    record_id = generate_unique_id()\n    record = {\n        'id': record_id,\n        'data': data,\n        'created_at': time.time()\n    }\n    return record",
    "actual_issue": "Uses random.choice() which is not cryptographically secure and doesn't guarantee uniqueness. Should use UUID or secrets module. High collision risk.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "code_sample_009",
    "issue_description": "Rate limiting decorator for API endpoints",
    "repo_context": "API rate limiting middleware",
    "codex_solution": "import time\nfrom functools import wraps\n\ndef rate_limit(max_calls, time_window):\n    \"\"\"Decorator to limit function calls per time window\"\"\"\n    calls = []\n    \n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            now = time.time()\n            # Remove old calls outside time window\n            calls[:] = [call for call in calls if call > now - time_window]\n            \n            if len(calls) >= max_calls:\n                raise Exception(\"Rate limit exceeded\")\n            \n            calls.append(now)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator",
    "actual_issue": "Race condition - not thread-safe. Multiple threads can check and modify 'calls' list simultaneously, allowing rate limit bypass.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "code_sample_010",
    "issue_description": "Email validation with regex pattern",
    "repo_context": "User registration form validation",
    "codex_solution": "import re\n\ndef validate_email(email):\n    \"\"\"Validate email address format\"\"\"\n    if not email or not isinstance(email, str):\n        return False\n    \n    # RFC 5322 simplified pattern\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    \n    if not re.match(pattern, email):\n        return False\n    \n    # Check length constraints\n    if len(email) > 254:\n        return False\n    \n    local, domain = email.rsplit('@', 1)\n    if len(local) > 64:\n        return False\n    \n    return True",
    "actual_issue": "None - correct code with proper regex validation and RFC-compliant length checks",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "auth_bypass_001",
    "issue_description": "JWT token validation with timing attack vulnerability",
    "repo_context": "Authentication middleware for API gateway",
    "codex_solution": "import jwt\nimport time\nfrom flask import request, jsonify\n\nSECRET_KEY = 'production-secret-key'\n\ndef validate_token(token):\n    \"\"\"Validate JWT token\"\"\"\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])\n        expected_signature = jwt.encode(payload, SECRET_KEY, algorithm='HS256')\n        \n        # BUG: String comparison vulnerable to timing attacks\n        if token == expected_signature:\n            return payload\n        return None\n    except:\n        return None\n\n@app.before_request\ndef authenticate():\n    token = request.headers.get('Authorization', '').replace('Bearer ', '')\n    if not token:\n        return jsonify({'error': 'No token'}), 401\n    \n    user = validate_token(token)\n    if not user:\n        return jsonify({'error': 'Invalid token'}), 401\n    \n    request.user = user",
    "actual_issue": "Uses string comparison for token validation which is vulnerable to timing attacks. Should use hmac.compare_digest() for constant-time comparison",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "race_condition_002",
    "issue_description": "File-based cache implementation with race conditions",
    "repo_context": "Caching layer for high-traffic web application",
    "codex_solution": "import os\nimport json\nimport time\nfrom pathlib import Path\n\nclass FileCache:\n    def __init__(self, cache_dir='/tmp/cache'):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n    \n    def get(self, key):\n        \"\"\"Get value from cache\"\"\"\n        cache_file = self.cache_dir / f\"{key}.json\"\n        \n        # BUG: Race condition - file can be deleted between exists() and open()\n        if cache_file.exists():\n            with open(cache_file, 'r') as f:\n                data = json.load(f)\n                if data['expires'] > time.time():\n                    return data['value']\n        return None\n    \n    def set(self, key, value, ttl=3600):\n        \"\"\"Set value in cache\"\"\"\n        cache_file = self.cache_dir / f\"{key}.json\"\n        data = {\n            'value': value,\n            'expires': time.time() + ttl\n        }\n        \n        # BUG: Non-atomic write - partial data if interrupted\n        with open(cache_file, 'w') as f:\n            json.dump(data, f)",
    "actual_issue": "Race condition between exists() check and file open, non-atomic writes can corrupt cache. Should use file locking or atomic operations",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "sql_injection_003",
    "issue_description": "Database query builder with SQL injection vulnerability",
    "repo_context": "ORM-like query builder for legacy database",
    "codex_solution": "class QueryBuilder:\n    def __init__(self, table):\n        self.table = table\n        self.conditions = []\n        self.order = None\n    \n    def where(self, field, operator, value):\n        \"\"\"Add WHERE condition\"\"\"\n        # BUG: Direct string interpolation allows SQL injection\n        condition = f\"{field} {operator} '{value}'\"\n        self.conditions.append(condition)\n        return self\n    \n    def order_by(self, field, direction='ASC'):\n        \"\"\"Add ORDER BY clause\"\"\"\n        # BUG: Unsanitized field and direction\n        self.order = f\"{field} {direction}\"\n        return self\n    \n    def execute(self, db):\n        \"\"\"Execute the query\"\"\"\n        query = f\"SELECT * FROM {self.table}\"\n        \n        if self.conditions:\n            query += \" WHERE \" + \" AND \".join(self.conditions)\n        \n        if self.order:\n            query += f\" ORDER BY {self.order}\"\n        \n        return db.execute(query).fetchall()\n\n# Usage\nresults = QueryBuilder('users').where('email', '=', user_input).execute(db)",
    "actual_issue": "Multiple SQL injection vulnerabilities through unsanitized field names, operators, and values. Should use parameterized queries",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "memory_leak_004",
    "issue_description": "Event handler system with memory leaks",
    "repo_context": "Event-driven architecture for real-time application",
    "codex_solution": "class EventEmitter:\n    def __init__(self):\n        self.listeners = {}\n    \n    def on(self, event_name, callback):\n        \"\"\"Register event listener\"\"\"\n        if event_name not in self.listeners:\n            self.listeners[event_name] = []\n        # BUG: No way to remove listeners - memory leak\n        self.listeners[event_name].append(callback)\n    \n    def emit(self, event_name, *args, **kwargs):\n        \"\"\"Emit event to all listeners\"\"\"\n        if event_name in self.listeners:\n            for callback in self.listeners[event_name]:\n                try:\n                    callback(*args, **kwargs)\n                except Exception as e:\n                    print(f\"Error in listener: {e}\")\n\nclass WebSocketHandler:\n    def __init__(self, emitter):\n        self.emitter = emitter\n        # BUG: Lambda captures self, creating circular reference\n        self.emitter.on('message', lambda msg: self.handle_message(msg))\n        self.emitter.on('close', lambda: self.cleanup())\n    \n    def handle_message(self, msg):\n        print(f\"Received: {msg}\")\n    \n    def cleanup(self):\n        # BUG: No listener removal\n        pass",
    "actual_issue": "Memory leak from listeners never being removed and circular references with lambda closures. Should implement remove_listener() and weak references",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "integer_overflow_005",
    "issue_description": "Financial calculation with precision loss",
    "repo_context": "Payment processing system handling transactions",
    "codex_solution": "class PaymentProcessor:\n    def __init__(self):\n        self.transactions = []\n    \n    def process_payment(self, amount, currency='USD'):\n        \"\"\"Process a payment transaction\"\"\"\n        # BUG: Using float for money calculations\n        fee = amount * 0.029 + 0.30\n        net_amount = amount - fee\n        \n        transaction = {\n            'amount': amount,\n            'fee': fee,\n            'net': net_amount,\n            'currency': currency\n        }\n        self.transactions.append(transaction)\n        return transaction\n    \n    def calculate_total_revenue(self):\n        \"\"\"Calculate total revenue from fees\"\"\"\n        # BUG: Accumulating floating point errors\n        total = 0.0\n        for txn in self.transactions:\n            total += txn['fee']\n        return total\n    \n    def apply_discount(self, percentage):\n        \"\"\"Apply discount to amount\"\"\"\n        # BUG: Division can cause precision loss\n        return lambda amount: amount * (1 - percentage / 100)",
    "actual_issue": "Uses floating point for financial calculations causing precision loss and rounding errors. Should use Decimal type for monetary values",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "good_async_pool_001",
    "issue_description": "Properly implemented async connection pool",
    "repo_context": "Database connection pooling for async web framework",
    "codex_solution": "import asyncio\nfrom contextlib import asynccontextmanager\n\nclass AsyncConnectionPool:\n    def __init__(self, max_size=10, connect_func=None):\n        self.max_size = max_size\n        self.connect_func = connect_func\n        self._pool = asyncio.Queue(maxsize=max_size)\n        self._size = 0\n        self._lock = asyncio.Lock()\n    \n    async def _create_connection(self):\n        \"\"\"Create a new connection\"\"\"\n        return await self.connect_func()\n    \n    @asynccontextmanager\n    async def acquire(self):\n        \"\"\"Acquire connection from pool\"\"\"\n        conn = None\n        try:\n            conn = await asyncio.wait_for(\n                self._pool.get(), \n                timeout=5.0\n            )\n        except asyncio.TimeoutError:\n            async with self._lock:\n                if self._size < self.max_size:\n                    conn = await self._create_connection()\n                    self._size += 1\n                else:\n                    raise Exception(\"Pool exhausted\")\n        \n        try:\n            yield conn\n        finally:\n            if conn:\n                await self._pool.put(conn)",
    "actual_issue": "None - correct code with proper async resource management, timeout handling, and context managers",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "path_traversal_006",
    "issue_description": "File upload handler with path traversal vulnerability",
    "repo_context": "File upload endpoint for user documents",
    "codex_solution": "import os\nfrom flask import request, send_file\n\nUPLOAD_DIR = '/var/www/uploads'\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    \"\"\"Handle file upload\"\"\"\n    file = request.files.get('file')\n    if not file:\n        return {'error': 'No file'}, 400\n    \n    filename = file.filename\n    # BUG: No sanitization of filename\n    filepath = os.path.join(UPLOAD_DIR, filename)\n    file.save(filepath)\n    \n    return {'success': True, 'path': filename}\n\n@app.route('/download/<path:filename>')\ndef download_file(filename):\n    \"\"\"Download uploaded file\"\"\"\n    # BUG: Path traversal vulnerability\n    filepath = os.path.join(UPLOAD_DIR, filename)\n    \n    if os.path.exists(filepath):\n        return send_file(filepath)\n    return {'error': 'Not found'}, 404",
    "actual_issue": "Path traversal vulnerability allows accessing files outside upload directory using '../' in filename. Should sanitize and validate file paths",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "good_validation_002",
    "issue_description": "Robust input validation for API endpoint",
    "repo_context": "User registration endpoint with comprehensive validation",
    "codex_solution": "import re\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any\n\nclass ValidationError(Exception):\n    pass\n\nclass UserValidator:\n    EMAIL_REGEX = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n    USERNAME_REGEX = re.compile(r'^[a-zA-Z0-9_-]{3,20}$')\n    \n    @staticmethod\n    def validate_email(email: str) -> str:\n        \"\"\"Validate email format\"\"\"\n        if not email or not isinstance(email, str):\n            raise ValidationError(\"Email is required\")\n        \n        email = email.strip().lower()\n        if not UserValidator.EMAIL_REGEX.match(email):\n            raise ValidationError(\"Invalid email format\")\n        \n        return email\n    \n    @staticmethod\n    def validate_username(username: str) -> str:\n        \"\"\"Validate username\"\"\"\n        if not username or not isinstance(username, str):\n            raise ValidationError(\"Username is required\")\n        \n        username = username.strip()\n        if not UserValidator.USERNAME_REGEX.match(username):\n            raise ValidationError(\"Username must be 3-20 alphanumeric characters\")\n        \n        return username\n    \n    @staticmethod\n    def validate_age(age: Any) -> int:\n        \"\"\"Validate age\"\"\"\n        try:\n            age = int(age)\n        except (ValueError, TypeError):\n            raise ValidationError(\"Age must be a number\")\n        \n        if age < 13 or age > 120:\n            raise ValidationError(\"Age must be between 13 and 120\")\n        \n        return age\n    \n    @classmethod\n    def validate_user_data(cls, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate complete user data\"\"\"\n        return {\n            'email': cls.validate_email(data.get('email')),\n            'username': cls.validate_username(data.get('username')),\n            'age': cls.validate_age(data.get('age'))\n        }",
    "actual_issue": "None - correct code with proper input validation, type checking, and sanitization",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "deadlock_007",
    "issue_description": "Thread synchronization with deadlock potential",
    "repo_context": "Banking system with account transfers",
    "codex_solution": "import threading\n\nclass BankAccount:\n    def __init__(self, account_id, balance=0):\n        self.account_id = account_id\n        self.balance = balance\n        self.lock = threading.Lock()\n    \n    def withdraw(self, amount):\n        \"\"\"Withdraw money from account\"\"\"\n        if self.balance >= amount:\n            self.balance -= amount\n            return True\n        return False\n    \n    def deposit(self, amount):\n        \"\"\"Deposit money to account\"\"\"\n        self.balance += amount\n\ndef transfer(from_account, to_account, amount):\n    \"\"\"Transfer money between accounts\"\"\"\n    # BUG: Can cause deadlock if two threads transfer in opposite directions\n    with from_account.lock:\n        if from_account.balance >= amount:\n            from_account.withdraw(amount)\n            \n            # BUG: Acquiring second lock while holding first\n            with to_account.lock:\n                to_account.deposit(amount)\n            return True\n    return False\n\n# Thread 1: transfer(account_a, account_b, 100)\n# Thread 2: transfer(account_b, account_a, 50)\n# Results in deadlock",
    "actual_issue": "Classic deadlock scenario when acquiring multiple locks. Should use lock ordering (by account_id) or try-lock pattern to prevent deadlock",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "good_error_handling_003",
    "issue_description": "Robust error handling for external API calls",
    "repo_context": "Service integration with retry logic and circuit breaker",
    "codex_solution": "import time\nimport requests\nfrom functools import wraps\nfrom typing import Callable, Any\n\nclass CircuitBreaker:\n    def __init__(self, failure_threshold=5, timeout=60):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.failures = 0\n        self.last_failure_time = None\n        self.state = 'closed'  # closed, open, half_open\n    \n    def call(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Execute function with circuit breaker\"\"\"\n        if self.state == 'open':\n            if time.time() - self.last_failure_time > self.timeout:\n                self.state = 'half_open'\n            else:\n                raise Exception(\"Circuit breaker is open\")\n        \n        try:\n            result = func(*args, **kwargs)\n            if self.state == 'half_open':\n                self.state = 'closed'\n                self.failures = 0\n            return result\n        except Exception as e:\n            self.failures += 1\n            self.last_failure_time = time.time()\n            \n            if self.failures >= self.failure_threshold:\n                self.state = 'open'\n            raise\n\ndef retry_with_backoff(max_retries=3, base_delay=1):\n    \"\"\"Decorator for exponential backoff retry\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except requests.RequestException as e:\n                    if attempt == max_retries - 1:\n                        raise\n                    delay = base_delay * (2 ** attempt)\n                    time.sleep(delay)\n        return wrapper\n    return decorator\n\nclass ExternalAPIClient:\n    def __init__(self, base_url):\n        self.base_url = base_url\n        self.circuit_breaker = CircuitBreaker()\n    \n    @retry_with_backoff(max_retries=3)\n    def get_data(self, endpoint):\n        \"\"\"Fetch data from external API\"\"\"\n        def api_call():\n            response = requests.get(\n                f\"{self.base_url}/{endpoint}\",\n                timeout=5\n            )\n            response.raise_for_status()\n            return response.json()\n        \n        return self.circuit_breaker.call(api_call)",
    "actual_issue": "None - correct code with proper retry logic, circuit breaker pattern, and error handling",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "password_reset_001",
    "issue_description": "Password reset token generation and validation",
    "repo_context": "User authentication system for web application",
    "codex_solution": "import hashlib\nimport time\nimport secrets\n\nclass PasswordResetManager:\n    def __init__(self):\n        self.tokens = {}\n    \n    def generate_reset_token(self, user_id):\n        \"\"\"Generate password reset token\"\"\"\n        token = secrets.token_urlsafe(32)\n        timestamp = int(time.time())\n        \n        # Store token with timestamp\n        self.tokens[user_id] = {\n            'token': token,\n            'created_at': timestamp\n        }\n        return token\n    \n    def validate_token(self, user_id, provided_token):\n        \"\"\"Validate reset token\"\"\"\n        if user_id not in self.tokens:\n            return False\n        \n        stored_data = self.tokens[user_id]\n        current_time = int(time.time())\n        \n        # BUG: Using == for token comparison (timing attack vulnerable)\n        if stored_data['token'] == provided_token:\n            # Check if token expired (15 minutes)\n            if current_time - stored_data['created_at'] < 900:\n                return True\n        return False\n    \n    def reset_password(self, user_id, token, new_password):\n        \"\"\"Reset user password\"\"\"\n        if self.validate_token(user_id, token):\n            # Hash and store new password\n            hashed = hashlib.sha256(new_password.encode()).hexdigest()\n            # Update database...\n            del self.tokens[user_id]\n            return True\n        return False",
    "actual_issue": "Uses timing-unsafe string comparison (==) for token validation, allowing timing attacks to guess valid tokens character by character",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "json_parser_002",
    "issue_description": "Parse and validate JSON configuration files",
    "repo_context": "Configuration management system",
    "codex_solution": "import json\n\ndef parse_config(config_path):\n    \"\"\"Parse JSON configuration file\"\"\"\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    \n    # Validate required fields\n    required = ['database', 'api_key', 'port']\n    for field in required:\n        if field not in config:\n            raise ValueError(f\"Missing required field: {field}\")\n    \n    # Validate port range\n    if not isinstance(config['port'], int):\n        raise TypeError(\"Port must be integer\")\n    \n    if config['port'] < 1024 or config['port'] > 65535:\n        raise ValueError(\"Port must be between 1024 and 65535\")\n    \n    # Validate database config\n    if 'host' not in config['database'] or 'name' not in config['database']:\n        raise ValueError(\"Database must have host and name\")\n    \n    return config\n\ndef merge_configs(base_config, override_config):\n    \"\"\"Merge two configuration dictionaries\"\"\"\n    merged = base_config.copy()\n    \n    for key, value in override_config.items():\n        if isinstance(value, dict) and key in merged:\n            merged[key] = merge_configs(merged[key], value)\n        else:\n            merged[key] = value\n    \n    return merged",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "rate_limiter_003",
    "issue_description": "Implement sliding window rate limiter",
    "repo_context": "API rate limiting middleware",
    "codex_solution": "import time\nfrom collections import defaultdict\n\nclass RateLimiter:\n    def __init__(self, max_requests=100, window_seconds=60):\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self.requests = defaultdict(list)\n    \n    def is_allowed(self, user_id):\n        \"\"\"Check if request is allowed under rate limit\"\"\"\n        current_time = time.time()\n        user_requests = self.requests[user_id]\n        \n        # BUG: Not removing old requests, only filtering for count\n        # This causes memory leak as list grows unbounded\n        recent_requests = [req_time for req_time in user_requests \n                          if current_time - req_time < self.window_seconds]\n        \n        if len(recent_requests) >= self.max_requests:\n            return False\n        \n        # Add current request\n        user_requests.append(current_time)\n        return True\n    \n    def get_remaining(self, user_id):\n        \"\"\"Get remaining requests in current window\"\"\"\n        current_time = time.time()\n        user_requests = self.requests[user_id]\n        \n        recent_requests = [req_time for req_time in user_requests \n                          if current_time - req_time < self.window_seconds]\n        \n        return max(0, self.max_requests - len(recent_requests))",
    "actual_issue": "Memory leak: filters requests for counting but never removes old entries from the stored list, causing unbounded memory growth",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "data_aggregator_004",
    "issue_description": "Aggregate user metrics from multiple sources",
    "repo_context": "Analytics dashboard backend",
    "codex_solution": "def aggregate_user_metrics(user_ids, date_range):\n    \"\"\"Aggregate metrics for multiple users\"\"\"\n    results = []\n    \n    for user_id in user_ids:\n        metrics = {\n            'user_id': user_id,\n            'total_views': 0,\n            'total_clicks': 0,\n            'conversion_rate': 0.0\n        }\n        \n        # Get view data\n        views = db.query(\n            \"SELECT COUNT(*) FROM page_views WHERE user_id = ? AND date BETWEEN ? AND ?\",\n            user_id, date_range[0], date_range[1]\n        ).scalar()\n        metrics['total_views'] = views\n        \n        # Get click data\n        clicks = db.query(\n            \"SELECT COUNT(*) FROM clicks WHERE user_id = ? AND date BETWEEN ? AND ?\",\n            user_id, date_range[0], date_range[1]\n        ).scalar()\n        metrics['total_clicks'] = clicks\n        \n        # BUG: Division by zero when views is 0\n        metrics['conversion_rate'] = clicks / views\n        \n        results.append(metrics)\n    \n    return results",
    "actual_issue": "Division by zero error when user has no views, causing crash instead of handling edge case",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "sql_query_builder_005",
    "issue_description": "Build dynamic SQL queries with filters",
    "repo_context": "Database query abstraction layer",
    "codex_solution": "class QueryBuilder:\n    def __init__(self, table_name):\n        self.table = table_name\n        self.conditions = []\n        self.params = []\n    \n    def where(self, field, operator, value):\n        \"\"\"Add WHERE condition\"\"\"\n        # BUG: String interpolation creates SQL injection vulnerability\n        condition = f\"{field} {operator} '{value}'\"\n        self.conditions.append(condition)\n        return self\n    \n    def where_in(self, field, values):\n        \"\"\"Add WHERE IN condition\"\"\"\n        placeholders = ','.join([f\"'{v}'\" for v in values])\n        condition = f\"{field} IN ({placeholders})\"\n        self.conditions.append(condition)\n        return self\n    \n    def build(self):\n        \"\"\"Build final SQL query\"\"\"\n        query = f\"SELECT * FROM {self.table}\"\n        \n        if self.conditions:\n            query += \" WHERE \" + \" AND \".join(self.conditions)\n        \n        return query\n\ndef search_users(username, status):\n    \"\"\"Search users by username and status\"\"\"\n    builder = QueryBuilder('users')\n    builder.where('username', 'LIKE', f'%{username}%')\n    builder.where('status', '=', status)\n    \n    query = builder.build()\n    return db.execute(query).fetchall()",
    "actual_issue": "SQL injection vulnerability: directly interpolates user input into SQL query without parameterization or escaping",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "email_validator_006",
    "issue_description": "Validate email addresses with proper format checking",
    "repo_context": "User registration form validation",
    "codex_solution": "import re\n\ndef validate_email(email):\n    \"\"\"Validate email address format\"\"\"\n    if not email or not isinstance(email, str):\n        return False\n    \n    # Check length constraints\n    if len(email) > 254:\n        return False\n    \n    # Split into local and domain parts\n    if email.count('@') != 1:\n        return False\n    \n    local, domain = email.split('@')\n    \n    # Validate local part\n    if not local or len(local) > 64:\n        return False\n    \n    # Validate domain part\n    if not domain or len(domain) > 253:\n        return False\n    \n    # Check for valid characters\n    local_pattern = r'^[a-zA-Z0-9.!#$%&\\'*+/=?^_`{|}~-]+$'\n    if not re.match(local_pattern, local):\n        return False\n    \n    # Domain must have at least one dot\n    if '.' not in domain:\n        return False\n    \n    # Check domain labels\n    labels = domain.split('.')\n    for label in labels:\n        if not label or len(label) > 63:\n            return False\n        if not re.match(r'^[a-zA-Z0-9-]+$', label):\n            return False\n        if label.startswith('-') or label.endswith('-'):\n            return False\n    \n    return True",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "cache_invalidation_007",
    "issue_description": "Cache invalidation for distributed system",
    "repo_context": "Distributed caching layer",
    "codex_solution": "import redis\nimport json\nimport time\n\nclass DistributedCache:\n    def __init__(self, redis_client):\n        self.redis = redis_client\n        self.ttl = 3600\n    \n    def get(self, key):\n        \"\"\"Get value from cache\"\"\"\n        value = self.redis.get(key)\n        if value:\n            return json.loads(value)\n        return None\n    \n    def set(self, key, value, ttl=None):\n        \"\"\"Set value in cache\"\"\"\n        ttl = ttl or self.ttl\n        serialized = json.dumps(value)\n        self.redis.setex(key, ttl, serialized)\n    \n    def invalidate_pattern(self, pattern):\n        \"\"\"Invalidate all keys matching pattern\"\"\"\n        # BUG: KEYS command blocks Redis and doesn't scale\n        # Also race condition between KEYS and DELETE\n        keys = self.redis.keys(pattern)\n        if keys:\n            for key in keys:\n                self.redis.delete(key)\n    \n    def get_or_compute(self, key, compute_fn, ttl=None):\n        \"\"\"Get from cache or compute and store\"\"\"\n        value = self.get(key)\n        if value is not None:\n            return value\n        \n        # Compute value\n        value = compute_fn()\n        self.set(key, value, ttl)\n        return value",
    "actual_issue": "Uses KEYS command which blocks Redis server and has O(n) complexity; race condition between getting keys and deleting them in distributed environment",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "binary_search_008",
    "issue_description": "Binary search implementation for sorted arrays",
    "repo_context": "Algorithm library for data processing",
    "codex_solution": "def binary_search(arr, target):\n    \"\"\"Binary search for target in sorted array\"\"\"\n    if not arr:\n        return -1\n    \n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\ndef find_insertion_point(arr, target):\n    \"\"\"Find position where target should be inserted\"\"\"\n    if not arr:\n        return 0\n    \n    left, right = 0, len(arr)\n    \n    while left < right:\n        mid = (left + right) // 2\n        \n        if arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid\n    \n    return left\n\ndef search_range(arr, target):\n    \"\"\"Find first and last position of target\"\"\"\n    if not arr:\n        return [-1, -1]\n    \n    # Find first position\n    left, right = 0, len(arr)\n    first = -1\n    \n    while left < right:\n        mid = (left + right) // 2\n        if arr[mid] >= target:\n            right = mid\n        else:\n            left = mid + 1\n    \n    if left < len(arr) and arr[left] == target:\n        first = left\n    else:\n        return [-1, -1]\n    \n    # Find last position\n    left, right = first, len(arr)\n    \n    while left < right:\n        mid = (left + right) // 2\n        if arr[mid] <= target:\n            left = mid + 1\n        else:\n            right = mid\n    \n    return [first, left - 1]",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "file_processor_009",
    "issue_description": "Process large CSV files in batches",
    "repo_context": "ETL pipeline for data import",
    "codex_solution": "import csv\n\ndef process_csv_file(file_path, batch_size=1000):\n    \"\"\"Process CSV file in batches\"\"\"\n    results = []\n    batch = []\n    \n    # BUG: File handle never closed if exception occurs\n    file_handle = open(file_path, 'r', encoding='utf-8')\n    reader = csv.DictReader(file_handle)\n    \n    for row in reader:\n        # Validate row\n        if not row.get('id') or not row.get('value'):\n            continue\n        \n        # Transform data\n        transformed = {\n            'id': int(row['id']),\n            'value': float(row['value']),\n            'processed': True\n        }\n        \n        batch.append(transformed)\n        \n        # Process batch\n        if len(batch) >= batch_size:\n            save_batch(batch)\n            results.extend(batch)\n            batch = []\n    \n    # Process remaining items\n    if batch:\n        save_batch(batch)\n        results.extend(batch)\n    \n    file_handle.close()\n    return results\n\ndef save_batch(batch):\n    \"\"\"Save batch to database\"\"\"\n    # Database insertion logic\n    pass",
    "actual_issue": "Resource leak: file handle not closed if exception occurs during processing; should use context manager (with statement)",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "concurrent_counter_010",
    "issue_description": "Thread-safe counter for concurrent operations",
    "repo_context": "Metrics collection system",
    "codex_solution": "import threading\n\nclass ConcurrentCounter:\n    def __init__(self):\n        self.counts = {}\n        self.lock = threading.Lock()\n    \n    def increment(self, key, amount=1):\n        \"\"\"Increment counter for key\"\"\"\n        with self.lock:\n            if key not in self.counts:\n                self.counts[key] = 0\n            self.counts[key] += amount\n    \n    def get(self, key):\n        \"\"\"Get current count for key\"\"\"\n        with self.lock:\n            return self.counts.get(key, 0)\n    \n    def get_all(self):\n        \"\"\"Get all counts as dictionary\"\"\"\n        with self.lock:\n            return self.counts.copy()\n    \n    def reset(self, key):\n        \"\"\"Reset counter for key\"\"\"\n        with self.lock:\n            if key in self.counts:\n                self.counts[key] = 0\n    \n    def increment_multiple(self, updates):\n        \"\"\"Increment multiple counters atomically\"\"\"\n        with self.lock:\n            for key, amount in updates.items():\n                if key not in self.counts:\n                    self.counts[key] = 0\n                self.counts[key] += amount",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "cache_invalidation_001",
    "issue_description": "LRU cache implementation with thread-safe access",
    "repo_context": "Caching layer for high-performance web application",
    "codex_solution": "import threading\nimport time\nfrom collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity=100):\n        self.capacity = capacity\n        self.cache = OrderedDict()\n        self.lock = threading.Lock()\n        self.stats = {'hits': 0, 'misses': 0}\n    \n    def get(self, key):\n        with self.lock:\n            if key in self.cache:\n                self.stats['hits'] += 1\n                self.cache.move_to_end(key)\n                return self.cache[key]\n            self.stats['misses'] += 1\n            return None\n    \n    def put(self, key, value):\n        # BUG: Lock released before eviction, race condition\n        if key in self.cache:\n            with self.lock:\n                self.cache.move_to_end(key)\n                self.cache[key] = value\n        else:\n            if len(self.cache) >= self.capacity:\n                oldest = next(iter(self.cache))\n                del self.cache[oldest]  # BUG: Not thread-safe\n            with self.lock:\n                self.cache[key] = value\n    \n    def clear(self):\n        self.cache.clear()  # BUG: No lock\n        self.stats = {'hits': 0, 'misses': 0}",
    "actual_issue": "Race conditions in put() and clear() methods - eviction and clearing happen outside lock, allowing concurrent modifications",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "password_validation_002",
    "issue_description": "Password strength validator for user registration",
    "repo_context": "Authentication module for enterprise application",
    "codex_solution": "import re\nimport hashlib\n\nclass PasswordValidator:\n    def __init__(self):\n        self.min_length = 8\n        self.common_passwords = ['password', '12345678', 'qwerty']\n    \n    def validate(self, password, username=None):\n        errors = []\n        \n        if len(password) < self.min_length:\n            errors.append(f'Password must be at least {self.min_length} characters')\n        \n        if not re.search(r'[A-Z]', password):\n            errors.append('Password must contain uppercase letter')\n        \n        if not re.search(r'[a-z]', password):\n            errors.append('Password must contain lowercase letter')\n        \n        if not re.search(r'\\d', password):\n            errors.append('Password must contain digit')\n        \n        # BUG: Case-sensitive comparison allows 'Password', 'PASSWORD'\n        if password in self.common_passwords:\n            errors.append('Password is too common')\n        \n        # BUG: Substring check allows 'admin123' for username 'admin'\n        if username and username in password:\n            errors.append('Password cannot contain username')\n        \n        return len(errors) == 0, errors\n    \n    def hash_password(self, password):\n        # BUG: MD5 is cryptographically broken\n        return hashlib.md5(password.encode()).hexdigest()",
    "actual_issue": "Multiple security bugs: case-sensitive common password check, weak substring username check, MD5 for password hashing instead of bcrypt/argon2",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "json_parser_003",
    "issue_description": "Custom JSON parser with type coercion",
    "repo_context": "Data processing pipeline for API responses",
    "codex_solution": "import json\nimport decimal\n\nclass SafeJSONParser:\n    def parse(self, json_string):\n        try:\n            data = json.loads(json_string)\n            return self._sanitize(data)\n        except json.JSONDecodeError as e:\n            raise ValueError(f'Invalid JSON: {e}')\n    \n    def _sanitize(self, obj):\n        if isinstance(obj, dict):\n            return {k: self._sanitize(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [self._sanitize(item) for item in obj]\n        elif isinstance(obj, str):\n            return obj.strip()\n        elif isinstance(obj, (int, float)):\n            # BUG: Float comparison with == is unreliable\n            if obj == float('inf') or obj == float('-inf'):\n                return None\n            return obj\n        return obj\n    \n    def parse_decimal(self, json_string):\n        # BUG: parse_float doesn't handle all numeric strings\n        return json.loads(json_string, parse_float=decimal.Decimal)",
    "actual_issue": "Float infinity comparison using == is unreliable, NaN not handled, parse_float doesn't handle integer fields that should be decimals",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "file_upload_004",
    "issue_description": "File upload handler with size and type validation",
    "repo_context": "Web application file upload endpoint",
    "codex_solution": "import os\nimport mimetypes\nfrom werkzeug.utils import secure_filename\n\nclass FileUploadHandler:\n    def __init__(self, upload_dir='/tmp/uploads'):\n        self.upload_dir = upload_dir\n        self.max_size = 10 * 1024 * 1024  # 10MB\n        self.allowed_extensions = {'.jpg', '.png', '.pdf', '.txt'}\n        os.makedirs(upload_dir, exist_ok=True)\n    \n    def save_file(self, file_obj, filename):\n        # Validate file size\n        file_obj.seek(0, 2)\n        size = file_obj.tell()\n        file_obj.seek(0)\n        \n        if size > self.max_size:\n            raise ValueError('File too large')\n        \n        # Validate extension\n        ext = os.path.splitext(filename)[1].lower()\n        if ext not in self.allowed_extensions:\n            raise ValueError('Invalid file type')\n        \n        # BUG: secure_filename can return empty string\n        safe_name = secure_filename(filename)\n        filepath = os.path.join(self.upload_dir, safe_name)\n        \n        # BUG: No check for existing files, can overwrite\n        with open(filepath, 'wb') as f:\n            f.write(file_obj.read())\n        \n        return filepath",
    "actual_issue": "secure_filename can return empty string for non-ASCII filenames causing file write errors, no collision handling allows file overwriting, no MIME type validation (only extension)",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "rate_limiter_005",
    "issue_description": "Token bucket rate limiter for API endpoints",
    "repo_context": "API rate limiting middleware",
    "codex_solution": "import time\nimport threading\nfrom collections import defaultdict\n\nclass TokenBucketRateLimiter:\n    def __init__(self, rate=10, capacity=20):\n        self.rate = rate  # tokens per second\n        self.capacity = capacity\n        self.buckets = defaultdict(lambda: {'tokens': capacity, 'last_update': time.time()})\n        self.lock = threading.Lock()\n    \n    def allow_request(self, client_id):\n        with self.lock:\n            bucket = self.buckets[client_id]\n            now = time.time()\n            \n            # Add tokens based on elapsed time\n            elapsed = now - bucket['last_update']\n            bucket['tokens'] = min(self.capacity, bucket['tokens'] + elapsed * self.rate)\n            bucket['last_update'] = now\n            \n            # Check if request allowed\n            if bucket['tokens'] >= 1:\n                bucket['tokens'] -= 1\n                return True\n            return False\n    \n    def reset(self, client_id):\n        with self.lock:\n            if client_id in self.buckets:\n                del self.buckets[client_id]",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "binary_search_006",
    "issue_description": "Binary search implementation with custom comparator",
    "repo_context": "Search utilities for sorted data structures",
    "codex_solution": "class BinarySearch:\n    @staticmethod\n    def search(arr, target, comparator=None):\n        if not arr:\n            return -1\n        \n        if comparator is None:\n            comparator = lambda a, b: (a > b) - (a < b)\n        \n        left, right = 0, len(arr) - 1\n        \n        while left <= right:\n            # BUG: Integer overflow possible in other languages,\n            # but also inefficient - should use left + (right - left) // 2\n            mid = (left + right) // 2\n            cmp = comparator(arr[mid], target)\n            \n            if cmp == 0:\n                return mid\n            elif cmp < 0:\n                left = mid + 1\n            else:\n                right = mid - 1\n        \n        return -1\n    \n    @staticmethod\n    def find_first(arr, target):\n        # BUG: Doesn't handle duplicates correctly\n        result = BinarySearch.search(arr, target)\n        # Should continue searching left for first occurrence\n        return result",
    "actual_issue": "find_first() returns any occurrence, not the first one when duplicates exist; should continue searching left after finding target",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "config_parser_007",
    "issue_description": "Configuration file parser with environment variable substitution",
    "repo_context": "Application configuration management",
    "codex_solution": "import os\nimport re\nimport json\n\nclass ConfigParser:\n    def __init__(self, config_file):\n        self.config_file = config_file\n        self.config = {}\n    \n    def load(self):\n        with open(self.config_file, 'r') as f:\n            raw_config = json.load(f)\n        \n        self.config = self._substitute_env_vars(raw_config)\n        return self.config\n    \n    def _substitute_env_vars(self, obj):\n        if isinstance(obj, dict):\n            return {k: self._substitute_env_vars(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [self._substitute_env_vars(item) for item in obj]\n        elif isinstance(obj, str):\n            # BUG: Regex doesn't handle ${VAR:-default} syntax\n            # BUG: No protection against infinite recursion\n            pattern = r'\\$\\{([A-Z_][A-Z0-9_]*)\\}'\n            return re.sub(pattern, lambda m: os.getenv(m.group(1), ''), obj)\n        return obj\n    \n    def get(self, key, default=None):\n        keys = key.split('.')\n        value = self.config\n        for k in keys:\n            if isinstance(value, dict):\n                value = value.get(k)\n                if value is None:\n                    return default\n            else:\n                return default\n        return value",
    "actual_issue": "Environment variable substitution doesn't support default values syntax ${VAR:-default}, missing variables become empty strings instead of raising errors, potential infinite recursion if env var references another",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "merkle_tree_008",
    "issue_description": "Merkle tree implementation for data integrity verification",
    "repo_context": "Blockchain-like data structure for tamper detection",
    "codex_solution": "import hashlib\nimport math\n\nclass MerkleTree:\n    def __init__(self, data_blocks):\n        self.leaves = [self._hash(block) for block in data_blocks]\n        self.tree = self._build_tree(self.leaves[:])\n    \n    def _hash(self, data):\n        if isinstance(data, str):\n            data = data.encode()\n        return hashlib.sha256(data).hexdigest()\n    \n    def _build_tree(self, nodes):\n        if len(nodes) == 0:\n            return [self._hash(b'')]\n        if len(nodes) == 1:\n            return nodes\n        \n        tree = nodes[:]\n        while len(nodes) > 1:\n            level = []\n            for i in range(0, len(nodes), 2):\n                if i + 1 < len(nodes):\n                    combined = nodes[i] + nodes[i + 1]\n                else:\n                    combined = nodes[i] + nodes[i]\n                level.append(self._hash(combined))\n            tree.extend(level)\n            nodes = level\n        return tree\n    \n    def get_root(self):\n        return self.tree[-1] if self.tree else None\n    \n    def verify_leaf(self, leaf_data, proof, index):\n        current = self._hash(leaf_data)\n        for sibling, direction in proof:\n            if direction == 'left':\n                current = self._hash(sibling + current)\n            else:\n                current = self._hash(current + sibling)\n        return current == self.get_root()",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "datetime_parser_009",
    "issue_description": "Flexible datetime parser supporting multiple formats",
    "repo_context": "Data import module handling various date formats",
    "codex_solution": "from datetime import datetime\nimport re\n\nclass DateTimeParser:\n    def __init__(self):\n        self.formats = [\n            '%Y-%m-%d %H:%M:%S',\n            '%Y-%m-%d',\n            '%m/%d/%Y',\n            '%d-%m-%Y',\n            '%Y%m%d'\n        ]\n    \n    def parse(self, date_string):\n        if not date_string or not isinstance(date_string, str):\n            raise ValueError('Invalid date string')\n        \n        date_string = date_string.strip()\n        \n        # Try each format\n        for fmt in self.formats:\n            try:\n                return datetime.strptime(date_string, fmt)\n            except ValueError:\n                continue\n        \n        # BUG: Ambiguous date format - 01/02/2023 could be Jan 2 or Feb 1\n        # Tries %m/%d/%Y first, may not match user expectation\n        \n        raise ValueError(f'Unable to parse date: {date_string}')\n    \n    def parse_range(self, start_str, end_str):\n        start = self.parse(start_str)\n        end = self.parse(end_str)\n        \n        # BUG: No validation that start <= end\n        return start, end",
    "actual_issue": "Ambiguous date format handling - %m/%d/%Y vs %d/%m/%Y tried in fixed order without locale awareness; parse_range doesn't validate start <= end",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "priority_queue_010",
    "issue_description": "Thread-safe priority queue with dynamic priority updates",
    "repo_context": "Task scheduling system for distributed workers",
    "codex_solution": "import heapq\nimport threading\nimport time\n\nclass PriorityQueue:\n    def __init__(self):\n        self.heap = []\n        self.entry_map = {}\n        self.counter = 0\n        self.lock = threading.Lock()\n        self.not_empty = threading.Condition(self.lock)\n    \n    def push(self, item, priority):\n        with self.lock:\n            entry = [priority, self.counter, item]\n            self.entry_map[item] = entry\n            heapq.heappush(self.heap, entry)\n            self.counter += 1\n            self.not_empty.notify()\n    \n    def pop(self, timeout=None):\n        with self.not_empty:\n            end_time = time.time() + timeout if timeout else None\n            while not self.heap:\n                if timeout is not None:\n                    remaining = end_time - time.time()\n                    if remaining <= 0:\n                        return None\n                    self.not_empty.wait(remaining)\n                else:\n                    self.not_empty.wait()\n            \n            priority, count, item = heapq.heappop(self.heap)\n            del self.entry_map[item]\n            return item\n    \n    def update_priority(self, item, new_priority):\n        with self.lock:\n            if item in self.entry_map:\n                old_entry = self.entry_map[item]\n                old_entry[0] = -1\n                new_entry = [new_priority, self.counter, item]\n                self.entry_map[item] = new_entry\n                heapq.heappush(self.heap, new_entry)\n                self.counter += 1\n                self.not_empty.notify()",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "jwt_auth_001",
    "issue_description": "Implement JWT token validation for API authentication",
    "repo_context": "REST API authentication middleware",
    "codex_solution": "import jwt\nfrom flask import request, jsonify\nfrom functools import wraps\n\nSECRET_KEY = 'my-secret-key'\n\ndef token_required(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        token = request.headers.get('Authorization')\n        \n        if not token:\n            return jsonify({'message': 'Token missing'}), 401\n        \n        try:\n            # BUG: algorithm not specified, allows 'none' algorithm attack\n            data = jwt.decode(token, SECRET_KEY, options={'verify_signature': True})\n            current_user = data['user_id']\n        except jwt.ExpiredSignatureError:\n            return jsonify({'message': 'Token expired'}), 401\n        except jwt.InvalidTokenError:\n            return jsonify({'message': 'Invalid token'}), 401\n        \n        return f(current_user, *args, **kwargs)\n    \n    return decorated\n\n@app.route('/api/protected', methods=['GET'])\n@token_required\ndef protected_route(current_user):\n    return jsonify({'data': f'Hello user {current_user}'})",
    "actual_issue": "Missing algorithm specification in jwt.decode allows attackers to use 'none' algorithm, bypassing signature verification entirely",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "cache_decorator_001",
    "issue_description": "Create a caching decorator for expensive function calls",
    "repo_context": "Performance optimization utility",
    "codex_solution": "from functools import wraps\nimport time\n\ndef cache_result(ttl=300):\n    def decorator(func):\n        cache = {}\n        \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # BUG: kwargs not included in cache key, lists/dicts not hashable\n            cache_key = args\n            current_time = time.time()\n            \n            if cache_key in cache:\n                result, timestamp = cache[cache_key]\n                if current_time - timestamp < ttl:\n                    return result\n            \n            result = func(*args, **kwargs)\n            cache[cache_key] = (result, current_time)\n            return result\n        \n        return wrapper\n    return decorator\n\n@cache_result(ttl=60)\ndef expensive_calculation(data_list, multiplier=2):\n    time.sleep(2)\n    return sum(data_list) * multiplier",
    "actual_issue": "Cache key uses args directly which fails with unhashable types (lists, dicts). Also ignores kwargs, causing incorrect cache hits when kwargs differ",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "pagination_query_001",
    "issue_description": "Implement database pagination for large result sets",
    "repo_context": "Database query optimization",
    "codex_solution": "def get_paginated_users(page=1, per_page=20, filters=None):\n    \"\"\"\n    Retrieve paginated user records with optional filters\n    \"\"\"\n    offset = (page - 1) * per_page\n    \n    query = \"SELECT * FROM users WHERE 1=1\"\n    params = []\n    \n    if filters:\n        if 'status' in filters:\n            query += \" AND status = %s\"\n            params.append(filters['status'])\n        if 'created_after' in filters:\n            query += \" AND created_at > %s\"\n            params.append(filters['created_after'])\n    \n    # BUG: No limit on per_page, attacker can request millions of records\n    query += f\" ORDER BY created_at DESC LIMIT {per_page} OFFSET {offset}\"\n    \n    cursor.execute(query, params)\n    results = cursor.fetchall()\n    \n    # BUG: Counting total without same filters\n    total_query = \"SELECT COUNT(*) FROM users\"\n    cursor.execute(total_query)\n    total = cursor.fetchone()[0]\n    \n    return {\n        'data': results,\n        'page': page,\n        'per_page': per_page,\n        'total': total\n    }",
    "actual_issue": "No validation on per_page parameter allows DoS attacks. Total count query doesn't apply same filters, returning incorrect pagination metadata",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "json_parser_001",
    "issue_description": "Parse and validate JSON configuration files",
    "repo_context": "Configuration management system",
    "codex_solution": "import json\nimport os\n\nclass ConfigParser:\n    def __init__(self, config_path):\n        self.config_path = config_path\n        self.config = {}\n    \n    def load_config(self):\n        \"\"\"Load and validate configuration from JSON file\"\"\"\n        if not os.path.exists(self.config_path):\n            raise FileNotFoundError(f\"Config file not found: {self.config_path}\")\n        \n        with open(self.config_path, 'r') as f:\n            self.config = json.load(f)\n        \n        self._validate_config()\n        return self.config\n    \n    def _validate_config(self):\n        \"\"\"Validate required configuration fields\"\"\"\n        required_fields = ['database', 'api_key', 'port']\n        \n        for field in required_fields:\n            if field not in self.config:\n                raise ValueError(f\"Missing required field: {field}\")\n        \n        if not isinstance(self.config['port'], int):\n            raise TypeError(\"Port must be an integer\")\n        \n        if self.config['port'] < 1 or self.config['port'] > 65535:\n            raise ValueError(\"Port must be between 1 and 65535\")\n    \n    def get(self, key, default=None):\n        return self.config.get(key, default)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "concurrent_counter_001",
    "issue_description": "Implement thread-safe counter for metrics collection",
    "repo_context": "Multi-threaded metrics system",
    "codex_solution": "import threading\nfrom collections import defaultdict\n\nclass MetricsCounter:\n    def __init__(self):\n        self.counters = defaultdict(int)\n        self.lock = threading.Lock()\n    \n    def increment(self, metric_name, value=1):\n        \"\"\"Increment a metric counter\"\"\"\n        with self.lock:\n            self.counters[metric_name] += value\n    \n    def get(self, metric_name):\n        \"\"\"Get current value of a metric\"\"\"\n        with self.lock:\n            return self.counters[metric_name]\n    \n    def get_all(self):\n        \"\"\"Get all metrics as dictionary\"\"\"\n        with self.lock:\n            return dict(self.counters)\n    \n    def reset(self, metric_name=None):\n        \"\"\"Reset specific metric or all metrics\"\"\"\n        with self.lock:\n            if metric_name:\n                self.counters[metric_name] = 0\n            else:\n                self.counters.clear()\n\n# Usage\ncounter = MetricsCounter()\ncounter.increment('api_calls')\ncounter.increment('errors', 2)\nprint(counter.get_all())",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "email_validator_001",
    "issue_description": "Validate email addresses with regex pattern",
    "repo_context": "User registration input validation",
    "codex_solution": "import re\n\ndef validate_email(email):\n    \"\"\"\n    Validate email address format\n    Returns True if valid, False otherwise\n    \"\"\"\n    if not email or not isinstance(email, str):\n        return False\n    \n    # BUG: Catastrophic backtracking vulnerability with nested quantifiers\n    pattern = r'^([a-zA-Z0-9_.+-]+)+@([a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}$'\n    \n    # BUG: No timeout, ReDoS attack possible\n    match = re.match(pattern, email)\n    \n    if not match:\n        return False\n    \n    # Additional checks\n    if len(email) > 254:\n        return False\n    \n    local, domain = email.rsplit('@', 1)\n    if len(local) > 64:\n        return False\n    \n    return True\n\ndef validate_user_input(user_data):\n    email = user_data.get('email', '')\n    return validate_email(email)",
    "actual_issue": "Regex pattern has catastrophic backtracking vulnerability. Input like 'aaaaaaaaaaaaaaaaaaaaaa@' causes exponential time complexity (ReDoS attack)",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "decimal_precision_001",
    "issue_description": "Calculate financial transactions with precision",
    "repo_context": "Payment processing system",
    "codex_solution": "def calculate_order_total(items, tax_rate=0.08, discount_percent=0):\n    \"\"\"\n    Calculate total cost with tax and discount\n    items: list of {'price': float, 'quantity': int}\n    \"\"\"\n    # BUG: Using float for money calculations causes precision errors\n    subtotal = 0.0\n    \n    for item in items:\n        # BUG: Multiplication and addition with floats accumulate errors\n        subtotal += item['price'] * item['quantity']\n    \n    # Apply discount\n    if discount_percent > 0:\n        discount_amount = subtotal * (discount_percent / 100.0)\n        subtotal -= discount_amount\n    \n    # Calculate tax\n    tax_amount = subtotal * tax_rate\n    total = subtotal + tax_amount\n    \n    # BUG: Rounding at the end doesn't fix accumulated precision errors\n    return round(total, 2)\n\n# Example usage\nitems = [\n    {'price': 10.10, 'quantity': 3},\n    {'price': 20.20, 'quantity': 2},\n    {'price': 5.55, 'quantity': 10}\n]\n\ntotal = calculate_order_total(items, tax_rate=0.08, discount_percent=5)",
    "actual_issue": "Using float for financial calculations causes precision errors. Should use Decimal type for accurate monetary calculations",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "rate_limiter_001",
    "issue_description": "Implement sliding window rate limiter",
    "repo_context": "API rate limiting middleware",
    "codex_solution": "import time\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, max_requests, window_seconds):\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self.requests = {}\n    \n    def allow_request(self, user_id):\n        \"\"\"Check if request should be allowed\"\"\"\n        current_time = time.time()\n        \n        if user_id not in self.requests:\n            self.requests[user_id] = deque()\n        \n        user_requests = self.requests[user_id]\n        \n        # Remove old requests outside window\n        while user_requests and user_requests[0] < current_time - self.window_seconds:\n            user_requests.popleft()\n        \n        # Check if under limit\n        if len(user_requests) < self.max_requests:\n            user_requests.append(current_time)\n            return True\n        \n        return False\n    \n    def get_retry_after(self, user_id):\n        \"\"\"Get seconds until next request allowed\"\"\"\n        if user_id not in self.requests or not self.requests[user_id]:\n            return 0\n        \n        oldest_request = self.requests[user_id][0]\n        return max(0, self.window_seconds - (time.time() - oldest_request))",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "binary_search_001",
    "issue_description": "Find element in sorted array using binary search",
    "repo_context": "Search algorithm implementation",
    "codex_solution": "def binary_search(arr, target):\n    \"\"\"\n    Find target in sorted array using binary search\n    Returns index if found, -1 otherwise\n    \"\"\"\n    left = 0\n    right = len(arr) - 1\n    \n    while left <= right:\n        # BUG: Integer overflow possible with large indices\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\ndef find_insertion_point(arr, target):\n    \"\"\"Find index where target should be inserted\"\"\"\n    left = 0\n    right = len(arr)\n    \n    while left < right:\n        # BUG: Same overflow issue\n        mid = (left + right) // 2\n        \n        if arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid\n    \n    return left",
    "actual_issue": "While Python handles large integers, the (left + right) calculation pattern is problematic and should use left + (right - left) // 2 for consistency with other languages and best practices",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "connection_pool_001",
    "issue_description": "Implement database connection pool with resource management",
    "repo_context": "Database connection management",
    "codex_solution": "import queue\nimport threading\nimport psycopg2\n\nclass ConnectionPool:\n    def __init__(self, db_config, pool_size=5):\n        self.db_config = db_config\n        self.pool_size = pool_size\n        self.pool = queue.Queue(maxsize=pool_size)\n        self.lock = threading.Lock()\n        self._initialize_pool()\n    \n    def _initialize_pool(self):\n        for _ in range(self.pool_size):\n            conn = psycopg2.connect(**self.db_config)\n            self.pool.put(conn)\n    \n    def get_connection(self, timeout=5):\n        try:\n            # BUG: No validation if connection is still alive\n            conn = self.pool.get(timeout=timeout)\n            return conn\n        except queue.Empty:\n            raise Exception(\"No connections available\")\n    \n    def return_connection(self, conn):\n        # BUG: No check if connection is in valid state or from this pool\n        try:\n            self.pool.put_nowait(conn)\n        except queue.Full:\n            # BUG: Silently closes connection if pool is full, losing the connection\n            conn.close()\n    \n    def close_all(self):\n        while not self.pool.empty():\n            conn = self.pool.get()\n            conn.close()",
    "actual_issue": "Multiple bugs: doesn't validate connection health before returning, doesn't verify connection belongs to pool, silently loses connections when pool is full",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "unique_id_7_001",
    "issue_description": "Password validation function for user registration",
    "repo_context": "Authentication module for web application",
    "codex_solution": "def validate_password(password, username):\n    \"\"\"Validate password meets security requirements\"\"\"\n    if len(password) < 8:\n        return False\n    \n    has_upper = any(c.isupper() for c in password)\n    has_lower = any(c.islower() for c in password)\n    has_digit = any(c.isdigit() for c in password)\n    \n    # BUG: Using 'in' operator is case-sensitive and can be bypassed\n    # username=\"John\" password=\"john123ABC\" would pass\n    if username in password:\n        return False\n    \n    # BUG: No check for special characters\n    # BUG: No check for common passwords\n    return has_upper and has_lower and has_digit",
    "actual_issue": "Case-sensitive username check allows bypass (e.g., 'John' vs 'john'), missing special character requirement, no common password validation",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_002",
    "issue_description": "Binary search implementation for sorted array",
    "repo_context": "Utility functions for data lookup",
    "codex_solution": "def binary_search(arr, target):\n    \"\"\"Find target in sorted array using binary search\"\"\"\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        # BUG: Integer overflow possible with large indices\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_7_003",
    "issue_description": "SQL query builder with user input",
    "repo_context": "Database access layer for customer management",
    "codex_solution": "def get_user_by_email(email):\n    \"\"\"Retrieve user record by email address\"\"\"\n    import sqlite3\n    \n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    \n    # BUG: SQL injection vulnerability - direct string interpolation\n    query = f\"SELECT * FROM users WHERE email = '{email}'\"\n    cursor.execute(query)\n    \n    result = cursor.fetchone()\n    conn.close()\n    \n    return result",
    "actual_issue": "Critical SQL injection vulnerability using f-string interpolation instead of parameterized queries",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_7_004",
    "issue_description": "Cache implementation with LRU eviction",
    "repo_context": "Performance optimization module",
    "codex_solution": "from collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n    \n    def get(self, key):\n        if key not in self.cache:\n            return -1\n        # Move to end to mark as recently used\n        self.cache.move_to_end(key)\n        return self.cache[key]\n    \n    def put(self, key, value):\n        if key in self.cache:\n            self.cache.move_to_end(key)\n        self.cache[key] = value\n        if len(self.cache) > self.capacity:\n            # Remove least recently used (first item)\n            self.cache.popitem(last=False)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_005",
    "issue_description": "Float comparison for financial calculations",
    "repo_context": "Payment processing system",
    "codex_solution": "def calculate_discount(price, discount_percent):\n    \"\"\"Calculate final price after discount\"\"\"\n    discount_amount = price * (discount_percent / 100)\n    final_price = price - discount_amount\n    \n    # BUG: Direct float comparison - precision issues\n    if final_price == 0.0:\n        return 0.0\n    \n    # BUG: No rounding for currency - can have precision errors\n    # Example: 10.00 * 0.29 = 2.8999999999999995\n    return final_price\n\ndef apply_coupon(total, coupon_value):\n    \"\"\"Apply coupon to total\"\"\"\n    new_total = total - coupon_value\n    \n    # BUG: Float comparison without epsilon tolerance\n    if new_total == 0.0:\n        return 0.0\n    elif new_total < 0:\n        return 0.0\n    \n    return new_total",
    "actual_issue": "Float comparison without epsilon tolerance causes precision errors in financial calculations, no proper rounding for currency values",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_006",
    "issue_description": "Thread-safe counter implementation",
    "repo_context": "Concurrent request tracking system",
    "codex_solution": "class RequestCounter:\n    def __init__(self):\n        self.count = 0\n    \n    def increment(self):\n        \"\"\"Increment request counter\"\"\"\n        # BUG: Race condition - not thread-safe\n        # Read-modify-write is not atomic\n        self.count += 1\n    \n    def get_count(self):\n        \"\"\"Get current count\"\"\"\n        return self.count\n    \n    def reset(self):\n        \"\"\"Reset counter\"\"\"\n        # BUG: Also not atomic\n        self.count = 0",
    "actual_issue": "Race condition in multi-threaded environment - increment operation is not atomic, missing lock mechanism",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "unique_id_7_007",
    "issue_description": "Parse and validate JSON configuration file",
    "repo_context": "Application configuration loader",
    "codex_solution": "import json\n\ndef load_config(file_path):\n    \"\"\"Load and validate configuration from JSON file\"\"\"\n    try:\n        with open(file_path, 'r') as f:\n            config = json.load(f)\n        \n        # Validate required fields\n        required = ['host', 'port', 'database']\n        for field in required:\n            if field not in config:\n                raise ValueError(f\"Missing required field: {field}\")\n        \n        # Validate types\n        if not isinstance(config['port'], int):\n            raise TypeError(\"Port must be an integer\")\n        \n        if config['port'] < 1 or config['port'] > 65535:\n            raise ValueError(\"Port must be between 1 and 65535\")\n        \n        return config\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Config file not found: {file_path}\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"Invalid JSON in config file: {e}\")",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_7_008",
    "issue_description": "Recursive directory traversal with file processing",
    "repo_context": "File system utility for log aggregation",
    "codex_solution": "import os\n\ndef process_directory(path, results=[]):\n    \"\"\"Recursively process all files in directory\"\"\"\n    # BUG: Mutable default argument - shared across calls\n    # results list will accumulate across multiple function calls\n    \n    for item in os.listdir(path):\n        full_path = os.path.join(path, item)\n        \n        if os.path.isfile(full_path):\n            # BUG: No error handling for file operations\n            with open(full_path, 'r') as f:\n                content = f.read()\n                results.append({'path': full_path, 'size': len(content)})\n        elif os.path.isdir(full_path):\n            # BUG: No symlink detection - infinite loop possible\n            process_directory(full_path, results)\n    \n    return results",
    "actual_issue": "Mutable default argument causes state pollution across calls, no symlink detection leads to infinite loops, missing error handling for file operations",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "unique_id_7_009",
    "issue_description": "User session timeout checker",
    "repo_context": "Web application session management",
    "codex_solution": "from datetime import datetime, timedelta\n\ndef is_session_valid(last_activity, timeout_minutes=30):\n    \"\"\"Check if user session is still valid\"\"\"\n    # BUG: Using naive datetime without timezone awareness\n    current_time = datetime.now()\n    \n    # BUG: Comparing potentially mixed timezone-aware/naive datetimes\n    time_diff = current_time - last_activity\n    \n    # BUG: No validation that last_activity isn't in the future\n    if time_diff.total_seconds() < timeout_minutes * 60:\n        return True\n    \n    return False",
    "actual_issue": "Timezone-naive datetime comparison can cause incorrect results across timezones, no validation for future timestamps, potential security issue with negative time differences",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_010",
    "issue_description": "Merge two sorted lists into one sorted list",
    "repo_context": "Data processing utility functions",
    "codex_solution": "def merge_sorted_lists(list1, list2):\n    \"\"\"Merge two sorted lists into a single sorted list\"\"\"\n    result = []\n    i, j = 0, 0\n    \n    while i < len(list1) and j < len(list2):\n        if list1[i] <= list2[j]:\n            result.append(list1[i])\n            i += 1\n        else:\n            result.append(list2[j])\n            j += 1\n    \n    # Append remaining elements\n    while i < len(list1):\n        result.append(list1[i])\n        i += 1\n    \n    while j < len(list2):\n        result.append(list2[j])\n        j += 1\n    \n    return result",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  }
]