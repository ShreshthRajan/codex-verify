[
  {
    "problem_id": "sql_injection_001",
    "issue_description": "User authentication system with database queries",
    "repo_context": "Authentication module for web application",
    "codex_solution": "import sqlite3\nimport hashlib\n\nclass UserAuth:\n    def __init__(self, db_path):\n        self.conn = sqlite3.connect(db_path)\n        self.cursor = self.conn.cursor()\n    \n    def authenticate_user(self, username, password):\n        \"\"\"Authenticate user against database\"\"\"\n        # BUG: SQL injection vulnerability - direct string concatenation\n        query = f\"SELECT * FROM users WHERE username = '{username}' AND password = '{password}'\"\n        self.cursor.execute(query)\n        result = self.cursor.fetchone()\n        \n        if result:\n            return {'status': 'success', 'user_id': result[0]}\n        return {'status': 'failed'}\n    \n    def get_user_data(self, user_id):\n        \"\"\"Retrieve user data by ID\"\"\"\n        # BUG: Another SQL injection point\n        query = \"SELECT * FROM users WHERE id = \" + str(user_id)\n        self.cursor.execute(query)\n        return self.cursor.fetchone()",
    "actual_issue": "SQL injection vulnerabilities through string concatenation in queries allowing attackers to bypass authentication or extract data",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "race_condition_001",
    "issue_description": "Bank account transfer system with concurrent transactions",
    "repo_context": "Financial transaction processing system",
    "codex_solution": "import threading\nimport time\n\nclass BankAccount:\n    def __init__(self, account_id, balance):\n        self.account_id = account_id\n        self.balance = balance\n    \n    def transfer(self, target_account, amount):\n        \"\"\"Transfer money to another account\"\"\"\n        # BUG: Race condition - check and update not atomic\n        if self.balance >= amount:\n            time.sleep(0.001)  # Simulate processing delay\n            self.balance -= amount\n            target_account.balance += amount\n            return True\n        return False\n    \n    def withdraw(self, amount):\n        \"\"\"Withdraw money from account\"\"\"\n        # BUG: No locking mechanism\n        if self.balance >= amount:\n            current = self.balance\n            time.sleep(0.001)\n            self.balance = current - amount\n            return True\n        return False\n\ndef concurrent_withdrawals(account):\n    for _ in range(5):\n        account.withdraw(100)",
    "actual_issue": "Race condition in balance checks and updates allowing overdrafts when multiple threads access the same account simultaneously",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "path_traversal_001",
    "issue_description": "File serving system for user uploads",
    "repo_context": "Web application file management module",
    "codex_solution": "import os\nimport mimetypes\n\nclass FileServer:\n    def __init__(self, base_dir):\n        self.base_dir = base_dir\n    \n    def get_file(self, filename):\n        \"\"\"Retrieve file from user directory\"\"\"\n        # BUG: Path traversal vulnerability - no sanitization\n        file_path = os.path.join(self.base_dir, filename)\n        \n        if os.path.exists(file_path):\n            with open(file_path, 'rb') as f:\n                return f.read()\n        return None\n    \n    def save_user_file(self, username, filename, content):\n        \"\"\"Save uploaded file to user directory\"\"\"\n        # BUG: No validation of filename\n        user_dir = os.path.join(self.base_dir, username)\n        file_path = os.path.join(user_dir, filename)\n        \n        os.makedirs(user_dir, exist_ok=True)\n        with open(file_path, 'wb') as f:\n            f.write(content)\n        return file_path",
    "actual_issue": "Path traversal vulnerability allowing attackers to access files outside intended directory using '../' sequences",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "memory_leak_001",
    "issue_description": "Event listener system with subscription management",
    "repo_context": "Event-driven application framework",
    "codex_solution": "class EventManager:\n    def __init__(self):\n        self.listeners = {}\n        self.event_history = []\n    \n    def subscribe(self, event_name, callback):\n        \"\"\"Subscribe to an event\"\"\"\n        if event_name not in self.listeners:\n            self.listeners[event_name] = []\n        self.listeners[event_name].append(callback)\n    \n    def emit(self, event_name, data):\n        \"\"\"Emit an event to all subscribers\"\"\"\n        # BUG: Unbounded memory growth - history never cleaned\n        self.event_history.append({\n            'event': event_name,\n            'data': data,\n            'timestamp': time.time()\n        })\n        \n        if event_name in self.listeners:\n            for callback in self.listeners[event_name]:\n                callback(data)\n    \n    def unsubscribe(self, event_name, callback):\n        \"\"\"Unsubscribe from event\"\"\"\n        # BUG: Only removes one instance, doesn't handle all cases\n        if event_name in self.listeners:\n            self.listeners[event_name].remove(callback)",
    "actual_issue": "Memory leak from unbounded event history growth and incomplete unsubscribe implementation that may leave dangling references",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "crypto_weak_001",
    "issue_description": "Password hashing and token generation system",
    "repo_context": "User security module",
    "codex_solution": "import hashlib\nimport random\nimport string\n\nclass SecurityManager:\n    def __init__(self):\n        self.tokens = {}\n    \n    def hash_password(self, password):\n        \"\"\"Hash user password for storage\"\"\"\n        # BUG: Using MD5 which is cryptographically broken\n        return hashlib.md5(password.encode()).hexdigest()\n    \n    def generate_session_token(self, user_id):\n        \"\"\"Generate session token for user\"\"\"\n        # BUG: Using weak random for security-critical token\n        token = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(16))\n        self.tokens[token] = user_id\n        return token\n    \n    def verify_password(self, password, stored_hash):\n        \"\"\"Verify password against stored hash\"\"\"\n        return self.hash_password(password) == stored_hash\n    \n    def generate_reset_code(self):\n        \"\"\"Generate password reset code\"\"\"\n        # BUG: Predictable random code\n        return str(random.randint(100000, 999999))",
    "actual_issue": "Weak cryptography using MD5 for passwords and predictable random for security tokens, making system vulnerable to attacks",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "float_precision_001",
    "issue_description": "Financial calculation system for monetary transactions",
    "repo_context": "Payment processing module",
    "codex_solution": "class PaymentProcessor:\n    def __init__(self):\n        self.transactions = []\n    \n    def calculate_total(self, items):\n        \"\"\"Calculate total price with tax\"\"\"\n        # BUG: Using float for money calculations\n        subtotal = 0.0\n        for item in items:\n            subtotal += item['price'] * item['quantity']\n        \n        tax = subtotal * 0.0825  # 8.25% tax\n        total = subtotal + tax\n        return total\n    \n    def split_bill(self, total, num_people):\n        \"\"\"Split bill among people\"\"\"\n        # BUG: Float division can cause precision errors\n        per_person = total / num_people\n        return per_person\n    \n    def calculate_discount(self, price, discount_percent):\n        \"\"\"Apply discount to price\"\"\"\n        # BUG: Accumulating float errors\n        discount = price * (discount_percent / 100.0)\n        return price - discount",
    "actual_issue": "Using floating-point arithmetic for monetary calculations leading to precision errors and rounding issues",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "xml_bomb_001",
    "issue_description": "XML parser for configuration files",
    "repo_context": "Configuration management system",
    "codex_solution": "import xml.etree.ElementTree as ET\nimport xml.dom.minidom\n\nclass ConfigParser:\n    def __init__(self):\n        self.config = {}\n    \n    def parse_xml_config(self, xml_string):\n        \"\"\"Parse XML configuration from string\"\"\"\n        # BUG: No protection against XML bombs (billion laughs attack)\n        root = ET.fromstring(xml_string)\n        \n        for child in root:\n            self.config[child.tag] = child.text\n        return self.config\n    \n    def parse_xml_file(self, filepath):\n        \"\"\"Parse XML configuration from file\"\"\"\n        # BUG: No size limit or entity expansion protection\n        tree = ET.parse(filepath)\n        root = tree.getroot()\n        \n        for element in root.iter():\n            if element.text:\n                self.config[element.tag] = element.text\n        return self.config",
    "actual_issue": "XML external entity (XXE) and billion laughs attack vulnerabilities due to unsafe XML parsing without entity expansion limits",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "correct_cache_001",
    "issue_description": "Thread-safe LRU cache implementation",
    "repo_context": "Caching layer for database queries",
    "codex_solution": "from collections import OrderedDict\nimport threading\n\nclass LRUCache:\n    def __init__(self, capacity):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n        self.lock = threading.Lock()\n    \n    def get(self, key):\n        \"\"\"Get value from cache\"\"\"\n        with self.lock:\n            if key not in self.cache:\n                return None\n            self.cache.move_to_end(key)\n            return self.cache[key]\n    \n    def put(self, key, value):\n        \"\"\"Put value in cache\"\"\"\n        with self.lock:\n            if key in self.cache:\n                self.cache.move_to_end(key)\n            self.cache[key] = value\n            if len(self.cache) > self.capacity:\n                self.cache.popitem(last=False)\n    \n    def clear(self):\n        \"\"\"Clear all cache entries\"\"\"\n        with self.lock:\n            self.cache.clear()",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "correct_validator_001",
    "issue_description": "Input validation for user registration",
    "repo_context": "User registration system",
    "codex_solution": "import re\nfrom typing import Dict, List\n\nclass UserValidator:\n    def __init__(self):\n        self.email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n        self.username_pattern = re.compile(r'^[a-zA-Z0-9_]{3,20}$')\n    \n    def validate_email(self, email: str) -> bool:\n        \"\"\"Validate email format\"\"\"\n        if not email or len(email) > 254:\n            return False\n        return bool(self.email_pattern.match(email))\n    \n    def validate_username(self, username: str) -> bool:\n        \"\"\"Validate username format\"\"\"\n        if not username:\n            return False\n        return bool(self.username_pattern.match(username))\n    \n    def validate_password(self, password: str) -> Dict[str, any]:\n        \"\"\"Validate password strength\"\"\"\n        errors = []\n        if len(password) < 8:\n            errors.append('Password must be at least 8 characters')\n        if not re.search(r'[A-Z]', password):\n            errors.append('Password must contain uppercase letter')\n        if not re.search(r'[a-z]', password):\n            errors.append('Password must contain lowercase letter')\n        if not re.search(r'\\d', password):\n            errors.append('Password must contain digit')\n        \n        return {'valid': len(errors) == 0, 'errors': errors}",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "correct_retry_001",
    "issue_description": "Retry mechanism with exponential backoff",
    "repo_context": "API client with fault tolerance",
    "codex_solution": "import time\nimport random\nfrom typing import Callable, Any\n\nclass RetryHandler:\n    def __init__(self, max_retries=3, base_delay=1.0, max_delay=60.0):\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n    \n    def execute_with_retry(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Execute function with exponential backoff retry\"\"\"\n        last_exception = None\n        \n        for attempt in range(self.max_retries + 1):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                last_exception = e\n                \n                if attempt < self.max_retries:\n                    delay = min(self.base_delay * (2 ** attempt), self.max_delay)\n                    jitter = random.uniform(0, delay * 0.1)\n                    time.sleep(delay + jitter)\n                else:\n                    raise last_exception\n        \n        raise last_exception",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_2_001",
    "issue_description": "User authentication with session management",
    "repo_context": "Web application authentication module",
    "codex_solution": "import hashlib\nimport time\n\ndef authenticate_user(username, password, stored_hash):\n    \"\"\"Authenticate user with password hash comparison\"\"\"\n    if not username or not password:\n        return False\n    \n    # Hash the provided password\n    password_hash = hashlib.md5(password.encode()).hexdigest()\n    \n    # Compare with stored hash\n    if password_hash == stored_hash:\n        session_token = generate_session_token(username)\n        return True, session_token\n    \n    return False, None\n\ndef generate_session_token(username):\n    \"\"\"Generate session token for authenticated user\"\"\"\n    timestamp = str(time.time())\n    token = hashlib.md5((username + timestamp).encode()).hexdigest()\n    return token",
    "actual_issue": "Uses MD5 for password hashing which is cryptographically broken and vulnerable to rainbow table attacks. Should use bcrypt, scrypt, or argon2.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_2_002",
    "issue_description": "Calculate average of numeric list",
    "repo_context": "Data processing utility functions",
    "codex_solution": "def calculate_average(numbers):\n    \"\"\"Calculate the average of a list of numbers\"\"\"\n    if not numbers:\n        return 0\n    \n    total = sum(numbers)\n    average = total / len(numbers)\n    \n    return round(average, 2)\n\ndef get_statistics(data):\n    \"\"\"Get basic statistics from numeric data\"\"\"\n    if not data:\n        return {'average': 0, 'min': 0, 'max': 0}\n    \n    return {\n        'average': calculate_average(data),\n        'min': min(data),\n        'max': max(data),\n        'count': len(data)\n    }",
    "actual_issue": "None - correct code with proper empty list handling",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_2_003",
    "issue_description": "Parse and validate JSON configuration",
    "repo_context": "Configuration loader for application settings",
    "codex_solution": "import json\n\ndef load_config(config_path):\n    \"\"\"Load and validate JSON configuration file\"\"\"\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    \n    # Validate required fields\n    required_fields = ['database', 'api_key', 'port']\n    for field in required_fields:\n        if field not in config:\n            raise ValueError(f\"Missing required field: {field}\")\n    \n    # Validate port number\n    if config['port'] < 1024 or config['port'] > 65535:\n        raise ValueError(\"Port must be between 1024 and 65535\")\n    \n    return config",
    "actual_issue": "File handle is not properly closed if an exception occurs during validation. Should use try-finally or context manager properly.",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_2_004",
    "issue_description": "Find duplicate elements in array",
    "repo_context": "Array manipulation utilities",
    "codex_solution": "def find_duplicates(arr):\n    \"\"\"Find all duplicate elements in an array\"\"\"\n    seen = set()\n    duplicates = set()\n    \n    for item in arr:\n        if item in seen:\n            duplicates.add(item)\n        else:\n            seen.add(item)\n    \n    return list(duplicates)\n\ndef has_duplicates(arr):\n    \"\"\"Check if array contains any duplicates\"\"\"\n    return len(arr) != len(set(arr))",
    "actual_issue": "None - correct code with efficient O(n) time complexity using sets",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_2_005",
    "issue_description": "Execute SQL query with user input",
    "repo_context": "Database query interface for user search",
    "codex_solution": "import sqlite3\n\ndef search_users(database_path, search_term):\n    \"\"\"Search for users by name or email\"\"\"\n    conn = sqlite3.connect(database_path)\n    cursor = conn.cursor()\n    \n    # Build and execute search query\n    query = f\"SELECT * FROM users WHERE name LIKE '%{search_term}%' OR email LIKE '%{search_term}%'\"\n    cursor.execute(query)\n    \n    results = cursor.fetchall()\n    conn.close()\n    \n    return results",
    "actual_issue": "SQL injection vulnerability - user input is directly concatenated into SQL query without parameterization or sanitization.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_2_006",
    "issue_description": "Merge sorted arrays efficiently",
    "repo_context": "Array sorting and merging utilities",
    "codex_solution": "def merge_sorted_arrays(arr1, arr2):\n    \"\"\"Merge two sorted arrays into one sorted array\"\"\"\n    result = []\n    i, j = 0, 0\n    \n    while i < len(arr1) and j < len(arr2):\n        if arr1[i] <= arr2[j]:\n            result.append(arr1[i])\n            i += 1\n        else:\n            result.append(arr2[j])\n            j += 1\n    \n    # Append remaining elements\n    result.extend(arr1[i:])\n    result.extend(arr2[j:])\n    \n    return result",
    "actual_issue": "None - correct code implementing efficient two-pointer merge algorithm",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_2_007",
    "issue_description": "Cache expensive computation results",
    "repo_context": "Performance optimization for data processing",
    "codex_solution": "cache = {}\n\ndef expensive_computation(n):\n    \"\"\"Perform expensive computation with caching\"\"\"\n    if n in cache:\n        return cache[n]\n    \n    # Simulate expensive operation\n    result = 0\n    for i in range(n):\n        result += i ** 2\n    \n    cache[n] = result\n    return result\n\ndef process_batch(numbers):\n    \"\"\"Process batch of numbers with caching\"\"\"\n    results = []\n    for num in numbers:\n        results.append(expensive_computation(num))\n    return results",
    "actual_issue": "Global cache dictionary grows unbounded leading to memory leak. No cache eviction policy or size limit implemented.",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_2_008",
    "issue_description": "Parse date string to datetime object",
    "repo_context": "Date/time utilities for data processing",
    "codex_solution": "from datetime import datetime\n\ndef parse_date(date_string, format_string='%Y-%m-%d'):\n    \"\"\"Parse date string to datetime object\"\"\"\n    try:\n        date_obj = datetime.strptime(date_string, format_string)\n        return date_obj\n    except ValueError:\n        return None\n\ndef calculate_days_between(date1_str, date2_str):\n    \"\"\"Calculate days between two date strings\"\"\"\n    date1 = parse_date(date1_str)\n    date2 = parse_date(date2_str)\n    \n    if date1 and date2:\n        delta = date2 - date1\n        return abs(delta.days)\n    return None",
    "actual_issue": "Does not handle None return values properly - will raise AttributeError when date1 or date2 is None in subtraction operation.",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_2_009",
    "issue_description": "Validate email address format",
    "repo_context": "User input validation module",
    "codex_solution": "import re\n\ndef validate_email(email):\n    \"\"\"Validate email address format\"\"\"\n    if not email or not isinstance(email, str):\n        return False\n    \n    # Email regex pattern\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    \n    if re.match(pattern, email):\n        return True\n    return False\n\ndef sanitize_email(email):\n    \"\"\"Sanitize and validate email\"\"\"\n    if not email:\n        return None\n    \n    email = email.strip().lower()\n    return email if validate_email(email) else None",
    "actual_issue": "Regex pattern allows consecutive dots in local part (e.g., user..name@example.com) which is invalid per RFC 5322.",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "unique_id_2_010",
    "issue_description": "Thread-safe counter implementation",
    "repo_context": "Concurrent request counting for API rate limiting",
    "codex_solution": "import threading\n\nclass ThreadSafeCounter:\n    \"\"\"Thread-safe counter for concurrent access\"\"\"\n    \n    def __init__(self):\n        self.count = 0\n        self.lock = threading.Lock()\n    \n    def increment(self):\n        \"\"\"Increment counter by 1\"\"\"\n        with self.lock:\n            self.count += 1\n    \n    def decrement(self):\n        \"\"\"Decrement counter by 1\"\"\"\n        with self.lock:\n            self.count -= 1\n    \n    def get_value(self):\n        \"\"\"Get current counter value\"\"\"\n        with self.lock:\n            return self.count\n    \n    def reset(self):\n        \"\"\"Reset counter to zero\"\"\"\n        with self.lock:\n            self.count = 0",
    "actual_issue": "None - correct code with proper lock-based synchronization for thread safety",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "auth_bypass_001",
    "issue_description": "JWT token validation with signature verification",
    "repo_context": "Authentication middleware for REST API",
    "codex_solution": "import jwt\nimport os\nfrom functools import wraps\nfrom flask import request, jsonify\n\nSECRET_KEY = os.getenv('JWT_SECRET', 'default-secret')\n\ndef require_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        token = request.headers.get('Authorization', '').replace('Bearer ', '')\n        \n        if not token:\n            return jsonify({'error': 'No token provided'}), 401\n        \n        try:\n            # BUG: verify=False disables signature verification!\n            payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256'], options={'verify_signature': False})\n            request.user_id = payload.get('user_id')\n            return f(*args, **kwargs)\n        except jwt.ExpiredSignatureError:\n            return jsonify({'error': 'Token expired'}), 401\n        except jwt.InvalidTokenError:\n            return jsonify({'error': 'Invalid token'}), 401\n    \n    return decorated\n\n@app.route('/api/admin/users')\n@require_auth\ndef get_admin_users():\n    return jsonify({'users': User.query.all()})",
    "actual_issue": "JWT signature verification is disabled with verify_signature=False, allowing attackers to forge tokens with arbitrary user_id claims",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "race_condition_002",
    "issue_description": "Bank account transfer with balance checks",
    "repo_context": "Financial transaction processing system",
    "codex_solution": "import threading\nimport time\n\nclass BankAccount:\n    def __init__(self, account_id, balance):\n        self.account_id = account_id\n        self.balance = balance\n        self.lock = threading.Lock()\n    \n    def transfer(self, target_account, amount):\n        \"\"\"Transfer money between accounts\"\"\"\n        if amount <= 0:\n            raise ValueError('Amount must be positive')\n        \n        # BUG: Check-then-act race condition\n        if self.balance >= amount:\n            time.sleep(0.001)  # Simulate processing delay\n            self.balance -= amount\n            target_account.balance += amount\n            return True\n        else:\n            raise ValueError('Insufficient funds')\n    \n    def get_balance(self):\n        with self.lock:\n            return self.balance\n\ndef concurrent_transfer(account_a, account_b, amount):\n    \"\"\"Execute transfer in thread\"\"\"\n    try:\n        account_a.transfer(account_b, amount)\n        print(f'Transfer of {amount} completed')\n    except ValueError as e:\n        print(f'Transfer failed: {e}')",
    "actual_issue": "Race condition: balance check and deduction are not atomic. Multiple concurrent transfers can overdraw the account because the lock is only used in get_balance, not in transfer",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "sql_injection_003",
    "issue_description": "User search functionality with filtering",
    "repo_context": "User management API endpoint",
    "codex_solution": "from flask import request, jsonify\nimport sqlite3\n\n@app.route('/api/users/search', methods=['GET'])\ndef search_users():\n    \"\"\"Search users by various criteria\"\"\"\n    search_term = request.args.get('q', '')\n    role = request.args.get('role', '')\n    status = request.args.get('status', 'active')\n    \n    conn = sqlite3.connect('users.db')\n    cursor = conn.cursor()\n    \n    # Build query with filters\n    query = \"SELECT id, username, email, role FROM users WHERE status = ?\"\n    params = [status]\n    \n    if search_term:\n        # BUG: String concatenation allows SQL injection\n        query += f\" AND (username LIKE '%{search_term}%' OR email LIKE '%{search_term}%')\"\n    \n    if role:\n        query += \" AND role = ?\"\n        params.append(role)\n    \n    cursor.execute(query, params)\n    results = cursor.fetchall()\n    conn.close()\n    \n    users = [{'id': r[0], 'username': r[1], 'email': r[2], 'role': r[3]} for r in results]\n    return jsonify({'users': users, 'count': len(users)})",
    "actual_issue": "SQL injection vulnerability in search_term parameter - uses string formatting instead of parameterized query, allowing arbitrary SQL execution",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "memory_leak_004",
    "issue_description": "Event listener registration system",
    "repo_context": "WebSocket connection manager with event handlers",
    "codex_solution": "import weakref\nfrom collections import defaultdict\n\nclass EventManager:\n    def __init__(self):\n        self.listeners = defaultdict(list)\n        self.event_history = []\n    \n    def subscribe(self, event_type, callback, context=None):\n        \"\"\"Subscribe to events\"\"\"\n        # BUG: Storing strong references to callbacks with context\n        listener = {'callback': callback, 'context': context}\n        self.listeners[event_type].append(listener)\n        return len(self.listeners[event_type]) - 1\n    \n    def emit(self, event_type, data):\n        \"\"\"Emit event to all subscribers\"\"\"\n        event_record = {'type': event_type, 'data': data, 'timestamp': time.time()}\n        # BUG: Unbounded list growth\n        self.event_history.append(event_record)\n        \n        for listener in self.listeners.get(event_type, []):\n            try:\n                if listener['context']:\n                    listener['callback'](listener['context'], data)\n                else:\n                    listener['callback'](data)\n            except Exception as e:\n                print(f'Error in listener: {e}')\n    \n    def unsubscribe(self, event_type, index):\n        \"\"\"Unsubscribe from events\"\"\"\n        if event_type in self.listeners and index < len(self.listeners[event_type]):\n            self.listeners[event_type][index] = None",
    "actual_issue": "Memory leak: event_history grows unbounded and listeners are never truly removed (set to None but remain in list). Context objects cannot be garbage collected",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "good_cache_001",
    "issue_description": "Thread-safe LRU cache implementation",
    "repo_context": "Caching layer for database queries",
    "codex_solution": "import threading\nfrom collections import OrderedDict\nimport time\n\nclass ThreadSafeLRUCache:\n    def __init__(self, capacity, ttl=300):\n        self.capacity = capacity\n        self.ttl = ttl\n        self.cache = OrderedDict()\n        self.lock = threading.RLock()\n    \n    def get(self, key):\n        \"\"\"Get value from cache\"\"\"\n        with self.lock:\n            if key not in self.cache:\n                return None\n            \n            value, timestamp = self.cache[key]\n            \n            # Check if expired\n            if time.time() - timestamp > self.ttl:\n                del self.cache[key]\n                return None\n            \n            # Move to end (most recently used)\n            self.cache.move_to_end(key)\n            return value\n    \n    def put(self, key, value):\n        \"\"\"Put value in cache\"\"\"\n        with self.lock:\n            if key in self.cache:\n                del self.cache[key]\n            elif len(self.cache) >= self.capacity:\n                # Remove least recently used\n                self.cache.popitem(last=False)\n            \n            self.cache[key] = (value, time.time())\n    \n    def clear(self):\n        \"\"\"Clear all cached items\"\"\"\n        with self.lock:\n            self.cache.clear()",
    "actual_issue": "None - correct code with proper thread safety, TTL handling, and LRU eviction",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "float_comparison_005",
    "issue_description": "Financial calculation with precise amounts",
    "repo_context": "Payment processing system calculating totals",
    "codex_solution": "def calculate_invoice_total(line_items, tax_rate, discount_percent=0):\n    \"\"\"Calculate invoice total with tax and discount\"\"\"\n    subtotal = 0.0\n    \n    for item in line_items:\n        price = item['price']\n        quantity = item['quantity']\n        subtotal += price * quantity\n    \n    # Apply discount\n    discount_amount = subtotal * (discount_percent / 100.0)\n    subtotal_after_discount = subtotal - discount_amount\n    \n    # Calculate tax\n    tax_amount = subtotal_after_discount * (tax_rate / 100.0)\n    total = subtotal_after_discount + tax_amount\n    \n    # BUG: Direct float comparison for financial amounts\n    if total == 0.0:\n        return {'total': 0, 'tax': 0, 'discount': 0}\n    \n    # BUG: Float precision issues in financial calculations\n    return {\n        'subtotal': subtotal,\n        'discount': discount_amount,\n        'tax': tax_amount,\n        'total': total,\n        'is_zero': total == 0.0\n    }",
    "actual_issue": "Using float for financial calculations leads to precision errors. Should use Decimal type. Float comparison with == is unreliable for monetary values",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "path_traversal_006",
    "issue_description": "File download endpoint for user documents",
    "repo_context": "Document management API",
    "codex_solution": "import os\nfrom flask import send_file, request, abort\n\nDOCUMENT_ROOT = '/var/app/documents'\n\n@app.route('/api/documents/download/<path:filename>')\ndef download_document(filename):\n    \"\"\"Download user document\"\"\"\n    user_id = request.args.get('user_id')\n    \n    if not user_id:\n        abort(400, 'User ID required')\n    \n    # BUG: Insufficient path traversal protection\n    # Only checks for '..' but not encoded versions\n    if '..' in filename:\n        abort(400, 'Invalid filename')\n    \n    # Construct file path\n    file_path = os.path.join(DOCUMENT_ROOT, user_id, filename)\n    \n    # Check if file exists\n    if not os.path.exists(file_path):\n        abort(404, 'File not found')\n    \n    # Send file\n    return send_file(file_path, as_attachment=True)\n\n@app.route('/api/documents/list')\ndef list_documents():\n    \"\"\"List user documents\"\"\"\n    user_id = request.args.get('user_id', '')\n    user_dir = os.path.join(DOCUMENT_ROOT, user_id)\n    \n    if os.path.isdir(user_dir):\n        files = os.listdir(user_dir)\n        return {'files': files}\n    return {'files': []}",
    "actual_issue": "Path traversal vulnerability: doesn't prevent encoded traversal sequences (%2e%2e%2f), doesn't validate user_id, and doesn't use realpath() to verify final path is within DOCUMENT_ROOT",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "good_validator_002",
    "issue_description": "Input validation for user registration",
    "repo_context": "User registration API with comprehensive validation",
    "codex_solution": "import re\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\n\nclass ValidationError(Exception):\n    pass\n\nclass UserValidator:\n    EMAIL_PATTERN = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n    USERNAME_PATTERN = re.compile(r'^[a-zA-Z0-9_]{3,20}$')\n    \n    @staticmethod\n    def validate_email(email: str) -> str:\n        if not email or not isinstance(email, str):\n            raise ValidationError('Email is required')\n        \n        email = email.strip().lower()\n        if not UserValidator.EMAIL_PATTERN.match(email):\n            raise ValidationError('Invalid email format')\n        \n        return email\n    \n    @staticmethod\n    def validate_username(username: str) -> str:\n        if not username or not isinstance(username, str):\n            raise ValidationError('Username is required')\n        \n        username = username.strip()\n        if not UserValidator.USERNAME_PATTERN.match(username):\n            raise ValidationError('Username must be 3-20 alphanumeric characters or underscores')\n        \n        return username\n    \n    @staticmethod\n    def validate_password(password: str) -> None:\n        if not password or not isinstance(password, str):\n            raise ValidationError('Password is required')\n        \n        if len(password) < 8:\n            raise ValidationError('Password must be at least 8 characters')\n        \n        if not any(c.isupper() for c in password):\n            raise ValidationError('Password must contain uppercase letter')\n        \n        if not any(c.isdigit() for c in password):\n            raise ValidationError('Password must contain a digit')\n    \n    @classmethod\n    def validate_registration(cls, data: Dict) -> Dict:\n        validated = {}\n        validated['email'] = cls.validate_email(data.get('email'))\n        validated['username'] = cls.validate_username(data.get('username'))\n        cls.validate_password(data.get('password'))\n        return validated",
    "actual_issue": "None - correct code with proper input validation, type checking, and sanitization",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "deadlock_007",
    "issue_description": "Resource allocation with multiple locks",
    "repo_context": "Multi-threaded resource management system",
    "codex_solution": "import threading\nimport time\n\nclass ResourceManager:\n    def __init__(self):\n        self.resource_a = {'lock': threading.Lock(), 'value': 0}\n        self.resource_b = {'lock': threading.Lock(), 'value': 0}\n    \n    def transfer_a_to_b(self, amount):\n        \"\"\"Transfer from resource A to B\"\"\"\n        # BUG: Inconsistent lock ordering leads to deadlock\n        with self.resource_a['lock']:\n            time.sleep(0.01)  # Simulate work\n            with self.resource_b['lock']:\n                self.resource_a['value'] -= amount\n                self.resource_b['value'] += amount\n                print(f'Transferred {amount} from A to B')\n    \n    def transfer_b_to_a(self, amount):\n        \"\"\"Transfer from resource B to A\"\"\"\n        # BUG: Locks acquired in opposite order\n        with self.resource_b['lock']:\n            time.sleep(0.01)  # Simulate work\n            with self.resource_a['lock']:\n                self.resource_b['value'] -= amount\n                self.resource_a['value'] += amount\n                print(f'Transferred {amount} from B to A')\n    \n    def get_totals(self):\n        with self.resource_a['lock']:\n            with self.resource_b['lock']:\n                return self.resource_a['value'], self.resource_b['value']",
    "actual_issue": "Deadlock vulnerability: transfer_a_to_b and transfer_b_to_a acquire locks in opposite order, creating circular wait condition when called concurrently",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "good_retry_003",
    "issue_description": "Robust API client with exponential backoff",
    "repo_context": "HTTP client for external API integration",
    "codex_solution": "import requests\nimport time\nimport logging\nfrom typing import Optional, Dict, Any\n\nlogger = logging.getLogger(__name__)\n\nclass APIClient:\n    def __init__(self, base_url: str, api_key: str, max_retries: int = 3):\n        self.base_url = base_url.rstrip('/')\n        self.api_key = api_key\n        self.max_retries = max_retries\n        self.session = requests.Session()\n        self.session.headers.update({'Authorization': f'Bearer {api_key}'})\n    \n    def _should_retry(self, status_code: int) -> bool:\n        \"\"\"Determine if request should be retried\"\"\"\n        return status_code in {408, 429, 500, 502, 503, 504}\n    \n    def request(self, method: str, endpoint: str, **kwargs) -> Optional[Dict[Any, Any]]:\n        \"\"\"Make API request with retry logic\"\"\"\n        url = f'{self.base_url}/{endpoint.lstrip(\"/\")}'\n        \n        for attempt in range(self.max_retries + 1):\n            try:\n                response = self.session.request(method, url, timeout=30, **kwargs)\n                \n                if response.status_code == 200:\n                    return response.json()\n                elif response.status_code == 404:\n                    logger.warning(f'Resource not found: {url}')\n                    return None\n                elif self._should_retry(response.status_code) and attempt < self.max_retries:\n                    wait_time = (2 ** attempt) + (time.time() % 1)\n                    logger.info(f'Retry {attempt + 1}/{self.max_retries} after {wait_time:.2f}s')\n                    time.sleep(wait_time)\n                    continue\n                else:\n                    response.raise_for_status()\n            \n            except requests.exceptions.Timeout:\n                if attempt < self.max_retries:\n                    wait_time = (2 ** attempt)\n                    logger.warning(f'Timeout, retrying in {wait_time}s')\n                    time.sleep(wait_time)\n                else:\n                    raise\n            except requests.exceptions.RequestException as e:\n                logger.error(f'Request failed: {e}')\n                raise\n        \n        return None",
    "actual_issue": "None - correct code with proper retry logic, exponential backoff with jitter, timeout handling, and appropriate error handling",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "password_hash_001",
    "issue_description": "User authentication system with password hashing",
    "repo_context": "Web application user management module",
    "codex_solution": "import hashlib\nimport os\n\nclass UserAuth:\n    def __init__(self):\n        self.users = {}\n    \n    def register_user(self, username, password):\n        \"\"\"Register new user with hashed password\"\"\"\n        if username in self.users:\n            return False\n        \n        # BUG: Using MD5 for password hashing (cryptographically broken)\n        password_hash = hashlib.md5(password.encode()).hexdigest()\n        self.users[username] = password_hash\n        return True\n    \n    def authenticate(self, username, password):\n        \"\"\"Verify user credentials\"\"\"\n        if username not in self.users:\n            return False\n        \n        password_hash = hashlib.md5(password.encode()).hexdigest()\n        return self.users[username] == password_hash",
    "actual_issue": "Using MD5 for password hashing without salt - cryptographically insecure, vulnerable to rainbow table attacks",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "json_parser_002",
    "issue_description": "Parse and validate JSON configuration files",
    "repo_context": "Application configuration loader",
    "codex_solution": "import json\n\ndef load_config(config_path):\n    \"\"\"Load and validate configuration file\"\"\"\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    \n    # Validate required fields\n    required = ['host', 'port', 'database']\n    for field in required:\n        if field not in config:\n            raise ValueError(f\"Missing required field: {field}\")\n    \n    # Set defaults\n    config.setdefault('timeout', 30)\n    config.setdefault('max_connections', 100)\n    config.setdefault('ssl', True)\n    \n    return config",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "list_merge_003",
    "issue_description": "Merge sorted lists efficiently",
    "repo_context": "Data processing utility functions",
    "codex_solution": "def merge_sorted_lists(list1, list2):\n    \"\"\"Merge two sorted lists into one sorted list\"\"\"\n    result = []\n    i = j = 0\n    \n    # BUG: No bounds checking - will crash with IndexError\n    while i < len(list1) and j < len(list2):\n        if list1[i] < list2[j]:\n            result.append(list1[i])\n            i += 1\n        else:\n            result.append(list2[j])\n            j += 1\n    \n    # BUG: Missing code to append remaining elements\n    return result",
    "actual_issue": "Incomplete merge - doesn't append remaining elements from either list after one is exhausted",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "rate_limiter_004",
    "issue_description": "API rate limiting with sliding window",
    "repo_context": "API gateway rate limiting middleware",
    "codex_solution": "import time\nfrom collections import defaultdict\n\nclass RateLimiter:\n    def __init__(self, max_requests=100, window_seconds=60):\n        self.max_requests = max_requests\n        self.window = window_seconds\n        self.requests = defaultdict(list)\n    \n    def allow_request(self, user_id):\n        \"\"\"Check if request is allowed under rate limit\"\"\"\n        now = time.time()\n        user_requests = self.requests[user_id]\n        \n        # BUG: Memory leak - old timestamps never removed\n        user_requests.append(now)\n        \n        # Count requests in current window\n        cutoff = now - self.window\n        recent = [ts for ts in user_requests if ts > cutoff]\n        \n        return len(recent) <= self.max_requests",
    "actual_issue": "Memory leak - request timestamps accumulate indefinitely without cleanup, causing unbounded memory growth",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "binary_search_005",
    "issue_description": "Binary search implementation for sorted arrays",
    "repo_context": "Search algorithms library",
    "codex_solution": "def binary_search(arr, target):\n    \"\"\"Find target in sorted array using binary search\"\"\"\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\ndef find_insert_position(arr, target):\n    \"\"\"Find position where target should be inserted\"\"\"\n    idx = binary_search(arr, target)\n    return idx if idx != -1 else len(arr)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "sql_injection_006",
    "issue_description": "Database query builder for user search",
    "repo_context": "User management system database layer",
    "codex_solution": "import sqlite3\n\nclass UserDatabase:\n    def __init__(self, db_path):\n        self.conn = sqlite3.connect(db_path)\n    \n    def search_users(self, search_term):\n        \"\"\"Search users by name or email\"\"\"\n        cursor = self.conn.cursor()\n        \n        # BUG: SQL injection vulnerability - unsanitized input\n        query = f\"SELECT * FROM users WHERE name LIKE '%{search_term}%' OR email LIKE '%{search_term}%'\"\n        cursor.execute(query)\n        \n        results = cursor.fetchall()\n        return results\n    \n    def get_user_by_id(self, user_id):\n        \"\"\"Get user by ID\"\"\"\n        cursor = self.conn.cursor()\n        query = f\"SELECT * FROM users WHERE id = {user_id}\"\n        cursor.execute(query)\n        return cursor.fetchone()",
    "actual_issue": "SQL injection vulnerability - user input directly interpolated into SQL queries without parameterization",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "datetime_comparison_007",
    "issue_description": "Compare dates for scheduling system",
    "repo_context": "Event scheduling and booking system",
    "codex_solution": "from datetime import datetime, timedelta\n\ndef is_booking_available(start_time, end_time, existing_bookings):\n    \"\"\"Check if time slot is available\"\"\"\n    for booking in existing_bookings:\n        # BUG: Using string comparison instead of datetime comparison\n        if start_time < booking['end'] and end_time > booking['start']:\n            return False\n    return True\n\ndef get_next_available_slot(start_date, duration_hours=1):\n    \"\"\"Find next available time slot\"\"\"\n    # BUG: Timezone-naive datetime comparison\n    current = datetime.now()\n    if start_date < current:\n        start_date = current\n    \n    end_date = start_date + timedelta(hours=duration_hours)\n    return {'start': start_date, 'end': end_date}",
    "actual_issue": "Timezone-naive datetime operations can cause incorrect comparisons across timezones and DST boundaries",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "csv_processor_008",
    "issue_description": "Process CSV files with data validation",
    "repo_context": "Data import pipeline",
    "codex_solution": "import csv\n\ndef process_csv_file(filepath, required_columns):\n    \"\"\"Process CSV and validate required columns\"\"\"\n    results = []\n    \n    with open(filepath, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        \n        # Validate headers\n        headers = reader.fieldnames\n        missing = set(required_columns) - set(headers)\n        if missing:\n            raise ValueError(f\"Missing columns: {missing}\")\n        \n        # Process rows\n        for row_num, row in enumerate(reader, start=2):\n            try:\n                validated_row = {k: row[k].strip() for k in required_columns}\n                results.append(validated_row)\n            except Exception as e:\n                print(f\"Warning: Row {row_num} skipped - {e}\")\n    \n    return results",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "thread_counter_009",
    "issue_description": "Thread-safe counter for concurrent requests",
    "repo_context": "Multi-threaded web server metrics",
    "codex_solution": "import threading\n\nclass RequestCounter:\n    def __init__(self):\n        self.count = 0\n        self.successful = 0\n        self.failed = 0\n    \n    def increment_total(self):\n        \"\"\"Increment total request count\"\"\"\n        # BUG: Race condition - not thread-safe\n        self.count += 1\n    \n    def increment_success(self):\n        \"\"\"Increment successful request count\"\"\"\n        # BUG: Race condition\n        self.successful += 1\n    \n    def increment_failure(self):\n        \"\"\"Increment failed request count\"\"\"\n        # BUG: Race condition\n        self.failed += 1\n    \n    def get_stats(self):\n        \"\"\"Get current statistics\"\"\"\n        return {\n            'total': self.count,\n            'successful': self.successful,\n            'failed': self.failed\n        }",
    "actual_issue": "Race conditions in multi-threaded environment - counter increments are not atomic, leading to incorrect counts",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "division_calculator_010",
    "issue_description": "Calculate average from list of numbers",
    "repo_context": "Statistical analysis utilities",
    "codex_solution": "def calculate_average(numbers):\n    \"\"\"Calculate average of numbers with validation\"\"\"\n    if not numbers:\n        return 0\n    \n    # BUG: Integer division in Python 2 style (though works in Python 3)\n    # More critical: no handling of non-numeric values\n    total = sum(numbers)\n    return total / len(numbers)\n\ndef calculate_weighted_average(values, weights):\n    \"\"\"Calculate weighted average\"\"\"\n    # BUG: No check if lengths match\n    if not values:\n        return 0\n    \n    weighted_sum = sum(v * w for v, w in zip(values, weights))\n    total_weight = sum(weights)\n    \n    # BUG: Division by zero if total_weight is 0\n    return weighted_sum / total_weight",
    "actual_issue": "Division by zero when total_weight is 0, and no validation that values and weights lists have same length",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "cache_invalidation_001",
    "issue_description": "LRU cache implementation with TTL support",
    "repo_context": "Backend caching layer for API responses",
    "codex_solution": "import time\nfrom collections import OrderedDict\nimport threading\n\nclass LRUCache:\n    def __init__(self, capacity=100, ttl=300):\n        self.cache = OrderedDict()\n        self.capacity = capacity\n        self.ttl = ttl\n        self.lock = threading.Lock()\n    \n    def get(self, key):\n        with self.lock:\n            if key not in self.cache:\n                return None\n            value, timestamp = self.cache[key]\n            if time.time() - timestamp > self.ttl:\n                del self.cache[key]\n                return None\n            self.cache.move_to_end(key)\n            return value\n    \n    def put(self, key, value):\n        with self.lock:\n            if key in self.cache:\n                self.cache.move_to_end(key)\n            self.cache[key] = (value, time.time())\n            if len(self.cache) > self.capacity:\n                self.cache.popitem(last=False)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "password_validator_002",
    "issue_description": "Password validation with security requirements",
    "repo_context": "User authentication system",
    "codex_solution": "import re\nimport hashlib\n\nclass PasswordValidator:\n    def __init__(self):\n        self.min_length = 8\n        self.common_passwords = self._load_common_passwords()\n    \n    def _load_common_passwords(self):\n        return {'password', '12345678', 'qwerty', 'admin123'}\n    \n    def validate(self, password):\n        if len(password) < self.min_length:\n            return False, \"Password too short\"\n        \n        if password.lower() in self.common_passwords:\n            return False, \"Password too common\"\n        \n        if not re.search(r'[A-Z]', password):\n            return False, \"Need uppercase letter\"\n        \n        if not re.search(r'[a-z]', password):\n            return False, \"Need lowercase letter\"\n        \n        if not re.search(r'\\d', password):\n            return False, \"Need digit\"\n        \n        return True, \"Valid password\"\n    \n    def hash_password(self, password):\n        return hashlib.md5(password.encode()).hexdigest()",
    "actual_issue": "Uses MD5 for password hashing which is cryptographically broken and unsuitable for passwords. Should use bcrypt, argon2, or PBKDF2.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "json_parser_003",
    "issue_description": "Parse and validate JSON configuration files",
    "repo_context": "Application configuration loader",
    "codex_solution": "import json\nimport os\n\nclass ConfigParser:\n    def __init__(self, config_path):\n        self.config_path = config_path\n        self.config = {}\n    \n    def load(self):\n        if not os.path.exists(self.config_path):\n            raise FileNotFoundError(f\"Config not found: {self.config_path}\")\n        \n        with open(self.config_path, 'r') as f:\n            self.config = json.load(f)\n        \n        return self.config\n    \n    def get(self, key, default=None):\n        keys = key.split('.')\n        value = self.config\n        \n        for k in keys:\n            if isinstance(value, dict) and k in value:\n                value = value[k]\n            else:\n                return default\n        \n        return value\n    \n    def validate_required(self, required_keys):\n        for key in required_keys:\n            if self.get(key) is None:\n                raise ValueError(f\"Missing required config: {key}\")\n        return True",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "rate_limiter_004",
    "issue_description": "Token bucket rate limiter for API endpoints",
    "repo_context": "API gateway rate limiting",
    "codex_solution": "import time\nfrom collections import defaultdict\n\nclass RateLimiter:\n    def __init__(self, rate=10, per=60):\n        self.rate = rate\n        self.per = per\n        self.allowance = defaultdict(lambda: self.rate)\n        self.last_check = defaultdict(lambda: time.time())\n    \n    def allow_request(self, client_id):\n        current = time.time()\n        time_passed = current - self.last_check[client_id]\n        self.last_check[client_id] = current\n        \n        self.allowance[client_id] += time_passed * (self.rate / self.per)\n        \n        if self.allowance[client_id] > self.rate:\n            self.allowance[client_id] = self.rate\n        \n        if self.allowance[client_id] < 1.0:\n            return False\n        else:\n            self.allowance[client_id] -= 1.0\n            return True",
    "actual_issue": "Memory leak: defaultdict stores entries for every client_id forever without cleanup or expiration, causing unbounded memory growth.",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "binary_search_005",
    "issue_description": "Binary search with edge case handling",
    "repo_context": "Search utility library",
    "codex_solution": "def binary_search(arr, target):\n    if not arr:\n        return -1\n    \n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\ndef find_insert_position(arr, target):\n    if not arr:\n        return 0\n    \n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return left",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "file_processor_006",
    "issue_description": "Process large log files line by line",
    "repo_context": "Log analysis system",
    "codex_solution": "import os\nimport gzip\n\nclass LogProcessor:\n    def __init__(self, log_path):\n        self.log_path = log_path\n        self.stats = {'total': 0, 'errors': 0, 'warnings': 0}\n    \n    def process(self):\n        if self.log_path.endswith('.gz'):\n            file_obj = gzip.open(self.log_path, 'rt')\n        else:\n            file_obj = open(self.log_path, 'r')\n        \n        for line in file_obj:\n            self.stats['total'] += 1\n            \n            if 'ERROR' in line:\n                self.stats['errors'] += 1\n            elif 'WARNING' in line:\n                self.stats['warnings'] += 1\n        \n        return self.stats\n    \n    def get_error_lines(self):\n        errors = []\n        if self.log_path.endswith('.gz'):\n            file_obj = gzip.open(self.log_path, 'rt')\n        else:\n            file_obj = open(self.log_path, 'r')\n        \n        for line in file_obj:\n            if 'ERROR' in line:\n                errors.append(line.strip())\n        \n        return errors",
    "actual_issue": "Resource leak: file handles are never closed. Should use 'with' statement or explicit close() calls. Multiple file opens without cleanup.",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "url_shortener_007",
    "issue_description": "URL shortener with collision handling",
    "repo_context": "URL shortening service",
    "codex_solution": "import hashlib\nimport string\nimport random\n\nclass URLShortener:\n    def __init__(self):\n        self.url_map = {}\n        self.reverse_map = {}\n        self.chars = string.ascii_letters + string.digits\n    \n    def shorten(self, long_url):\n        if long_url in self.reverse_map:\n            return self.reverse_map[long_url]\n        \n        hash_obj = hashlib.md5(long_url.encode())\n        hash_hex = hash_obj.hexdigest()[:6]\n        \n        short_code = self._base62_encode(int(hash_hex, 16))\n        \n        while short_code in self.url_map:\n            short_code = ''.join(random.choices(self.chars, k=6))\n        \n        self.url_map[short_code] = long_url\n        self.reverse_map[long_url] = short_code\n        \n        return short_code\n    \n    def _base62_encode(self, num):\n        if num == 0:\n            return self.chars[0]\n        \n        result = []\n        while num:\n            result.append(self.chars[num % 62])\n            num //= 62\n        \n        return ''.join(reversed(result))\n    \n    def expand(self, short_code):\n        return self.url_map.get(short_code)",
    "actual_issue": "Race condition: collision check and insertion are not atomic. In concurrent scenarios, multiple threads could check the same short_code simultaneously and both insert, causing data corruption.",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "pagination_008",
    "issue_description": "Database query pagination helper",
    "repo_context": "API response pagination",
    "codex_solution": "class Paginator:\n    def __init__(self, items, page_size=20):\n        self.items = items\n        self.page_size = page_size\n        self.total_items = len(items)\n        self.total_pages = (self.total_items + page_size - 1) // page_size\n    \n    def get_page(self, page_number):\n        if page_number < 1 or page_number > self.total_pages:\n            return []\n        \n        start_idx = (page_number - 1) * self.page_size\n        end_idx = start_idx + self.page_size\n        \n        return self.items[start_idx:end_idx]\n    \n    def get_page_info(self, page_number):\n        return {\n            'page': page_number,\n            'page_size': self.page_size,\n            'total_items': self.total_items,\n            'total_pages': self.total_pages,\n            'has_next': page_number < self.total_pages,\n            'has_prev': page_number > 1\n        }",
    "actual_issue": "Integer overflow potential: when total_items is 0 and page_size is 0, division by zero occurs. Also doesn't handle negative page_size values.",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "email_validator_009",
    "issue_description": "Validate email addresses with RFC compliance",
    "repo_context": "User registration system",
    "codex_solution": "import re\n\nclass EmailValidator:\n    def __init__(self):\n        self.pattern = re.compile(\n            r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n        )\n        self.blocked_domains = {'tempmail.com', 'throwaway.email'}\n    \n    def validate(self, email):\n        if not email or not isinstance(email, str):\n            return False\n        \n        email = email.strip().lower()\n        \n        if not self.pattern.match(email):\n            return False\n        \n        domain = email.split('@')[1]\n        \n        if domain in self.blocked_domains:\n            return False\n        \n        if '..' in email:\n            return False\n        \n        if email.startswith('.') or email.endswith('.'):\n            return False\n        \n        return True\n    \n    def normalize(self, email):\n        email = email.strip().lower()\n        local, domain = email.split('@')\n        \n        if '+' in local:\n            local = local.split('+')[0]\n        \n        return f\"{local}@{domain}\"",
    "actual_issue": "ReDoS vulnerability: regex pattern is vulnerable to catastrophic backtracking with crafted input like 'a@' + 'a'*50 + '!' causing exponential time complexity.",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "merkle_tree_010",
    "issue_description": "Build Merkle tree for data verification",
    "repo_context": "Blockchain data structure",
    "codex_solution": "import hashlib\n\nclass MerkleTree:\n    def __init__(self, data_blocks):\n        self.leaves = [self._hash(block) for block in data_blocks]\n        self.root = self._build_tree(self.leaves)\n    \n    def _hash(self, data):\n        if isinstance(data, str):\n            data = data.encode()\n        return hashlib.sha256(data).hexdigest()\n    \n    def _build_tree(self, nodes):\n        if len(nodes) == 0:\n            return self._hash(b'')\n        \n        if len(nodes) == 1:\n            return nodes[0]\n        \n        if len(nodes) % 2 == 1:\n            nodes.append(nodes[-1])\n        \n        parent_nodes = []\n        for i in range(0, len(nodes), 2):\n            combined = nodes[i] + nodes[i + 1]\n            parent_nodes.append(self._hash(combined))\n        \n        return self._build_tree(parent_nodes)\n    \n    def get_root(self):\n        return self.root\n    \n    def verify_leaf(self, data, proof):\n        current = self._hash(data)\n        \n        for sibling, is_right in proof:\n            if is_right:\n                current = self._hash(current + sibling)\n            else:\n                current = self._hash(sibling + current)\n        \n        return current == self.root",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "jwt_auth_001",
    "issue_description": "JWT token validation for API authentication",
    "repo_context": "Authentication middleware for REST API",
    "codex_solution": "import jwt\nfrom flask import request, jsonify\nfrom functools import wraps\n\nSECRET_KEY = 'my-secret-key'\n\ndef token_required(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        token = request.headers.get('Authorization')\n        \n        if not token:\n            return jsonify({'error': 'Token missing'}), 401\n        \n        try:\n            # BUG: No algorithm verification - vulnerable to algorithm confusion\n            data = jwt.decode(token, SECRET_KEY, algorithms=['HS256', 'none'])\n            current_user = data['user_id']\n        except jwt.ExpiredSignatureError:\n            return jsonify({'error': 'Token expired'}), 401\n        except jwt.InvalidTokenError:\n            return jsonify({'error': 'Invalid token'}), 401\n        \n        return f(current_user, *args, **kwargs)\n    return decorated\n\n@app.route('/api/data')\n@token_required\ndef get_data(user_id):\n    return jsonify({'data': f'User {user_id} data'})",
    "actual_issue": "Allows 'none' algorithm in JWT decode which bypasses signature verification, enabling trivial token forgery",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "race_condition_001",
    "issue_description": "Thread-safe counter for concurrent requests",
    "repo_context": "Request rate limiter for API gateway",
    "codex_solution": "import time\nfrom threading import Lock\n\nclass RateLimiter:\n    def __init__(self, max_requests=100, window=60):\n        self.max_requests = max_requests\n        self.window = window\n        self.requests = {}\n        self.lock = Lock()\n    \n    def allow_request(self, user_id):\n        current_time = time.time()\n        \n        # BUG: Check happens outside lock - race condition\n        if user_id not in self.requests:\n            self.requests[user_id] = []\n        \n        with self.lock:\n            # Clean old requests\n            self.requests[user_id] = [\n                req_time for req_time in self.requests[user_id]\n                if current_time - req_time < self.window\n            ]\n            \n            # BUG: Count check after cleanup but multiple threads can pass\n            if len(self.requests[user_id]) >= self.max_requests:\n                return False\n            \n            self.requests[user_id].append(current_time)\n            return True",
    "actual_issue": "Check-then-act race condition: user_id existence check outside lock allows multiple threads to create conflicting entries",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "sql_injection_002",
    "issue_description": "User search functionality with filters",
    "repo_context": "Database query builder for user management system",
    "codex_solution": "import sqlite3\n\ndef search_users(conn, username=None, email=None, sort_by='created_at'):\n    \"\"\"Search users with optional filters\"\"\"\n    query = \"SELECT * FROM users WHERE 1=1\"\n    params = []\n    \n    if username:\n        query += \" AND username = ?\"\n        params.append(username)\n    \n    if email:\n        query += \" AND email = ?\"\n        params.append(email)\n    \n    # BUG: sort_by parameter not sanitized - SQL injection\n    query += f\" ORDER BY {sort_by}\"\n    \n    cursor = conn.cursor()\n    cursor.execute(query, params)\n    return cursor.fetchall()\n\n# Example usage\nresults = search_users(conn, username='john', sort_by=request.args.get('sort', 'created_at'))",
    "actual_issue": "SQL injection via sort_by parameter which is directly interpolated without validation or parameterization",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "memory_leak_001",
    "issue_description": "Cache implementation for expensive computations",
    "repo_context": "Caching layer for ML model predictions",
    "codex_solution": "import hashlib\nimport pickle\n\nclass PredictionCache:\n    def __init__(self):\n        self.cache = {}\n        self.hits = 0\n        self.misses = 0\n    \n    def get_key(self, input_data):\n        serialized = pickle.dumps(input_data)\n        return hashlib.md5(serialized).hexdigest()\n    \n    def predict(self, model, input_data):\n        key = self.get_key(input_data)\n        \n        if key in self.cache:\n            self.hits += 1\n            return self.cache[key]\n        \n        # BUG: No cache size limit or eviction policy\n        self.misses += 1\n        result = model.predict(input_data)\n        self.cache[key] = result\n        return result\n    \n    def get_stats(self):\n        return {\n            'hits': self.hits,\n            'misses': self.misses,\n            'cache_size': len(self.cache)\n        }",
    "actual_issue": "Unbounded cache growth leads to memory exhaustion - no size limit, TTL, or LRU eviction policy",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "integer_overflow_001",
    "issue_description": "Calculate total storage used by user files",
    "repo_context": "Storage quota management system",
    "codex_solution": "def calculate_user_storage(user_id, file_list):\n    \"\"\"Calculate total storage in bytes for user files\"\"\"\n    total_bytes = 0\n    \n    for file_info in file_list:\n        file_size = file_info.get('size', 0)\n        \n        # BUG: No overflow check for large file sizes\n        total_bytes += file_size\n    \n    # Convert to appropriate unit\n    if total_bytes < 1024:\n        return f\"{total_bytes} B\"\n    elif total_bytes < 1024 ** 2:\n        return f\"{total_bytes / 1024:.2f} KB\"\n    elif total_bytes < 1024 ** 3:\n        return f\"{total_bytes / (1024 ** 2):.2f} MB\"\n    else:\n        return f\"{total_bytes / (1024 ** 3):.2f} GB\"\n\ndef check_quota(user_id, quota_gb):\n    files = get_user_files(user_id)\n    used = calculate_user_storage(user_id, files)\n    # BUG: String comparison instead of numeric\n    return used < f\"{quota_gb} GB\"",
    "actual_issue": "Returns formatted string instead of numeric value, making quota comparison use string comparison instead of numeric, causing incorrect results",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "correct_pagination_001",
    "issue_description": "Paginate database results efficiently",
    "repo_context": "API endpoint for listing products with pagination",
    "codex_solution": "from flask import request, jsonify\nfrom sqlalchemy import func\n\ndef get_products(db_session, page=1, per_page=20):\n    \"\"\"Get paginated product list\"\"\"\n    # Validate inputs\n    page = max(1, int(page))\n    per_page = min(max(1, int(per_page)), 100)\n    \n    # Get total count\n    total = db_session.query(func.count(Product.id)).scalar()\n    \n    # Calculate offset\n    offset = (page - 1) * per_page\n    \n    # Get paginated results\n    products = db_session.query(Product)\\\n        .order_by(Product.created_at.desc())\\\n        .limit(per_page)\\\n        .offset(offset)\\\n        .all()\n    \n    return {\n        'products': [p.to_dict() for p in products],\n        'page': page,\n        'per_page': per_page,\n        'total': total,\n        'pages': (total + per_page - 1) // per_page\n    }\n\n@app.route('/api/products')\ndef list_products():\n    page = request.args.get('page', 1, type=int)\n    per_page = request.args.get('per_page', 20, type=int)\n    result = get_products(db.session, page, per_page)\n    return jsonify(result)",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "path_traversal_002",
    "issue_description": "Serve user-uploaded static files",
    "repo_context": "File serving endpoint for document management system",
    "codex_solution": "import os\nfrom flask import send_file, abort\n\nUPLOAD_DIR = '/var/app/uploads'\n\ndef serve_user_file(user_id, filename):\n    \"\"\"Serve uploaded file to user\"\"\"\n    # BUG: Insufficient path traversal protection\n    if '..' in filename:\n        abort(400, 'Invalid filename')\n    \n    # Construct file path\n    user_dir = os.path.join(UPLOAD_DIR, str(user_id))\n    # BUG: No normalization - can use encoded characters\n    file_path = os.path.join(user_dir, filename)\n    \n    if not os.path.exists(file_path):\n        abort(404, 'File not found')\n    \n    # BUG: No check if file_path is within user_dir\n    return send_file(file_path)\n\n@app.route('/files/<int:user_id>/<path:filename>')\ndef download_file(user_id, filename):\n    return serve_user_file(user_id, filename)",
    "actual_issue": "Path traversal via URL encoding or symlinks - only checks for literal '..' but doesn't verify final resolved path is within user directory",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "correct_validation_001",
    "issue_description": "Validate and sanitize user registration data",
    "repo_context": "User registration endpoint with input validation",
    "codex_solution": "import re\nfrom email_validator import validate_email, EmailNotValidError\n\ndef validate_registration(data):\n    \"\"\"Validate user registration data\"\"\"\n    errors = []\n    \n    # Validate username\n    username = data.get('username', '').strip()\n    if not username:\n        errors.append('Username is required')\n    elif len(username) < 3 or len(username) > 30:\n        errors.append('Username must be 3-30 characters')\n    elif not re.match(r'^[a-zA-Z0-9_-]+$', username):\n        errors.append('Username can only contain letters, numbers, - and _')\n    \n    # Validate email\n    email = data.get('email', '').strip()\n    try:\n        valid = validate_email(email)\n        email = valid.email\n    except EmailNotValidError as e:\n        errors.append(f'Invalid email: {str(e)}')\n    \n    # Validate password\n    password = data.get('password', '')\n    if len(password) < 8:\n        errors.append('Password must be at least 8 characters')\n    if not re.search(r'[A-Z]', password):\n        errors.append('Password must contain uppercase letter')\n    if not re.search(r'[a-z]', password):\n        errors.append('Password must contain lowercase letter')\n    if not re.search(r'[0-9]', password):\n        errors.append('Password must contain number')\n    \n    return {'valid': len(errors) == 0, 'errors': errors}",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "datetime_edge_case_001",
    "issue_description": "Calculate business days between two dates",
    "repo_context": "Scheduling system for project deadlines",
    "codex_solution": "from datetime import datetime, timedelta\n\ndef business_days_between(start_date, end_date):\n    \"\"\"Calculate number of business days between dates\"\"\"\n    if start_date > end_date:\n        return 0\n    \n    business_days = 0\n    current = start_date\n    \n    # BUG: Off-by-one error - doesn't include end_date\n    while current < end_date:\n        # Monday = 0, Sunday = 6\n        if current.weekday() < 5:\n            business_days += 1\n        current += timedelta(days=1)\n    \n    return business_days\n\ndef add_business_days(start_date, days):\n    \"\"\"Add business days to a date\"\"\"\n    current = start_date\n    added = 0\n    \n    while added < days:\n        current += timedelta(days=1)\n        # BUG: Increments even on weekends\n        if current.weekday() < 5:\n            added += 1\n    \n    return current",
    "actual_issue": "Off-by-one error in business_days_between (should include end_date), and add_business_days increments before checking weekday",
    "failure_category": "edge_case",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "correct_binary_search_001",
    "issue_description": "Find insertion point in sorted array",
    "repo_context": "Efficient data structure for maintaining sorted collection",
    "codex_solution": "def binary_search_insert(arr, target):\n    \"\"\"Find index where target should be inserted to maintain sorted order\"\"\"\n    if not arr:\n        return 0\n    \n    left, right = 0, len(arr)\n    \n    while left < right:\n        mid = (left + right) // 2\n        \n        if arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid\n    \n    return left\n\ndef insert_sorted(arr, value):\n    \"\"\"Insert value into sorted array maintaining order\"\"\"\n    index = binary_search_insert(arr, value)\n    arr.insert(index, value)\n    return arr\n\ndef test_binary_search():\n    arr = [1, 3, 5, 7, 9]\n    assert binary_search_insert(arr, 0) == 0\n    assert binary_search_insert(arr, 4) == 2\n    assert binary_search_insert(arr, 10) == 5\n    assert binary_search_insert(arr, 5) == 2\n    assert binary_search_insert([], 5) == 0",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_7_001",
    "issue_description": "Password validation function for user authentication system",
    "repo_context": "Security module for web application user registration",
    "codex_solution": "def validate_password(password, username):\n    \"\"\"Validate password meets security requirements\"\"\"\n    if len(password) < 8:\n        return False\n    \n    # Check for required character types\n    has_upper = any(c.isupper() for c in password)\n    has_lower = any(c.islower() for c in password)\n    has_digit = any(c.isdigit() for c in password)\n    \n    if not (has_upper and has_lower and has_digit):\n        return False\n    \n    # BUG: Case-insensitive comparison allows \"Password123\" for username \"password\"\n    if username.lower() in password.lower():\n        return False\n    \n    # BUG: No check for common passwords\n    # BUG: No special character requirement\n    return True",
    "actual_issue": "Case-insensitive username check allows easily guessable passwords; missing common password check and special character requirement",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_002",
    "issue_description": "Binary search implementation for sorted array",
    "repo_context": "Utility function for searching in sorted datasets",
    "codex_solution": "def binary_search(arr, target):\n    \"\"\"Find target in sorted array using binary search\"\"\"\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        # BUG: Integer overflow possible with large indices\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\ndef find_all_occurrences(arr, target):\n    \"\"\"Find all indices of target in sorted array\"\"\"\n    index = binary_search(arr, target)\n    if index == -1:\n        return []\n    \n    # BUG: Only returns first occurrence, not all\n    return [index]",
    "actual_issue": "find_all_occurrences only returns first match instead of all occurrences; binary_search has theoretical overflow issue",
    "failure_category": "correctness",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_7_003",
    "issue_description": "Cache implementation with LRU eviction policy",
    "repo_context": "Caching layer for database query results",
    "codex_solution": "import time\n\nclass LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = {}\n        self.access_times = {}\n    \n    def get(self, key):\n        \"\"\"Get value from cache\"\"\"\n        if key in self.cache:\n            self.access_times[key] = time.time()\n            return self.cache[key]\n        return None\n    \n    def put(self, key, value):\n        \"\"\"Put value in cache with LRU eviction\"\"\"\n        if key in self.cache:\n            self.cache[key] = value\n            self.access_times[key] = time.time()\n            return\n        \n        if len(self.cache) >= self.capacity:\n            # BUG: O(n) eviction on every insertion when full\n            lru_key = min(self.access_times, key=self.access_times.get)\n            del self.cache[lru_key]\n            del self.access_times[lru_key]\n        \n        self.cache[key] = value\n        self.access_times[key] = time.time()",
    "actual_issue": "O(n) complexity for LRU eviction instead of O(1) with proper data structure (OrderedDict or doubly-linked list)",
    "failure_category": "performance",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_004",
    "issue_description": "JSON API endpoint handler with input sanitization",
    "repo_context": "REST API handler for processing user-submitted data",
    "codex_solution": "import json\nimport subprocess\n\ndef process_user_command(request_data):\n    \"\"\"Process user command from API request\"\"\"\n    try:\n        data = json.loads(request_data)\n        command = data.get('command', '')\n        args = data.get('args', [])\n        \n        # BUG: Command injection vulnerability - no sanitization\n        allowed_commands = ['ls', 'cat', 'grep', 'find']\n        \n        if command in allowed_commands:\n            # BUG: Directly using user input in shell command\n            cmd = f\"{command} {' '.join(args)}\"\n            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n            return {'status': 'success', 'output': result.stdout}\n        else:\n            return {'status': 'error', 'message': 'Command not allowed'}\n    except json.JSONDecodeError:\n        return {'status': 'error', 'message': 'Invalid JSON'}",
    "actual_issue": "Command injection vulnerability through shell=True with unsanitized user input; args can contain malicious shell metacharacters",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "unique_id_7_005",
    "issue_description": "Merge sort implementation for sorting arrays",
    "repo_context": "Sorting utility for data processing pipeline",
    "codex_solution": "def merge_sort(arr):\n    \"\"\"Sort array using merge sort algorithm\"\"\"\n    if len(arr) <= 1:\n        return arr\n    \n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n    \n    return merge(left, right)\n\ndef merge(left, right):\n    \"\"\"Merge two sorted arrays\"\"\"\n    result = []\n    i = j = 0\n    \n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    \n    result.extend(left[i:])\n    result.extend(right[j:])\n    \n    return result",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "easy"
  },
  {
    "problem_id": "unique_id_7_006",
    "issue_description": "Database connection pool manager",
    "repo_context": "Connection pooling for high-traffic web application",
    "codex_solution": "import threading\nimport queue\nimport time\n\nclass ConnectionPool:\n    def __init__(self, max_connections=10):\n        self.max_connections = max_connections\n        self.pool = queue.Queue(maxsize=max_connections)\n        self.active_connections = 0\n        self.lock = threading.Lock()\n    \n    def get_connection(self, timeout=5):\n        \"\"\"Get connection from pool\"\"\"\n        try:\n            return self.pool.get(timeout=timeout)\n        except queue.Empty:\n            with self.lock:\n                if self.active_connections < self.max_connections:\n                    self.active_connections += 1\n                    return self._create_connection()\n            # BUG: Raises exception instead of waiting for available connection\n            raise Exception(\"No connections available\")\n    \n    def release_connection(self, conn):\n        \"\"\"Return connection to pool\"\"\"\n        # BUG: No validation if connection is still valid\n        # BUG: Pool can grow beyond max_connections if not careful\n        try:\n            self.pool.put_nowait(conn)\n        except queue.Full:\n            conn.close()\n    \n    def _create_connection(self):\n        return {'id': time.time(), 'active': True}",
    "actual_issue": "Connection pool doesn't properly handle connection reuse; no validation of connection health; exception instead of blocking when pool exhausted",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "unique_id_7_007",
    "issue_description": "Calculate median of two sorted arrays",
    "repo_context": "Statistical analysis module for data science library",
    "codex_solution": "def find_median_sorted_arrays(nums1, nums2):\n    \"\"\"Find median of two sorted arrays in O(log(m+n)) time\"\"\"\n    # Ensure nums1 is the smaller array\n    if len(nums1) > len(nums2):\n        nums1, nums2 = nums2, nums1\n    \n    m, n = len(nums1), len(nums2)\n    imin, imax = 0, m\n    \n    while imin <= imax:\n        i = (imin + imax) // 2\n        j = (m + n + 1) // 2 - i\n        \n        if i < m and nums2[j-1] > nums1[i]:\n            imin = i + 1\n        elif i > 0 and nums1[i-1] > nums2[j]:\n            imax = i - 1\n        else:\n            # Found correct partition\n            if i == 0: max_left = nums2[j-1]\n            elif j == 0: max_left = nums1[i-1]\n            else: max_left = max(nums1[i-1], nums2[j-1])\n            \n            if (m + n) % 2 == 1:\n                return max_left\n            \n            if i == m: min_right = nums2[j]\n            elif j == n: min_right = nums1[i]\n            else: min_right = min(nums1[i], nums2[j])\n            \n            return (max_left + min_right) / 2.0\n    \n    return 0.0",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "hard"
  },
  {
    "problem_id": "unique_id_7_008",
    "issue_description": "Rate limiter for API endpoints using sliding window",
    "repo_context": "Middleware for rate limiting API requests per user",
    "codex_solution": "import time\nfrom collections import defaultdict\n\nclass RateLimiter:\n    def __init__(self, max_requests, window_seconds):\n        self.max_requests = max_requests\n        self.window_seconds = window_seconds\n        self.requests = defaultdict(list)\n    \n    def is_allowed(self, user_id):\n        \"\"\"Check if request is allowed under rate limit\"\"\"\n        current_time = time.time()\n        user_requests = self.requests[user_id]\n        \n        # BUG: Memory leak - old timestamps never cleaned up\n        # Only removes from window, but list keeps growing\n        cutoff_time = current_time - self.window_seconds\n        \n        # BUG: O(n) filtering on every request\n        valid_requests = [ts for ts in user_requests if ts > cutoff_time]\n        self.requests[user_id] = valid_requests\n        \n        if len(valid_requests) < self.max_requests:\n            self.requests[user_id].append(current_time)\n            return True\n        \n        return False",
    "actual_issue": "Memory grows unboundedly with number of users; O(n) filtering per request; no cleanup of inactive users",
    "failure_category": "resource_management",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_009",
    "issue_description": "Depth-first search for graph traversal",
    "repo_context": "Graph algorithms library for network analysis",
    "codex_solution": "def dfs_iterative(graph, start):\n    \"\"\"Perform iterative DFS traversal of graph\"\"\"\n    visited = set()\n    stack = [start]\n    result = []\n    \n    while stack:\n        node = stack.pop()\n        \n        if node not in visited:\n            visited.add(node)\n            result.append(node)\n            \n            # Add neighbors to stack\n            for neighbor in reversed(graph.get(node, [])):\n                if neighbor not in visited:\n                    stack.append(neighbor)\n    \n    return result\n\ndef has_cycle(graph):\n    \"\"\"Check if directed graph has a cycle\"\"\"\n    visited = set()\n    rec_stack = set()\n    \n    def dfs(node):\n        visited.add(node)\n        rec_stack.add(node)\n        \n        for neighbor in graph.get(node, []):\n            if neighbor not in visited:\n                if dfs(neighbor):\n                    return True\n            elif neighbor in rec_stack:\n                return True\n        \n        rec_stack.remove(node)\n        return False\n    \n    for node in graph:\n        if node not in visited:\n            if dfs(node):\n                return True\n    \n    return False",
    "actual_issue": "None - correct code",
    "failure_category": "correctness",
    "expected_test_pass": true,
    "should_be_rejected": false,
    "difficulty_level": "medium"
  },
  {
    "problem_id": "unique_id_7_010",
    "issue_description": "Parse and evaluate mathematical expressions",
    "repo_context": "Calculator module for parsing user input expressions",
    "codex_solution": "def evaluate_expression(expr):\n    \"\"\"Evaluate mathematical expression string\"\"\"\n    # BUG: Using eval() on user input - code injection vulnerability\n    try:\n        # Remove whitespace\n        expr = expr.strip()\n        \n        # BUG: Weak validation - can be bypassed\n        allowed_chars = set('0123456789+-*/().')\n        if not all(c in allowed_chars or c.isspace() for c in expr):\n            return None\n        \n        # BUG: eval() allows arbitrary code execution\n        # Example: \"__import__('os').system('ls')\" could bypass char check with encoding\n        result = eval(expr)\n        return result\n    except:\n        return None\n\ndef calculate(expression):\n    \"\"\"Calculate result of expression\"\"\"\n    # BUG: No input length limit - DoS possible with huge expressions\n    return evaluate_expression(expression)",
    "actual_issue": "Code injection via eval() on user input; weak validation can be bypassed; no input length limit enables DoS attacks",
    "failure_category": "security",
    "expected_test_pass": false,
    "should_be_rejected": true,
    "difficulty_level": "easy"
  }
]